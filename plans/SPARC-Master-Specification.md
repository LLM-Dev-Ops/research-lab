# LLM-Research-Lab: Complete SPARC Specification

> **Master Specification Document**
> All Five SPARC Phases Combined
> Part of the LLM DevOps Ecosystem

---

## Document Overview

This master document consolidates the complete SPARC (Specification, Pseudocode, Architecture, Refinement, Completion) methodology specification for LLM-Research-Lab. It contains all five phases required for enterprise-grade, commercially viable, production-ready implementation.

**Project**: LLM-Research-Lab - Experimental Evaluation Platform for Large Language Models
**Ecosystem**: LLM DevOps (24+ foundational modules)
**Technology Stack**: Rust, Axum, Tokio, PostgreSQL, ClickHouse, Kafka, Redis, Kubernetes

---

## SPARC Methodology Overview

```
┌─────────────────────────────────────────────────────────────────┐
│                      SPARC Methodology                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │ Phase 1: SPECIFICATION                                   │   │
│  │ • Purpose, scope, problem definition                     │   │
│  │ • Objectives, users, dependencies                        │   │
│  │ • Design principles, success metrics                     │   │
│  └─────────────────────────────────────────────────────────┘   │
│                           │                                     │
│                           ▼                                     │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │ Phase 2: PSEUDOCODE                                      │   │
│  │ • Core data models and types                             │   │
│  │ • Algorithm design and business logic                    │   │
│  │ • API contracts and interfaces                           │   │
│  └─────────────────────────────────────────────────────────┘   │
│                           │                                     │
│                           ▼                                     │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │ Phase 3: ARCHITECTURE                                    │   │
│  │ • System design and service decomposition                │   │
│  │ • Infrastructure and deployment topology                 │   │
│  │ • Database schemas and data flow                         │   │
│  └─────────────────────────────────────────────────────────┘   │
│                           │                                     │
│                           ▼                                     │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │ Phase 4: REFINEMENT                                      │   │
│  │ • Code quality standards and testing strategy            │   │
│  │ • Performance optimization and security hardening        │   │
│  │ • Review processes and compliance                        │   │
│  └─────────────────────────────────────────────────────────┘   │
│                           │                                     │
│                           ▼                                     │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │ Phase 5: COMPLETION                                      │   │
│  │ • Deployment readiness and production infrastructure     │   │
│  │ • Operations, monitoring, disaster recovery              │   │
│  │ • Handoff, validation, and launch                        │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## Quality Targets Summary

| Metric | Target | Phase |
|--------|--------|-------|
| Code Coverage | ≥ 85% | Refinement |
| Mutation Score | ≥ 70% | Refinement |
| API Latency (p99) | < 100ms | Completion |
| Availability SLO | 99.9% | Completion |
| RTO | < 4 hours | Completion |
| RPO | < 1 hour | Completion |
| MTTR | < 30 minutes | Completion |
| Security Vulnerabilities | Zero critical/high | Refinement |
| Documentation Coverage | 100% public API | Refinement |

---

## Table of Contents

### Part I: Specification (Phase 1)
- [1. Purpose](#1-purpose)
- [2. Scope](#2-scope)
- [3. Problem Definition](#3-problem-definition)
- [4. Objectives](#4-objectives)
- [5. Users & Roles](#5-users--roles)
- [6. Dependencies](#6-dependencies)
- [7. Design Principles](#7-design-principles)
- [8. Success Metrics](#8-success-metrics)

### Part II: Pseudocode (Phase 2)
- [9. Core Data Models](#9-core-data-models)
- [10. Experiment Tracking](#10-experiment-tracking)
- [11. Metric System](#11-metric-system)
- [12. Dataset Management](#12-dataset-management)
- [13. Workflow Engine](#13-workflow-engine)
- [14. API Contracts](#14-api-contracts)

### Part III: Architecture (Phase 3)
- [15. System Overview](#15-system-overview)
- [16. Service Architecture](#16-service-architecture)
- [17. Database Design](#17-database-design)
- [18. Infrastructure](#18-infrastructure)
- [19. Security Architecture](#19-security-architecture)
- [20. Integration Patterns](#20-integration-patterns)

### Part IV: Refinement (Phase 4)
- [21. Code Quality Standards](#21-code-quality-standards)
- [22. Performance Optimization](#22-performance-optimization)
- [23. Security Hardening](#23-security-hardening)
- [24. Testing Strategy](#24-testing-strategy)
- [25. Documentation Standards](#25-documentation-standards)
- [26. Review Processes](#26-review-processes)
- [27. Compliance & Audit](#27-compliance--audit)

### Part V: Completion (Phase 5)
- [28. Deployment Readiness](#28-deployment-readiness)
- [29. Production Infrastructure](#29-production-infrastructure)
- [30. Operations & Monitoring](#30-operations--monitoring)
- [31. Disaster Recovery](#31-disaster-recovery)
- [32. Handoff & Knowledge Transfer](#32-handoff--knowledge-transfer)
- [33. Final Validation & Launch](#33-final-validation--launch)

---

# PART I: SPECIFICATION

> **SPARC Phase 1**
> Defines the what and why of the system

---

## 1. Purpose

LLM-Research-Lab serves as the experimental innovation hub within the LLM DevOps ecosystem, providing a dedicated environment for developing, testing, and validating new evaluation methodologies before they graduate into production-grade operational modules. Its primary purpose is to accelerate the evolution of AI quality assurance by enabling researchers and engineers to:

- **Pioneer Novel Evaluation Metrics**: Design and validate custom metrics for model quality, safety, bias detection, cost efficiency, and domain-specific performance criteria that extend beyond standard benchmarks
- **Optimize Model-Selection Strategies**: Experiment with advanced routing algorithms, multi-model comparison frameworks, and context-aware selection logic that can later be operationalized in production systems
- **Establish Reproducible Research Workflows**: Create standardized, version-controlled experimental pipelines that ensure findings can be replicated, audited, and built upon by the broader LLM DevOps community
- **Enable Continuous Metric Evolution**: Provide a feedback loop where insights from production telemetry inform new experimental metrics, which are then validated in the lab before deployment

LLM-Research-Lab exists to prevent stagnation in AI quality standards by maintaining a structured innovation pipeline where experimental work directly feeds operational improvements across the 24+ foundational modules in the LLM DevOps ecosystem.

---

## 2. Scope

LLM-Research-Lab focuses exclusively on the experimental and analytical dimensions of LLM operationalization, maintaining clear boundaries with production-oriented modules.

### In Scope

- **Experiment Tracking**: Comprehensive versioning of experiments including parameters, model configurations, prompt variations, and evaluation results with full lineage tracking
- **Metric Benchmarking**: Comparative analysis frameworks for evaluating model performance across multiple dimensions (accuracy, latency, cost, safety) under controlled conditions
- **Dataset Versioning**: Immutable versioning of evaluation datasets, test cases, and benchmark suites to ensure experimental reproducibility
- **Reproducible Research Workflows**: Declarative pipeline definitions that capture the complete experimental methodology from data preparation through analysis
- **Research Artifact Management**: Storage and organization of experimental outputs including model responses, evaluation scores, statistical analyses, and visualization artifacts
- **Hypothesis Testing Frameworks**: Statistical validation tools for comparing model variants, evaluation methodologies, and metric reliability

### Out of Scope

- **Production Deployment**: Model serving, API endpoints, and customer-facing inference belong to operational deployment modules
- **Real-Time Inference**: Live request routing, production traffic management, and latency-critical serving are handled by runtime modules
- **Model Training**: Fine-tuning, reinforcement learning, and model development activities are managed by dedicated training infrastructure
- **Production Monitoring**: Live telemetry collection, alerting, and operational observability are provided by the telemetry core modules
- **Cost Governance**: Production-level budget enforcement and cost allocation tracking are managed by financial operations modules

LLM-Research-Lab operates as a **research-first environment** where experimental rigor and methodological innovation take precedence over operational concerns like uptime, throughput, or production SLAs. Its outputs inform and enhance operational modules but do not replace them.

---

## 3. Problem Definition

LLM-Research-Lab addresses critical challenges in AI research workflows that impede progress and operational maturity:

### 3.1 Reproducibility Crisis

AI research suffers from a fundamental reproducibility problem. Experiments are often impossible to replicate because critical details are scattered or missing entirely. Environment configurations, model hyperparameters, dataset versions, random seeds, and dependency snapshots frequently go undocumented. Configuration drift between development and testing environments compounds the issue, making it difficult to verify results or build upon previous work. Without reproducibility, research findings lose credibility and collaborative progress stalls.

**LLM-Research-Lab Solution**: Provides a structured environment where every experiment is captured with complete provenance—including model configurations, dataset versions, environment snapshots, and execution parameters—enabling reliable reproduction of any research outcome.

### 3.2 Cross-Model Comparison Difficulty

Fairly comparing different LLMs requires more than running the same prompts. Models have varying tokenization strategies, context window sizes, inference characteristics, and optimization goals. Without standardized benchmarks, consistent evaluation protocols, and controlled testing environments, comparisons become unreliable. Ad-hoc evaluation approaches introduce biases and make it nearly impossible to determine which model genuinely performs better for specific use cases.

**LLM-Research-Lab Solution**: Establishes a unified framework for cross-model evaluation with standardized benchmarks, consistent metrics, and controlled experimental conditions that ensure apples-to-apples comparisons across different LLM architectures and providers.

### 3.3 Metric Evolution Gap

Evaluation metrics in AI research quickly become outdated as models evolve and new capabilities emerge. Traditional metrics like perplexity or BLEU scores fail to capture nuanced aspects of model performance such as reasoning capability, factual accuracy, or ethical alignment. Researchers need a systematic way to develop, validate, and iterate on new evaluation metrics—but existing tools lack the infrastructure to support metric experimentation as a first-class workflow within a DevOps pipeline.

**LLM-Research-Lab Solution**: Treats metrics as evolving research artifacts that can be developed, tested, versioned, and deployed through the same rigorous processes as code, enabling continuous improvement of evaluation methodologies alongside model development.

### 3.4 Research-to-Production Gap

Experimental AI research exists in isolation from production systems. Insights discovered in research environments—such as optimal prompting strategies, effective fine-tuning approaches, or failure mode patterns—rarely translate smoothly into operational improvements. The disconnect stems from different tooling, divergent workflows, and lack of integration between research and production infrastructure. This gap wastes valuable research findings and slows the deployment of improvements.

**LLM-Research-Lab Solution**: Bridges research and production by integrating directly with the LLM DevOps ecosystem, creating a seamless path from experimental findings to operational deployment through shared infrastructure, consistent tooling, and unified data flows.

### 3.5 Dataset Management Complexity

Research datasets require sophisticated management practices rarely found in experimental settings. Proper versioning ensures experiments can reference exact dataset snapshots. Lineage tracking reveals how datasets were derived, transformed, and augmented. Secure handling protects sensitive data and maintains compliance with privacy regulations. Integration with broader data governance frameworks ensures research practices align with organizational policies. Without these capabilities, dataset management becomes a manual, error-prone bottleneck that introduces inconsistencies and security risks.

**LLM-Research-Lab Solution**: Provides comprehensive dataset management with automatic versioning, complete lineage tracking, secure storage with access controls, and integration with data governance systems—treating research datasets with the same rigor as production data assets.

---

## 4. Objectives

LLM-Research-Lab serves as the experimental and analytical foundation of the LLM DevOps ecosystem, enabling rigorous evaluation, metric development, and reproducible research workflows for large language models.

### 4.1 Primary Objectives

#### 4.1.1 Experiment Tracking
- Capture comprehensive experiment metadata including hyperparameters, model configurations, and runtime environments
- Track performance metrics across multiple evaluation runs with temporal versioning
- Store and version experiment artifacts (checkpoints, outputs, logs) with full lineage
- Maintain immutable records of environment state (dependencies, hardware, system configurations)
- Enable comparison and analysis across experiment runs with queryable metadata

#### 4.1.2 Metric Benchmarking
- Provide a standardized framework for defining, implementing, and testing custom evaluation metrics
- Support comparative analysis of evaluation methodologies across diverse model architectures
- Enable side-by-side metric performance visualization and statistical validation
- Facilitate metric composition and aggregation strategies for multi-dimensional evaluation
- Maintain a versioned registry of proven evaluation metrics with documented reliability characteristics

#### 4.1.3 Dataset Versioning
- Implement content-addressable storage for research datasets with cryptographic integrity verification
- Track dataset lineage including transformations, splits, and preprocessing operations
- Support incremental versioning with delta-based storage optimization
- Enable reproducible dataset snapshots tied to specific experiment runs
- Provide dataset provenance tracking from raw sources through final evaluation sets

#### 4.1.4 DevOps Integration
- Seamlessly integrate with CI/CD pipelines for automated evaluation on code changes
- Support iterative experimentation workflows with rapid feedback loops
- Enable continuous benchmarking against baseline models and metrics
- Provide APIs for programmatic experiment submission and result retrieval
- Generate deployment-ready evaluation reports for production readiness assessment

### 4.2 Secondary Objectives

- **Collaborative Research**: Enable distributed teams to share experiments, datasets, and evaluation methodologies through centralized repositories with access control and collaboration features
- **A/B Testing for Evaluation**: Support controlled experiments comparing different evaluation strategies, metric implementations, and assessment methodologies to identify optimal approaches
- **Statistical Analysis Tools**: Provide built-in statistical significance testing, confidence interval calculation, and power analysis for rigorous experiment comparison and result validation
- **Reproducibility Reports**: Automatically generate comprehensive reproducibility documentation including environment specifications, dependency manifests, data provenance, and execution traces
- **Knowledge Sharing**: Maintain searchable experiment catalogs with rich metadata, enabling teams to discover relevant prior work, reuse proven evaluation strategies, and build on existing research

---

## 5. Users & Roles

### 5.1 Primary Users

#### 5.1.1 AI Researchers
- **Goals**: Develop and validate new evaluation metrics, conduct reproducible experiments, advance the state of LLM evaluation methodology
- **Activities**: Design experiments, analyze results, iterate on metric definitions, publish findings, collaborate with peers
- **Needs**: Experiment tracking with full provenance, statistical analysis tools, reproducibility features, collaborative workflows, export capabilities for publications

#### 5.1.2 Data Scientists
- **Goals**: Compare model performance across dimensions, optimize model selection for specific use cases, identify performance patterns
- **Activities**: Run benchmarks, analyze metrics across models, tune evaluation parameters, generate comparative reports
- **Needs**: Visualization dashboards, dataset management tools, metric libraries, model comparison frameworks, automated reporting

#### 5.1.3 MLOps Engineers
- **Goals**: Integrate research findings into production pipelines, operationalize proven evaluation methods, maintain research infrastructure
- **Activities**: Set up experiment infrastructure, automate benchmarks, manage compute resources, deploy validated metrics to production
- **Needs**: Pipeline integration APIs, resource management tools, automation frameworks, deployment workflows, monitoring capabilities

### 5.2 Secondary Users

- **Research Managers**: Track project progress across experiments, allocate compute and personnel resources, review team outputs, prioritize research directions
- **Compliance Officers**: Audit experiment trails for regulatory requirements, ensure data governance compliance, verify proper handling of sensitive datasets
- **External Collaborators**: Access shared experiments with appropriate permissions, contribute to open research initiatives, validate and extend published findings

---

## 6. Dependencies

### 6.1 Internal Dependencies (LLM DevOps Ecosystem)

The LLM-Research-Lab integrates with four sibling modules within the LLM DevOps ecosystem to provide a complete research-to-production pipeline.

#### 6.1.1 LLM-Test-Bench
- **Purpose**: Provides standardized benchmarking infrastructure for validating model performance and experimental results
- **Integration Points**:
  - Research Lab submits experimental models and configurations to Test-Bench for automated evaluation
  - Test-Bench returns performance metrics (latency, throughput, accuracy) against established baselines
  - Research Lab consumes benchmark definitions and test suites from Test-Bench to ensure consistency
- **Data Flow**:
  - **Outbound**: Experiment identifiers, model endpoints/artifacts, test parameters, hypothesis metadata
  - **Inbound**: Benchmark results, performance baselines, regression test outcomes, comparative analysis
- **API Contract**: REST endpoints for submitting benchmark jobs and retrieving results; event-driven notifications for long-running tests

#### 6.1.2 LLM-Analytics-Hub
- **Purpose**: Provides visualization dashboards and analytical reporting for experiment results and performance trends
- **Integration Points**:
  - Research Lab streams real-time experiment metrics to Analytics Hub during evaluation runs
  - Analytics Hub generates comparative visualizations across multiple experiments
  - Research Lab queries Analytics Hub for historical performance trends and anomaly detection
- **Data Flow**:
  - **Outbound**: Time-series metrics (evaluation scores, resource utilization), experiment metadata (hyperparameters, dataset versions), event logs (experiment start/stop, checkpoints)
  - **Inbound**: Dashboard URLs, aggregated statistics, trend analysis reports, alert notifications
- **API Contract**: Push metrics via time-series ingestion API; pull visualizations and reports via GraphQL queries

#### 6.1.3 LLM-Registry
- **Purpose**: Centralized registry for versioned models, metrics definitions, datasets, and research artifacts
- **Integration Points**:
  - Research Lab registers experimental models with version tags and metadata
  - Registry provides artifact storage and retrieval with content-addressable checksums
  - Research Lab publishes custom metric definitions for reuse across experiments
- **Data Flow**:
  - **Outbound**: Model binaries/checkpoints, metric definition schemas (JSON/YAML), dataset manifests, experiment lineage graphs
  - **Inbound**: Artifact URIs, version histories, dependency graphs, schema validation results
- **API Contract**: gRPC for binary uploads; REST for metadata queries; webhooks for version change notifications

#### 6.1.4 LLM-Data-Vault
- **Purpose**: Secure storage and governance layer for evaluation datasets with access control and lineage tracking
- **Integration Points**:
  - Research Lab requests dataset access with scoped permissions (read-only, time-limited)
  - Data Vault enforces data governance policies (PII handling, retention limits, usage tracking)
  - Research Lab references dataset versions immutably for reproducibility
- **Data Flow**:
  - **Outbound**: Access requests with justification, usage telemetry (rows read, features accessed), lineage metadata
  - **Inbound**: Dataset references (versioned URIs), streaming data access tokens, governance policy constraints
- **API Contract**: Token-based authentication; streaming APIs for large datasets; audit logs for compliance

### 6.2 External Dependencies

The LLM-Research-Lab requires the following external infrastructure components:

#### Data Layer
- **PostgreSQL** (v13+): Stores experiment metadata, user configurations, and relational data
- **ClickHouse** (v22+): High-performance time-series storage for evaluation metrics and telemetry

#### Storage Layer
- **S3-Compatible Object Storage** (MinIO/AWS S3/GCS): Durable storage for model artifacts and large files
  - Versioning enabled for audit trail
  - Lifecycle policies for cost optimization

#### Compute Layer
- **Kubernetes** (v1.24+): Container orchestration for distributed evaluation and experimentation
  - GPU node pools for model inference workloads
  - CPU node pools for data preprocessing and metrics computation

#### Messaging Layer
- **Message Queue** (RabbitMQ/Redis Streams/Apache Kafka): Asynchronous job processing and event distribution
  - At-least-once delivery guarantee
  - Dead-letter queues for failed jobs with retry logic

### 6.3 Dependency Management

- **Version Pinning**: All internal LLM DevOps modules must use semantic versioning with API compatibility guarantees
- **Circuit Breakers**: Research Lab implements fallback behavior if Analytics Hub or Test-Bench are unavailable
- **Health Checks**: Startup probes verify connectivity to Registry and Data Vault before accepting experiment submissions
- **Feature Flags**: Experimental integrations can be toggled without redeployment

---

## 7. Design Principles

### 7.1 Reproducibility First
- Every experiment must be fully reproducible from its recorded state
- Environment, data, and configuration are immutably captured
- Reproducibility is enforced, not optional
- Version pinning and artifact tracking are built into the core workflow
- Random seeds, dependency versions, and system configurations are automatically recorded
- Experiments can be re-run months or years later with identical results

### 7.2 Transparency
- All experiment parameters, decisions, and results are visible and auditable
- Clear lineage from data sources through final results
- No hidden state or implicit configurations
- Every transformation and decision point is logged with full context
- Results are traceable back to exact code versions and input data
- Audit trails enable verification of experimental integrity

### 7.3 Modular Experimentation
- Components (metrics, datasets, evaluators) are interchangeable
- Experiments compose from reusable building blocks
- Easy to swap implementations without rewriting pipelines
- Well-defined interfaces enable mix-and-match composition
- Custom components integrate seamlessly alongside built-in ones
- Modularity reduces coupling and accelerates iteration

### 7.4 Scientific Rigor
- Statistical significance is first-class
- Results include confidence intervals and uncertainty quantification
- Multiple trials and cross-validation are standard practice
- Automatic detection of insufficient sample sizes
- Built-in support for hypothesis testing and effect size calculation
- Encourages proper experimental design over p-hacking

### 7.5 Integration by Design
- Built to integrate with the broader LLM DevOps ecosystem
- Standard interfaces for data exchange
- Compatible with existing CI/CD and MLOps workflows
- Export results to common formats (JSON, CSV, MLflow, Weights & Biases)
- Works alongside deployment pipelines and monitoring systems
- Designed for both local development and production environments

### 7.6 Fail-Safe Defaults
- Sensible defaults that promote best practices
- Guard rails against common experimental pitfalls
- Warnings for potential reproducibility issues
- Automatic detection of data leakage between train/test sets
- Prompts for missing metadata that could impact reproducibility
- Conservative settings prevent accidental resource exhaustion or cost overruns

---

## 8. Success Metrics

### 8.1 Scientific Validity

| Metric | Target | Measurement Method |
|--------|--------|-------------------|
| **Reproducibility Rate** | >95% | Automated replay of randomly sampled experiments against stored artifacts |
| **Statistical Rigor Score** | 100% | Automated analysis of experiment configurations for hypothesis testing, confidence intervals, and multiple comparison corrections |
| **Peer Validation Rate** | >70% | Acceptance rate of research outputs in internal/external review processes |

### 8.2 Interoperability

| Metric | Target | Measurement Method |
|--------|--------|-------------------|
| **Integration Coverage** | 100% | Automated integration tests validating bidirectional data flow with all 4 core dependencies |
| **API Compatibility** | 0 violations | Static analysis against LLM DevOps API standards |
| **Data Exchange Success Rate** | >99.5% | Transaction logs tracking artifact transfers between modules |
| **Pipeline Integration** | 95% automated | Successful automated experiment execution in CI/CD environments |

### 8.3 Extensibility

| Metric | Target | Measurement Method |
|--------|--------|-------------------|
| **Plugin Adoption** | 25+ in year 1 | Registry count of contributed custom metrics, evaluators, and workflows |
| **Time to New Metric** | <5 days (simple), <15 days (complex) | Development cycle tracking from proposal to deployment |
| **Component Reuse Rate** | >60% | Dependency analysis showing experiments using 2+ shared components |
| **External Contribution Rate** | 20% | Pull requests and contributions from outside core team |

### 8.4 Operational Metrics

| Metric | Target | Measurement Method |
|--------|--------|-------------------|
| **Experiment Throughput** | 200+ experiments/week | Completed experiment runs logged in Analytics-Hub |
| **Mean Time to Results** | <2 hours (standard), <24 hours (large-scale) | Timestamp differential between submission and completion |
| **Storage Efficiency** | 3:1 compression ratio | Logical to physical storage ratio post-deduplication |
| **User Adoption** | 50+ active users in year 1 | Unique users executing experiments per month |

---

---

# PART II: PSEUDOCODE

> **SPARC Phase 2**
> Defines the algorithmic design and data structures

---

## 1. Core Data Models

### 1.1 Primary Entity Types

```rust
//! Core domain entities for LLM-Research-Lab
//! All types are designed for serialization, validation, and database persistence

use chrono::{DateTime, Utc};
use rust_decimal::Decimal;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use uuid::Uuid;

/// Unique identifier types with strong typing to prevent mixing IDs
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub struct ExperimentId(Uuid);

#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub struct RunId(Uuid);

#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub struct DatasetId(Uuid);

#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub struct DatasetVersionId(Uuid);

#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub struct MetricId(Uuid);

#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub struct ArtifactId(Uuid);

#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub struct UserId(Uuid);

#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub struct WorkflowId(Uuid);

#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub struct WorkflowRunId(Uuid);

/// Content-addressable hash for artifact integrity
#[derive(Debug, Clone, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub struct ContentHash(String); // SHA-256 hex string

impl ContentHash {
    pub fn from_bytes(data: &[u8]) -> Self {
        use sha2::{Sha256, Digest};
        let mut hasher = Sha256::new();
        hasher.update(data);
        Self(hex::encode(hasher.finalize()))
    }

    pub fn from_stream<R: std::io::Read>(reader: &mut R) -> std::io::Result<Self> {
        use sha2::{Sha256, Digest};
        let mut hasher = Sha256::new();
        let mut buffer = [0u8; 8192];
        loop {
            let bytes_read = reader.read(&mut buffer)?;
            if bytes_read == 0 { break; }
            hasher.update(&buffer[..bytes_read]);
        }
        Ok(Self(hex::encode(hasher.finalize())))
    }

    pub fn as_str(&self) -> &str {
        &self.0
    }
}

/// Semantic version for metrics, datasets, and artifacts
#[derive(Debug, Clone, PartialEq, Eq, PartialOrd, Ord, Serialize, Deserialize)]
pub struct SemanticVersion {
    pub major: u32,
    pub minor: u32,
    pub patch: u32,
    pub prerelease: Option<String>,
    pub build_metadata: Option<String>,
}

impl SemanticVersion {
    pub fn new(major: u32, minor: u32, patch: u32) -> Self {
        Self {
            major,
            minor,
            patch,
            prerelease: None,
            build_metadata: None,
        }
    }

    pub fn is_compatible_with(&self, other: &Self) -> bool {
        // Major version must match for compatibility
        self.major == other.major
    }

    pub fn increment_patch(&self) -> Self {
        Self {
            major: self.major,
            minor: self.minor,
            patch: self.patch + 1,
            prerelease: None,
            build_metadata: None,
        }
    }

    pub fn increment_minor(&self) -> Self {
        Self {
            major: self.major,
            minor: self.minor + 1,
            patch: 0,
            prerelease: None,
            build_metadata: None,
        }
    }

    pub fn increment_major(&self) -> Self {
        Self {
            major: self.major + 1,
            minor: 0,
            patch: 0,
            prerelease: None,
            build_metadata: None,
        }
    }
}

impl std::fmt::Display for SemanticVersion {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "{}.{}.{}", self.major, self.minor, self.patch)?;
        if let Some(ref pre) = self.prerelease {
            write!(f, "-{}", pre)?;
        }
        if let Some(ref build) = self.build_metadata {
            write!(f, "+{}", build)?;
        }
        Ok(())
    }
}
```

### 1.2 Experiment Domain

```rust
/// Core experiment entity representing a research initiative
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Experiment {
    pub id: ExperimentId,
    pub name: String,
    pub description: Option<String>,
    pub hypothesis: Option<String>,
    pub owner_id: UserId,
    pub collaborators: Vec<UserId>,
    pub tags: Vec<String>,
    pub status: ExperimentStatus,
    pub config: ExperimentConfig,
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
    pub archived_at: Option<DateTime<Utc>>,
    pub metadata: HashMap<String, serde_json::Value>,
}

/// Experiment lifecycle status
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ExperimentStatus {
    Draft,
    Active,
    Paused,
    Completed,
    Archived,
    Failed,
}

impl ExperimentStatus {
    pub fn can_transition_to(&self, target: ExperimentStatus) -> bool {
        use ExperimentStatus::*;
        match (self, target) {
            (Draft, Active) => true,
            (Draft, Archived) => true,
            (Active, Paused) => true,
            (Active, Completed) => true,
            (Active, Failed) => true,
            (Paused, Active) => true,
            (Paused, Archived) => true,
            (Completed, Archived) => true,
            (Failed, Archived) => true,
            _ => false,
        }
    }
}

/// Configuration for experiment execution
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ExperimentConfig {
    pub model_configs: Vec<ModelConfig>,
    pub dataset_refs: Vec<DatasetRef>,
    pub metric_configs: Vec<MetricConfig>,
    pub parameters: ExperimentParameters,
    pub resource_requirements: ResourceRequirements,
    pub reproducibility_settings: ReproducibilitySettings,
}

/// Model configuration for evaluation
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelConfig {
    pub model_id: String,
    pub provider: ModelProvider,
    pub variant: Option<String>,
    pub endpoint: Option<String>,
    pub parameters: ModelParameters,
    pub credentials_ref: Option<String>, // Reference to secrets manager
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ModelProvider {
    OpenAI,
    Anthropic,
    Google,
    Cohere,
    HuggingFace,
    AzureOpenAI,
    AWS Bedrock,
    Custom { name: String },
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelParameters {
    pub temperature: Option<f64>,
    pub max_tokens: Option<u32>,
    pub top_p: Option<f64>,
    pub top_k: Option<u32>,
    pub frequency_penalty: Option<f64>,
    pub presence_penalty: Option<f64>,
    pub stop_sequences: Vec<String>,
    pub seed: Option<u64>,
    pub custom: HashMap<String, serde_json::Value>,
}

/// Reference to a versioned dataset
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DatasetRef {
    pub dataset_id: DatasetId,
    pub version: DatasetVersionSelector,
    pub split: Option<DataSplit>,
    pub sample_config: Option<SampleConfig>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum DatasetVersionSelector {
    Latest,
    Specific(DatasetVersionId),
    Tag(String),
    ContentHash(ContentHash),
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum DataSplit {
    Train,
    Validation,
    Test,
    Custom(String),
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SampleConfig {
    pub strategy: SampleStrategy,
    pub size: SampleSize,
    pub seed: Option<u64>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum SampleStrategy {
    Random,
    Stratified { column: String },
    Systematic { interval: usize },
    Reservoir,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum SampleSize {
    Count(usize),
    Percentage(f64),
    All,
}

/// Configuration for metric evaluation
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MetricConfig {
    pub metric_id: MetricId,
    pub version: SemanticVersion,
    pub parameters: HashMap<String, serde_json::Value>,
    pub weight: Option<f64>, // For composite scores
    pub threshold: Option<MetricThreshold>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MetricThreshold {
    pub warning: Option<f64>,
    pub critical: Option<f64>,
    pub direction: ThresholdDirection,
}

#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ThresholdDirection {
    HigherIsBetter,
    LowerIsBetter,
}

/// Experiment-level parameters
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ExperimentParameters {
    pub hyperparameters: HashMap<String, ParameterValue>,
    pub search_space: Option<SearchSpace>,
    pub optimization_target: Option<OptimizationTarget>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(untagged)]
pub enum ParameterValue {
    Integer(i64),
    Float(f64),
    String(String),
    Boolean(bool),
    List(Vec<ParameterValue>),
    Map(HashMap<String, ParameterValue>),
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SearchSpace {
    pub parameters: HashMap<String, ParameterSearchConfig>,
    pub strategy: SearchStrategy,
    pub max_trials: usize,
    pub early_stopping: Option<EarlyStoppingConfig>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ParameterSearchConfig {
    Categorical { values: Vec<serde_json::Value> },
    Uniform { low: f64, high: f64 },
    LogUniform { low: f64, high: f64 },
    IntUniform { low: i64, high: i64 },
    Normal { mean: f64, std: f64 },
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum SearchStrategy {
    Grid,
    Random,
    Bayesian,
    Hyperband,
    Population Based,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EarlyStoppingConfig {
    pub metric: String,
    pub patience: usize,
    pub min_delta: f64,
    pub mode: ThresholdDirection,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OptimizationTarget {
    pub metric: String,
    pub direction: ThresholdDirection,
    pub constraints: Vec<OptimizationConstraint>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OptimizationConstraint {
    pub metric: String,
    pub operator: ConstraintOperator,
    pub value: f64,
}

#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ConstraintOperator {
    LessThan,
    LessThanOrEqual,
    GreaterThan,
    GreaterThanOrEqual,
    Equal,
}

/// Resource requirements for experiment execution
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ResourceRequirements {
    pub compute: ComputeRequirements,
    pub storage: StorageRequirements,
    pub timeout: Option<std::time::Duration>,
    pub priority: ExecutionPriority,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ComputeRequirements {
    pub cpu_cores: Option<f64>,
    pub memory_mb: Option<u64>,
    pub gpu: Option<GpuRequirements>,
    pub node_selector: HashMap<String, String>,
    pub tolerations: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GpuRequirements {
    pub count: u32,
    pub gpu_type: Option<String>, // e.g., "nvidia-a100", "nvidia-v100"
    pub memory_mb: Option<u64>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StorageRequirements {
    pub scratch_space_mb: u64,
    pub artifact_retention_days: Option<u32>,
    pub checkpoint_frequency: Option<std::time::Duration>,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ExecutionPriority {
    Low,
    Normal,
    High,
    Critical,
}

/// Reproducibility configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ReproducibilitySettings {
    pub capture_environment: bool,
    pub pin_dependencies: bool,
    pub deterministic_mode: bool,
    pub global_seed: Option<u64>,
    pub checkpoint_artifacts: bool,
    pub record_system_info: bool,
}

impl Default for ReproducibilitySettings {
    fn default() -> Self {
        Self {
            capture_environment: true,
            pin_dependencies: true,
            deterministic_mode: true,
            global_seed: None,
            checkpoint_artifacts: true,
            record_system_info: true,
        }
    }
}
```

### 1.3 Experiment Run Domain

```rust
/// Individual execution of an experiment
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ExperimentRun {
    pub id: RunId,
    pub experiment_id: ExperimentId,
    pub run_number: u64,
    pub name: Option<String>,
    pub status: RunStatus,
    pub parameters: HashMap<String, ParameterValue>,
    pub environment: EnvironmentSnapshot,
    pub metrics: RunMetrics,
    pub artifacts: Vec<ArtifactRef>,
    pub logs: LogSummary,
    pub parent_run_id: Option<RunId>, // For nested/child runs
    pub tags: Vec<String>,
    pub started_at: Option<DateTime<Utc>>,
    pub ended_at: Option<DateTime<Utc>>,
    pub created_at: DateTime<Utc>,
    pub created_by: UserId,
    pub error: Option<RunError>,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum RunStatus {
    Pending,
    Queued,
    Running,
    Completed,
    Failed,
    Cancelled,
    TimedOut,
}

impl RunStatus {
    pub fn is_terminal(&self) -> bool {
        matches!(
            self,
            RunStatus::Completed | RunStatus::Failed | RunStatus::Cancelled | RunStatus::TimedOut
        )
    }

    pub fn is_successful(&self) -> bool {
        matches!(self, RunStatus::Completed)
    }
}

/// Complete environment snapshot for reproducibility
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EnvironmentSnapshot {
    pub os: OsInfo,
    pub hardware: HardwareInfo,
    pub runtime: RuntimeInfo,
    pub dependencies: DependencyManifest,
    pub environment_variables: HashMap<String, String>, // Filtered for safety
    pub git_state: Option<GitState>,
    pub container_info: Option<ContainerInfo>,
    pub captured_at: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OsInfo {
    pub name: String,
    pub version: String,
    pub architecture: String,
    pub kernel_version: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct HardwareInfo {
    pub cpu_model: String,
    pub cpu_cores: u32,
    pub cpu_threads: u32,
    pub memory_total_mb: u64,
    pub gpus: Vec<GpuInfo>,
    pub hostname_hash: String, // Hashed for privacy
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GpuInfo {
    pub name: String,
    pub memory_mb: u64,
    pub driver_version: String,
    pub cuda_version: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RuntimeInfo {
    pub rust_version: Option<String>,
    pub python_version: Option<String>,
    pub cuda_version: Option<String>,
    pub custom_runtimes: HashMap<String, String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DependencyManifest {
    pub format: DependencyFormat,
    pub content_hash: ContentHash,
    pub packages: Vec<PackageInfo>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum DependencyFormat {
    CargoLock,
    PipFreeze,
    CondaEnvironment,
    NpmPackageLock,
    Custom(String),
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PackageInfo {
    pub name: String,
    pub version: String,
    pub source: Option<String>,
    pub checksum: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GitState {
    pub repository_url: Option<String>,
    pub branch: String,
    pub commit_hash: String,
    pub is_dirty: bool,
    pub uncommitted_changes: Option<String>, // Diff summary
    pub remote_tracking: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ContainerInfo {
    pub image: String,
    pub image_digest: String,
    pub runtime: String, // docker, containerd, etc.
    pub resource_limits: Option<ContainerResourceLimits>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ContainerResourceLimits {
    pub cpu_limit: Option<f64>,
    pub memory_limit_mb: Option<u64>,
    pub gpu_count: Option<u32>,
}

/// Collected metrics during a run
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RunMetrics {
    pub scalars: HashMap<String, ScalarMetricSeries>,
    pub distributions: HashMap<String, DistributionMetric>,
    pub confusion_matrices: HashMap<String, ConfusionMatrix>,
    pub custom: HashMap<String, serde_json::Value>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ScalarMetricSeries {
    pub values: Vec<ScalarDataPoint>,
    pub aggregations: MetricAggregations,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ScalarDataPoint {
    pub value: f64,
    pub step: Option<u64>,
    pub timestamp: DateTime<Utc>,
    pub context: HashMap<String, String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MetricAggregations {
    pub min: f64,
    pub max: f64,
    pub mean: f64,
    pub std: f64,
    pub median: f64,
    pub p95: f64,
    pub p99: f64,
    pub count: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DistributionMetric {
    pub histogram: Histogram,
    pub summary: DistributionSummary,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Histogram {
    pub bucket_boundaries: Vec<f64>,
    pub bucket_counts: Vec<u64>,
    pub sum: f64,
    pub count: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DistributionSummary {
    pub min: f64,
    pub max: f64,
    pub mean: f64,
    pub std: f64,
    pub percentiles: HashMap<String, f64>, // "p50", "p90", "p95", "p99"
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ConfusionMatrix {
    pub labels: Vec<String>,
    pub matrix: Vec<Vec<u64>>,
    pub normalized: Option<Vec<Vec<f64>>>,
}

/// Reference to stored artifact
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ArtifactRef {
    pub id: ArtifactId,
    pub name: String,
    pub artifact_type: ArtifactType,
    pub content_hash: ContentHash,
    pub size_bytes: u64,
    pub storage_uri: String,
    pub metadata: HashMap<String, String>,
    pub created_at: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ArtifactType {
    Model,
    Checkpoint,
    Dataset,
    Visualization,
    Log,
    Config,
    Report,
    Custom(String),
}

/// Log summary for a run
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LogSummary {
    pub total_lines: u64,
    pub error_count: u64,
    pub warning_count: u64,
    pub log_uri: String,
    pub highlights: Vec<LogHighlight>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LogHighlight {
    pub level: LogLevel,
    pub message: String,
    pub timestamp: DateTime<Utc>,
    pub line_number: u64,
}

#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum LogLevel {
    Trace,
    Debug,
    Info,
    Warning,
    Error,
    Fatal,
}

/// Error information for failed runs
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RunError {
    pub error_type: String,
    pub message: String,
    pub stack_trace: Option<String>,
    pub occurred_at: DateTime<Utc>,
    pub recoverable: bool,
    pub context: HashMap<String, String>,
}
```

---

## 2. Experiment Tracking System

### 2.1 Experiment Tracker Core

```rust
//! Experiment tracking system providing full lifecycle management
//! with reproducibility guarantees and artifact persistence

use async_trait::async_trait;
use std::sync::Arc;
use tokio::sync::RwLock;

/// Main experiment tracker service
pub struct ExperimentTracker {
    experiment_store: Arc<dyn ExperimentStore>,
    run_store: Arc<dyn RunStore>,
    artifact_store: Arc<dyn ArtifactStore>,
    metric_store: Arc<dyn MetricStore>,
    environment_capture: Arc<EnvironmentCapture>,
    lineage_tracker: Arc<LineageTracker>,
    event_publisher: Arc<dyn EventPublisher>,
    config: TrackerConfig,
}

#[derive(Debug, Clone)]
pub struct TrackerConfig {
    pub auto_capture_environment: bool,
    pub auto_checkpoint: bool,
    pub checkpoint_interval: std::time::Duration,
    pub metric_buffer_size: usize,
    pub max_artifact_size_bytes: u64,
    pub compression_enabled: bool,
}

impl Default for TrackerConfig {
    fn default() -> Self {
        Self {
            auto_capture_environment: true,
            auto_checkpoint: true,
            checkpoint_interval: std::time::Duration::from_secs(300),
            metric_buffer_size: 1000,
            max_artifact_size_bytes: 10 * 1024 * 1024 * 1024, // 10GB
            compression_enabled: true,
        }
    }
}

impl ExperimentTracker {
    /// Create a new experiment tracker with the given dependencies
    pub fn new(
        experiment_store: Arc<dyn ExperimentStore>,
        run_store: Arc<dyn RunStore>,
        artifact_store: Arc<dyn ArtifactStore>,
        metric_store: Arc<dyn MetricStore>,
        environment_capture: Arc<EnvironmentCapture>,
        lineage_tracker: Arc<LineageTracker>,
        event_publisher: Arc<dyn EventPublisher>,
        config: TrackerConfig,
    ) -> Self {
        Self {
            experiment_store,
            run_store,
            artifact_store,
            metric_store,
            environment_capture,
            lineage_tracker,
            event_publisher,
            config,
        }
    }

    /// Create a new experiment
    pub async fn create_experiment(
        &self,
        request: CreateExperimentRequest,
    ) -> Result<Experiment, TrackerError> {
        // Validate request
        self.validate_experiment_request(&request)?;

        // Generate unique ID
        let id = ExperimentId(Uuid::new_v4());
        let now = Utc::now();

        // Build experiment entity
        let experiment = Experiment {
            id,
            name: request.name,
            description: request.description,
            hypothesis: request.hypothesis,
            owner_id: request.owner_id,
            collaborators: request.collaborators.unwrap_or_default(),
            tags: request.tags.unwrap_or_default(),
            status: ExperimentStatus::Draft,
            config: request.config,
            created_at: now,
            updated_at: now,
            archived_at: None,
            metadata: request.metadata.unwrap_or_default(),
        };

        // Persist experiment
        self.experiment_store.save(&experiment).await?;

        // Record lineage
        self.lineage_tracker
            .record_experiment_creation(id, &experiment)
            .await?;

        // Publish event
        self.event_publisher
            .publish(Event::ExperimentCreated {
                experiment_id: id,
                owner_id: experiment.owner_id,
                timestamp: now,
            })
            .await?;

        Ok(experiment)
    }

    /// Start a new run for an experiment
    pub async fn start_run(
        &self,
        experiment_id: ExperimentId,
        request: StartRunRequest,
    ) -> Result<ExperimentRun, TrackerError> {
        // Verify experiment exists and is active
        let experiment = self.experiment_store.get(experiment_id).await?;
        if experiment.status != ExperimentStatus::Active {
            return Err(TrackerError::InvalidState {
                message: format!(
                    "Experiment must be active to start runs, current status: {:?}",
                    experiment.status
                ),
            });
        }

        // Generate run ID and number
        let run_id = RunId(Uuid::new_v4());
        let run_number = self.run_store.get_next_run_number(experiment_id).await?;
        let now = Utc::now();

        // Capture environment if enabled
        let environment = if self.config.auto_capture_environment {
            self.environment_capture.capture().await?
        } else {
            request.environment.unwrap_or_else(|| {
                EnvironmentSnapshot::minimal()
            })
        };

        // Build run entity
        let run = ExperimentRun {
            id: run_id,
            experiment_id,
            run_number,
            name: request.name,
            status: RunStatus::Running,
            parameters: request.parameters,
            environment,
            metrics: RunMetrics::default(),
            artifacts: Vec::new(),
            logs: LogSummary::default(),
            parent_run_id: request.parent_run_id,
            tags: request.tags.unwrap_or_default(),
            started_at: Some(now),
            ended_at: None,
            created_at: now,
            created_by: request.user_id,
            error: None,
        };

        // Persist run
        self.run_store.save(&run).await?;

        // Record lineage
        self.lineage_tracker
            .record_run_start(run_id, experiment_id, &run)
            .await?;

        // Publish event
        self.event_publisher
            .publish(Event::RunStarted {
                run_id,
                experiment_id,
                timestamp: now,
            })
            .await?;

        Ok(run)
    }

    /// Log scalar metrics for a run
    pub async fn log_metrics(
        &self,
        run_id: RunId,
        metrics: Vec<MetricEntry>,
    ) -> Result<(), TrackerError> {
        // Validate run is active
        let run = self.run_store.get(run_id).await?;
        if run.status != RunStatus::Running {
            return Err(TrackerError::InvalidState {
                message: "Cannot log metrics to a non-running run".to_string(),
            });
        }

        let timestamp = Utc::now();

        // Process each metric
        for entry in metrics {
            let data_point = ScalarDataPoint {
                value: entry.value,
                step: entry.step,
                timestamp,
                context: entry.context.unwrap_or_default(),
            };

            self.metric_store
                .append_scalar(run_id, &entry.name, data_point)
                .await?;
        }

        // Publish metrics event (batched)
        self.event_publisher
            .publish(Event::MetricsLogged {
                run_id,
                count: metrics.len(),
                timestamp,
            })
            .await?;

        Ok(())
    }

    /// Log an artifact for a run
    pub async fn log_artifact(
        &self,
        run_id: RunId,
        request: LogArtifactRequest,
    ) -> Result<ArtifactRef, TrackerError> {
        // Validate run is active
        let run = self.run_store.get(run_id).await?;
        if run.status != RunStatus::Running {
            return Err(TrackerError::InvalidState {
                message: "Cannot log artifacts to a non-running run".to_string(),
            });
        }

        // Validate artifact size
        if request.data.len() as u64 > self.config.max_artifact_size_bytes {
            return Err(TrackerError::ArtifactTooLarge {
                size: request.data.len() as u64,
                max_size: self.config.max_artifact_size_bytes,
            });
        }

        // Compute content hash
        let content_hash = ContentHash::from_bytes(&request.data);

        // Compress if enabled
        let (storage_data, compressed) = if self.config.compression_enabled {
            (compress_data(&request.data)?, true)
        } else {
            (request.data.clone(), false)
        };

        // Store artifact
        let artifact_id = ArtifactId(Uuid::new_v4());
        let storage_uri = self.artifact_store
            .store(artifact_id, &storage_data, compressed)
            .await?;

        let now = Utc::now();
        let artifact_ref = ArtifactRef {
            id: artifact_id,
            name: request.name,
            artifact_type: request.artifact_type,
            content_hash,
            size_bytes: request.data.len() as u64,
            storage_uri,
            metadata: request.metadata.unwrap_or_default(),
            created_at: now,
        };

        // Update run with artifact reference
        self.run_store
            .add_artifact(run_id, artifact_ref.clone())
            .await?;

        // Record lineage
        self.lineage_tracker
            .record_artifact(run_id, &artifact_ref)
            .await?;

        // Publish event
        self.event_publisher
            .publish(Event::ArtifactLogged {
                run_id,
                artifact_id,
                artifact_type: artifact_ref.artifact_type.clone(),
                timestamp: now,
            })
            .await?;

        Ok(artifact_ref)
    }

    /// Complete a run successfully
    pub async fn complete_run(
        &self,
        run_id: RunId,
        final_metrics: Option<HashMap<String, f64>>,
    ) -> Result<ExperimentRun, TrackerError> {
        let mut run = self.run_store.get(run_id).await?;

        if run.status != RunStatus::Running {
            return Err(TrackerError::InvalidState {
                message: format!(
                    "Run must be running to complete, current status: {:?}",
                    run.status
                ),
            });
        }

        let now = Utc::now();
        run.status = RunStatus::Completed;
        run.ended_at = Some(now);

        // Log final metrics if provided
        if let Some(metrics) = final_metrics {
            let entries: Vec<_> = metrics
                .into_iter()
                .map(|(name, value)| MetricEntry {
                    name,
                    value,
                    step: None,
                    context: None,
                })
                .collect();

            // Don't fail completion if metric logging fails
            if let Err(e) = self.log_metrics_internal(run_id, entries).await {
                tracing::warn!("Failed to log final metrics: {}", e);
            }
        }

        // Compute final aggregations
        run.metrics = self.metric_store.compute_aggregations(run_id).await?;

        // Persist updated run
        self.run_store.save(&run).await?;

        // Record lineage
        self.lineage_tracker.record_run_completion(run_id, &run).await?;

        // Publish event
        self.event_publisher
            .publish(Event::RunCompleted {
                run_id,
                experiment_id: run.experiment_id,
                status: RunStatus::Completed,
                duration: run.ended_at.unwrap() - run.started_at.unwrap(),
                timestamp: now,
            })
            .await?;

        Ok(run)
    }

    /// Fail a run with error information
    pub async fn fail_run(
        &self,
        run_id: RunId,
        error: RunError,
    ) -> Result<ExperimentRun, TrackerError> {
        let mut run = self.run_store.get(run_id).await?;

        if run.status.is_terminal() {
            return Err(TrackerError::InvalidState {
                message: format!("Run is already in terminal state: {:?}", run.status),
            });
        }

        let now = Utc::now();
        run.status = RunStatus::Failed;
        run.ended_at = Some(now);
        run.error = Some(error);

        // Persist updated run
        self.run_store.save(&run).await?;

        // Record lineage
        self.lineage_tracker.record_run_failure(run_id, &run).await?;

        // Publish event
        self.event_publisher
            .publish(Event::RunFailed {
                run_id,
                experiment_id: run.experiment_id,
                error_type: run.error.as_ref().map(|e| e.error_type.clone()),
                timestamp: now,
            })
            .await?;

        Ok(run)
    }

    /// Compare multiple runs
    pub async fn compare_runs(
        &self,
        run_ids: Vec<RunId>,
        comparison_config: ComparisonConfig,
    ) -> Result<RunComparison, TrackerError> {
        if run_ids.len() < 2 {
            return Err(TrackerError::ValidationError {
                message: "At least 2 runs required for comparison".to_string(),
            });
        }

        // Load all runs
        let mut runs = Vec::new();
        for run_id in &run_ids {
            let run = self.run_store.get(*run_id).await?;
            runs.push(run);
        }

        // Verify all runs belong to the same experiment if required
        if comparison_config.same_experiment_only {
            let experiment_id = runs[0].experiment_id;
            for run in &runs[1..] {
                if run.experiment_id != experiment_id {
                    return Err(TrackerError::ValidationError {
                        message: "All runs must belong to the same experiment".to_string(),
                    });
                }
            }
        }

        // Build comparison
        let mut comparison = RunComparison {
            run_ids: run_ids.clone(),
            metric_comparisons: HashMap::new(),
            parameter_diff: HashMap::new(),
            environment_diff: None,
            statistical_tests: HashMap::new(),
            generated_at: Utc::now(),
        };

        // Compare metrics
        let metric_names: std::collections::HashSet<_> = runs
            .iter()
            .flat_map(|r| r.metrics.scalars.keys())
            .collect();

        for metric_name in metric_names {
            let values: Vec<_> = runs
                .iter()
                .map(|r| {
                    r.metrics
                        .scalars
                        .get(metric_name)
                        .map(|s| s.aggregations.mean)
                })
                .collect();

            comparison.metric_comparisons.insert(
                metric_name.clone(),
                MetricComparison {
                    metric_name: metric_name.clone(),
                    values: values.clone(),
                    best_run_index: find_best_index(&values, comparison_config.higher_is_better),
                    improvement_percentages: calculate_improvements(&values),
                },
            );

            // Run statistical test if enough data
            if comparison_config.run_statistical_tests && values.iter().all(|v| v.is_some()) {
                if let Some(test_result) = run_statistical_test(
                    &runs,
                    metric_name,
                    &comparison_config,
                ).await? {
                    comparison.statistical_tests.insert(metric_name.clone(), test_result);
                }
            }
        }

        // Compare parameters
        comparison.parameter_diff = compute_parameter_diff(&runs);

        // Compare environments if requested
        if comparison_config.compare_environments {
            comparison.environment_diff = Some(compute_environment_diff(&runs));
        }

        Ok(comparison)
    }

    /// Query runs with filtering and pagination
    pub async fn query_runs(
        &self,
        query: RunQuery,
    ) -> Result<QueryResult<ExperimentRun>, TrackerError> {
        self.run_store.query(query).await
    }

    /// Get experiment lineage graph
    pub async fn get_lineage(
        &self,
        experiment_id: ExperimentId,
    ) -> Result<LineageGraph, TrackerError> {
        self.lineage_tracker.get_experiment_lineage(experiment_id).await
    }

    // Private helper methods

    fn validate_experiment_request(
        &self,
        request: &CreateExperimentRequest,
    ) -> Result<(), TrackerError> {
        if request.name.is_empty() {
            return Err(TrackerError::ValidationError {
                message: "Experiment name cannot be empty".to_string(),
            });
        }

        if request.name.len() > 256 {
            return Err(TrackerError::ValidationError {
                message: "Experiment name cannot exceed 256 characters".to_string(),
            });
        }

        // Validate model configs
        for model_config in &request.config.model_configs {
            if model_config.model_id.is_empty() {
                return Err(TrackerError::ValidationError {
                    message: "Model ID cannot be empty".to_string(),
                });
            }
        }

        // Validate dataset refs
        for dataset_ref in &request.config.dataset_refs {
            // Additional validation could check if dataset exists
        }

        Ok(())
    }

    async fn log_metrics_internal(
        &self,
        run_id: RunId,
        metrics: Vec<MetricEntry>,
    ) -> Result<(), TrackerError> {
        let timestamp = Utc::now();
        for entry in metrics {
            let data_point = ScalarDataPoint {
                value: entry.value,
                step: entry.step,
                timestamp,
                context: entry.context.unwrap_or_default(),
            };
            self.metric_store
                .append_scalar(run_id, &entry.name, data_point)
                .await?;
        }
        Ok(())
    }
}

/// Request types for experiment tracker operations
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CreateExperimentRequest {
    pub name: String,
    pub description: Option<String>,
    pub hypothesis: Option<String>,
    pub owner_id: UserId,
    pub collaborators: Option<Vec<UserId>>,
    pub tags: Option<Vec<String>>,
    pub config: ExperimentConfig,
    pub metadata: Option<HashMap<String, serde_json::Value>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StartRunRequest {
    pub user_id: UserId,
    pub name: Option<String>,
    pub parameters: HashMap<String, ParameterValue>,
    pub tags: Option<Vec<String>>,
    pub parent_run_id: Option<RunId>,
    pub environment: Option<EnvironmentSnapshot>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MetricEntry {
    pub name: String,
    pub value: f64,
    pub step: Option<u64>,
    pub context: Option<HashMap<String, String>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LogArtifactRequest {
    pub name: String,
    pub artifact_type: ArtifactType,
    pub data: Vec<u8>,
    pub metadata: Option<HashMap<String, String>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ComparisonConfig {
    pub same_experiment_only: bool,
    pub higher_is_better: bool,
    pub compare_environments: bool,
    pub run_statistical_tests: bool,
    pub significance_level: f64,
}

impl Default for ComparisonConfig {
    fn default() -> Self {
        Self {
            same_experiment_only: true,
            higher_is_better: true,
            compare_environments: false,
            run_statistical_tests: true,
            significance_level: 0.05,
        }
    }
}

/// Comparison result types
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RunComparison {
    pub run_ids: Vec<RunId>,
    pub metric_comparisons: HashMap<String, MetricComparison>,
    pub parameter_diff: HashMap<String, ParameterDiff>,
    pub environment_diff: Option<EnvironmentDiff>,
    pub statistical_tests: HashMap<String, StatisticalTestResult>,
    pub generated_at: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MetricComparison {
    pub metric_name: String,
    pub values: Vec<Option<f64>>,
    pub best_run_index: Option<usize>,
    pub improvement_percentages: Vec<Option<f64>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ParameterDiff {
    pub parameter_name: String,
    pub values: Vec<Option<ParameterValue>>,
    pub is_different: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EnvironmentDiff {
    pub os_differences: Vec<String>,
    pub hardware_differences: Vec<String>,
    pub dependency_differences: Vec<DependencyDiff>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DependencyDiff {
    pub package: String,
    pub versions: Vec<Option<String>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StatisticalTestResult {
    pub test_type: StatisticalTest,
    pub statistic: f64,
    pub p_value: f64,
    pub significant: bool,
    pub effect_size: Option<f64>,
    pub confidence_interval: Option<(f64, f64)>,
}

#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum StatisticalTest {
    TTest,
    WilcoxonRankSum,
    Anova,
    KruskalWallis,
    PairedTTest,
    BootstrapComparison,
}
```

### 2.2 Storage Traits

```rust
/// Storage trait for experiments
#[async_trait]
pub trait ExperimentStore: Send + Sync {
    async fn save(&self, experiment: &Experiment) -> Result<(), TrackerError>;
    async fn get(&self, id: ExperimentId) -> Result<Experiment, TrackerError>;
    async fn update(&self, experiment: &Experiment) -> Result<(), TrackerError>;
    async fn delete(&self, id: ExperimentId) -> Result<(), TrackerError>;
    async fn query(&self, query: ExperimentQuery) -> Result<QueryResult<Experiment>, TrackerError>;
    async fn exists(&self, id: ExperimentId) -> Result<bool, TrackerError>;
}

/// Storage trait for experiment runs
#[async_trait]
pub trait RunStore: Send + Sync {
    async fn save(&self, run: &ExperimentRun) -> Result<(), TrackerError>;
    async fn get(&self, id: RunId) -> Result<ExperimentRun, TrackerError>;
    async fn update(&self, run: &ExperimentRun) -> Result<(), TrackerError>;
    async fn delete(&self, id: RunId) -> Result<(), TrackerError>;
    async fn query(&self, query: RunQuery) -> Result<QueryResult<ExperimentRun>, TrackerError>;
    async fn get_next_run_number(&self, experiment_id: ExperimentId) -> Result<u64, TrackerError>;
    async fn add_artifact(&self, run_id: RunId, artifact: ArtifactRef) -> Result<(), TrackerError>;
    async fn get_runs_for_experiment(
        &self,
        experiment_id: ExperimentId,
    ) -> Result<Vec<ExperimentRun>, TrackerError>;
}

/// Storage trait for artifacts
#[async_trait]
pub trait ArtifactStore: Send + Sync {
    async fn store(
        &self,
        id: ArtifactId,
        data: &[u8],
        compressed: bool,
    ) -> Result<String, TrackerError>; // Returns storage URI

    async fn retrieve(&self, uri: &str) -> Result<Vec<u8>, TrackerError>;
    async fn delete(&self, uri: &str) -> Result<(), TrackerError>;
    async fn exists(&self, uri: &str) -> Result<bool, TrackerError>;
    async fn get_metadata(&self, uri: &str) -> Result<ArtifactMetadata, TrackerError>;
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ArtifactMetadata {
    pub size_bytes: u64,
    pub content_type: Option<String>,
    pub created_at: DateTime<Utc>,
    pub checksum: String,
}

/// Storage trait for metrics time series
#[async_trait]
pub trait MetricStore: Send + Sync {
    async fn append_scalar(
        &self,
        run_id: RunId,
        metric_name: &str,
        data_point: ScalarDataPoint,
    ) -> Result<(), TrackerError>;

    async fn get_scalar_series(
        &self,
        run_id: RunId,
        metric_name: &str,
    ) -> Result<ScalarMetricSeries, TrackerError>;

    async fn compute_aggregations(&self, run_id: RunId) -> Result<RunMetrics, TrackerError>;

    async fn query_metrics(
        &self,
        run_id: RunId,
        metric_names: Option<Vec<String>>,
        time_range: Option<(DateTime<Utc>, DateTime<Utc>)>,
    ) -> Result<HashMap<String, ScalarMetricSeries>, TrackerError>;
}

/// Query types
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ExperimentQuery {
    pub owner_id: Option<UserId>,
    pub status: Option<Vec<ExperimentStatus>>,
    pub tags: Option<Vec<String>>,
    pub name_contains: Option<String>,
    pub created_after: Option<DateTime<Utc>>,
    pub created_before: Option<DateTime<Utc>>,
    pub pagination: Pagination,
    pub sort: Sort,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RunQuery {
    pub experiment_id: Option<ExperimentId>,
    pub status: Option<Vec<RunStatus>>,
    pub tags: Option<Vec<String>>,
    pub created_by: Option<UserId>,
    pub started_after: Option<DateTime<Utc>>,
    pub started_before: Option<DateTime<Utc>>,
    pub parameter_filters: HashMap<String, ParameterFilter>,
    pub metric_filters: HashMap<String, MetricFilter>,
    pub pagination: Pagination,
    pub sort: Sort,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Pagination {
    pub offset: u64,
    pub limit: u64,
}

impl Default for Pagination {
    fn default() -> Self {
        Self {
            offset: 0,
            limit: 50,
        }
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Sort {
    pub field: String,
    pub direction: SortDirection,
}

#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum SortDirection {
    Ascending,
    Descending,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ParameterFilter {
    Equals(ParameterValue),
    In(Vec<ParameterValue>),
    Range { min: Option<f64>, max: Option<f64> },
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum MetricFilter {
    GreaterThan(f64),
    LessThan(f64),
    Between { min: f64, max: f64 },
    Exists,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct QueryResult<T> {
    pub items: Vec<T>,
    pub total_count: u64,
    pub has_more: bool,
    pub next_offset: Option<u64>,
}
```

### 2.3 Environment Capture

```rust
/// Environment capture service for reproducibility
pub struct EnvironmentCapture {
    config: EnvironmentCaptureConfig,
}

#[derive(Debug, Clone)]
pub struct EnvironmentCaptureConfig {
    pub capture_env_vars: bool,
    pub env_var_allowlist: Option<Vec<String>>,
    pub env_var_blocklist: Vec<String>,
    pub capture_git: bool,
    pub capture_dependencies: bool,
    pub capture_hardware: bool,
}

impl Default for EnvironmentCaptureConfig {
    fn default() -> Self {
        Self {
            capture_env_vars: true,
            env_var_allowlist: None,
            env_var_blocklist: vec![
                "API_KEY".to_string(),
                "SECRET".to_string(),
                "PASSWORD".to_string(),
                "TOKEN".to_string(),
                "CREDENTIAL".to_string(),
                "AWS_".to_string(),
                "AZURE_".to_string(),
                "GCP_".to_string(),
            ],
            capture_git: true,
            capture_dependencies: true,
            capture_hardware: true,
        }
    }
}

impl EnvironmentCapture {
    pub fn new(config: EnvironmentCaptureConfig) -> Self {
        Self { config }
    }

    /// Capture complete environment snapshot
    pub async fn capture(&self) -> Result<EnvironmentSnapshot, TrackerError> {
        let os = self.capture_os_info()?;
        let hardware = if self.config.capture_hardware {
            self.capture_hardware_info().await?
        } else {
            HardwareInfo::minimal()
        };
        let runtime = self.capture_runtime_info()?;
        let dependencies = if self.config.capture_dependencies {
            self.capture_dependencies().await?
        } else {
            DependencyManifest::empty()
        };
        let environment_variables = if self.config.capture_env_vars {
            self.capture_env_vars()?
        } else {
            HashMap::new()
        };
        let git_state = if self.config.capture_git {
            self.capture_git_state().await.ok()
        } else {
            None
        };
        let container_info = self.detect_container_info().await.ok();

        Ok(EnvironmentSnapshot {
            os,
            hardware,
            runtime,
            dependencies,
            environment_variables,
            git_state,
            container_info,
            captured_at: Utc::now(),
        })
    }

    fn capture_os_info(&self) -> Result<OsInfo, TrackerError> {
        Ok(OsInfo {
            name: std::env::consts::OS.to_string(),
            version: get_os_version()?,
            architecture: std::env::consts::ARCH.to_string(),
            kernel_version: get_kernel_version().ok(),
        })
    }

    async fn capture_hardware_info(&self) -> Result<HardwareInfo, TrackerError> {
        let cpu_info = get_cpu_info()?;
        let memory_info = get_memory_info()?;
        let gpu_info = detect_gpus().await?;
        let hostname = gethostname::gethostname()
            .to_string_lossy()
            .to_string();

        Ok(HardwareInfo {
            cpu_model: cpu_info.model,
            cpu_cores: cpu_info.cores,
            cpu_threads: cpu_info.threads,
            memory_total_mb: memory_info.total_mb,
            gpus: gpu_info,
            hostname_hash: hash_string(&hostname),
        })
    }

    fn capture_runtime_info(&self) -> Result<RuntimeInfo, TrackerError> {
        Ok(RuntimeInfo {
            rust_version: option_env!("RUSTC_VERSION").map(String::from),
            python_version: detect_python_version().ok(),
            cuda_version: detect_cuda_version().ok(),
            custom_runtimes: detect_custom_runtimes(),
        })
    }

    async fn capture_dependencies(&self) -> Result<DependencyManifest, TrackerError> {
        // Try multiple dependency formats in order of preference
        if let Ok(manifest) = self.capture_cargo_lock().await {
            return Ok(manifest);
        }
        if let Ok(manifest) = self.capture_pip_freeze().await {
            return Ok(manifest);
        }
        if let Ok(manifest) = self.capture_conda_env().await {
            return Ok(manifest);
        }

        Ok(DependencyManifest::empty())
    }

    async fn capture_cargo_lock(&self) -> Result<DependencyManifest, TrackerError> {
        let cargo_lock_path = std::path::Path::new("Cargo.lock");
        if !cargo_lock_path.exists() {
            return Err(TrackerError::NotFound {
                message: "Cargo.lock not found".to_string(),
            });
        }

        let content = tokio::fs::read_to_string(cargo_lock_path).await?;
        let lockfile: toml::Value = toml::from_str(&content)?;

        let packages = lockfile
            .get("package")
            .and_then(|p| p.as_array())
            .map(|packages| {
                packages
                    .iter()
                    .filter_map(|p| {
                        Some(PackageInfo {
                            name: p.get("name")?.as_str()?.to_string(),
                            version: p.get("version")?.as_str()?.to_string(),
                            source: p.get("source").and_then(|s| s.as_str()).map(String::from),
                            checksum: p.get("checksum").and_then(|s| s.as_str()).map(String::from),
                        })
                    })
                    .collect()
            })
            .unwrap_or_default();

        Ok(DependencyManifest {
            format: DependencyFormat::CargoLock,
            content_hash: ContentHash::from_bytes(content.as_bytes()),
            packages,
        })
    }

    async fn capture_pip_freeze(&self) -> Result<DependencyManifest, TrackerError> {
        let output = tokio::process::Command::new("pip")
            .args(["freeze", "--all"])
            .output()
            .await?;

        if !output.status.success() {
            return Err(TrackerError::CommandFailed {
                command: "pip freeze".to_string(),
                stderr: String::from_utf8_lossy(&output.stderr).to_string(),
            });
        }

        let content = String::from_utf8_lossy(&output.stdout);
        let packages: Vec<PackageInfo> = content
            .lines()
            .filter_map(|line| {
                let parts: Vec<&str> = line.split("==").collect();
                if parts.len() == 2 {
                    Some(PackageInfo {
                        name: parts[0].to_string(),
                        version: parts[1].to_string(),
                        source: None,
                        checksum: None,
                    })
                } else {
                    None
                }
            })
            .collect();

        Ok(DependencyManifest {
            format: DependencyFormat::PipFreeze,
            content_hash: ContentHash::from_bytes(content.as_bytes()),
            packages,
        })
    }

    async fn capture_conda_env(&self) -> Result<DependencyManifest, TrackerError> {
        let output = tokio::process::Command::new("conda")
            .args(["list", "--export"])
            .output()
            .await?;

        if !output.status.success() {
            return Err(TrackerError::CommandFailed {
                command: "conda list".to_string(),
                stderr: String::from_utf8_lossy(&output.stderr).to_string(),
            });
        }

        let content = String::from_utf8_lossy(&output.stdout);
        let packages: Vec<PackageInfo> = content
            .lines()
            .filter(|line| !line.starts_with('#'))
            .filter_map(|line| {
                let parts: Vec<&str> = line.split('=').collect();
                if parts.len() >= 2 {
                    Some(PackageInfo {
                        name: parts[0].to_string(),
                        version: parts[1].to_string(),
                        source: parts.get(2).map(|s| s.to_string()),
                        checksum: None,
                    })
                } else {
                    None
                }
            })
            .collect();

        Ok(DependencyManifest {
            format: DependencyFormat::CondaEnvironment,
            content_hash: ContentHash::from_bytes(content.as_bytes()),
            packages,
        })
    }

    fn capture_env_vars(&self) -> Result<HashMap<String, String>, TrackerError> {
        let mut vars = HashMap::new();

        for (key, value) in std::env::vars() {
            // Check blocklist
            let blocked = self.config.env_var_blocklist.iter().any(|pattern| {
                key.to_uppercase().contains(&pattern.to_uppercase())
            });

            if blocked {
                continue;
            }

            // Check allowlist if specified
            if let Some(ref allowlist) = self.config.env_var_allowlist {
                if !allowlist.iter().any(|pattern| {
                    key.to_uppercase().contains(&pattern.to_uppercase())
                }) {
                    continue;
                }
            }

            vars.insert(key, value);
        }

        Ok(vars)
    }

    async fn capture_git_state(&self) -> Result<GitState, TrackerError> {
        // Get current branch
        let branch_output = tokio::process::Command::new("git")
            .args(["rev-parse", "--abbrev-ref", "HEAD"])
            .output()
            .await?;

        let branch = String::from_utf8_lossy(&branch_output.stdout)
            .trim()
            .to_string();

        // Get commit hash
        let commit_output = tokio::process::Command::new("git")
            .args(["rev-parse", "HEAD"])
            .output()
            .await?;

        let commit_hash = String::from_utf8_lossy(&commit_output.stdout)
            .trim()
            .to_string();

        // Check if dirty
        let status_output = tokio::process::Command::new("git")
            .args(["status", "--porcelain"])
            .output()
            .await?;

        let is_dirty = !status_output.stdout.is_empty();

        // Get uncommitted changes summary if dirty
        let uncommitted_changes = if is_dirty {
            let diff_output = tokio::process::Command::new("git")
                .args(["diff", "--stat"])
                .output()
                .await?;
            Some(String::from_utf8_lossy(&diff_output.stdout).to_string())
        } else {
            None
        };

        // Get remote URL
        let remote_output = tokio::process::Command::new("git")
            .args(["remote", "get-url", "origin"])
            .output()
            .await
            .ok();

        let repository_url = remote_output.and_then(|o| {
            if o.status.success() {
                Some(String::from_utf8_lossy(&o.stdout).trim().to_string())
            } else {
                None
            }
        });

        Ok(GitState {
            repository_url,
            branch,
            commit_hash,
            is_dirty,
            uncommitted_changes,
            remote_tracking: None,
        })
    }

    async fn detect_container_info(&self) -> Result<ContainerInfo, TrackerError> {
        // Check if running in container
        if !std::path::Path::new("/.dockerenv").exists()
            && !std::path::Path::new("/run/.containerenv").exists()
        {
            return Err(TrackerError::NotFound {
                message: "Not running in container".to_string(),
            });
        }

        // Try to get image info from environment
        let image = std::env::var("CONTAINER_IMAGE")
            .or_else(|_| std::env::var("IMAGE_NAME"))
            .unwrap_or_else(|_| "unknown".to_string());

        let image_digest = std::env::var("CONTAINER_DIGEST")
            .or_else(|_| std::env::var("IMAGE_DIGEST"))
            .unwrap_or_else(|_| "unknown".to_string());

        Ok(ContainerInfo {
            image,
            image_digest,
            runtime: detect_container_runtime(),
            resource_limits: detect_container_limits().await.ok(),
        })
    }
}

impl EnvironmentSnapshot {
    pub fn minimal() -> Self {
        Self {
            os: OsInfo {
                name: std::env::consts::OS.to_string(),
                version: "unknown".to_string(),
                architecture: std::env::consts::ARCH.to_string(),
                kernel_version: None,
            },
            hardware: HardwareInfo::minimal(),
            runtime: RuntimeInfo {
                rust_version: None,
                python_version: None,
                cuda_version: None,
                custom_runtimes: HashMap::new(),
            },
            dependencies: DependencyManifest::empty(),
            environment_variables: HashMap::new(),
            git_state: None,
            container_info: None,
            captured_at: Utc::now(),
        }
    }
}

impl HardwareInfo {
    pub fn minimal() -> Self {
        Self {
            cpu_model: "unknown".to_string(),
            cpu_cores: 0,
            cpu_threads: 0,
            memory_total_mb: 0,
            gpus: Vec::new(),
            hostname_hash: "unknown".to_string(),
        }
    }
}

impl DependencyManifest {
    pub fn empty() -> Self {
        Self {
            format: DependencyFormat::Custom("none".to_string()),
            content_hash: ContentHash("empty".to_string()),
            packages: Vec::new(),
        }
    }
}

impl Default for RunMetrics {
    fn default() -> Self {
        Self {
            scalars: HashMap::new(),
            distributions: HashMap::new(),
            confusion_matrices: HashMap::new(),
            custom: HashMap::new(),
        }
    }
}

impl Default for LogSummary {
    fn default() -> Self {
        Self {
            total_lines: 0,
            error_count: 0,
            warning_count: 0,
            log_uri: String::new(),
            highlights: Vec::new(),
        }
    }
}

// Helper functions (implementations would be platform-specific)
fn get_os_version() -> Result<String, TrackerError> {
    // Platform-specific implementation
    Ok("unknown".to_string())
}

fn get_kernel_version() -> Result<String, TrackerError> {
    // Platform-specific implementation
    Ok("unknown".to_string())
}

fn get_cpu_info() -> Result<CpuInfo, TrackerError> {
    // Platform-specific implementation
    Ok(CpuInfo {
        model: "unknown".to_string(),
        cores: num_cpus::get_physical() as u32,
        threads: num_cpus::get() as u32,
    })
}

struct CpuInfo {
    model: String,
    cores: u32,
    threads: u32,
}

fn get_memory_info() -> Result<MemoryInfo, TrackerError> {
    // Platform-specific implementation using sysinfo crate
    Ok(MemoryInfo { total_mb: 0 })
}

struct MemoryInfo {
    total_mb: u64,
}

async fn detect_gpus() -> Result<Vec<GpuInfo>, TrackerError> {
    // Use nvidia-smi or similar
    Ok(Vec::new())
}

fn detect_python_version() -> Result<String, TrackerError> {
    // Run python --version
    Ok("unknown".to_string())
}

fn detect_cuda_version() -> Result<String, TrackerError> {
    // Check CUDA installation
    Ok("unknown".to_string())
}

fn detect_custom_runtimes() -> HashMap<String, String> {
    HashMap::new()
}

fn hash_string(s: &str) -> String {
    use sha2::{Sha256, Digest};
    let mut hasher = Sha256::new();
    hasher.update(s.as_bytes());
    hex::encode(hasher.finalize())[..16].to_string()
}

fn detect_container_runtime() -> String {
    if std::path::Path::new("/.dockerenv").exists() {
        "docker".to_string()
    } else if std::path::Path::new("/run/.containerenv").exists() {
        "podman".to_string()
    } else {
        "unknown".to_string()
    }
}

async fn detect_container_limits() -> Result<ContainerResourceLimits, TrackerError> {
    // Read from cgroup files
    Ok(ContainerResourceLimits {
        cpu_limit: None,
        memory_limit_mb: None,
        gpu_count: None,
    })
}

fn compress_data(data: &[u8]) -> Result<Vec<u8>, TrackerError> {
    use flate2::write::GzEncoder;
    use flate2::Compression;
    use std::io::Write;

    let mut encoder = GzEncoder::new(Vec::new(), Compression::default());
    encoder.write_all(data)?;
    Ok(encoder.finish()?)
}
```

### 2.4 Lineage Tracker

```rust
/// Lineage tracker for experiment provenance
pub struct LineageTracker {
    store: Arc<dyn LineageStore>,
}

#[async_trait]
pub trait LineageStore: Send + Sync {
    async fn record_node(&self, node: LineageNode) -> Result<(), TrackerError>;
    async fn record_edge(&self, edge: LineageEdge) -> Result<(), TrackerError>;
    async fn get_node(&self, id: &str) -> Result<LineageNode, TrackerError>;
    async fn get_ancestors(&self, id: &str, depth: Option<u32>) -> Result<Vec<LineageNode>, TrackerError>;
    async fn get_descendants(&self, id: &str, depth: Option<u32>) -> Result<Vec<LineageNode>, TrackerError>;
    async fn get_graph(&self, root_id: &str) -> Result<LineageGraph, TrackerError>;
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LineageNode {
    pub id: String,
    pub node_type: LineageNodeType,
    pub name: String,
    pub version: Option<String>,
    pub metadata: HashMap<String, serde_json::Value>,
    pub created_at: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum LineageNodeType {
    Experiment,
    Run,
    Dataset,
    DatasetVersion,
    Model,
    Artifact,
    Metric,
    Transformation,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LineageEdge {
    pub source_id: String,
    pub target_id: String,
    pub edge_type: LineageEdgeType,
    pub metadata: HashMap<String, serde_json::Value>,
    pub created_at: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum LineageEdgeType {
    Contains,      // Experiment contains Run
    UsesDataset,   // Run uses Dataset
    UsesModel,     // Run uses Model
    Produces,      // Run produces Artifact
    DerivedFrom,   // Dataset derived from another Dataset
    Evaluates,     // Run evaluates with Metric
    Transforms,    // Transformation applied
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LineageGraph {
    pub nodes: Vec<LineageNode>,
    pub edges: Vec<LineageEdge>,
    pub root_id: String,
}

impl LineageTracker {
    pub fn new(store: Arc<dyn LineageStore>) -> Self {
        Self { store }
    }

    pub async fn record_experiment_creation(
        &self,
        experiment_id: ExperimentId,
        experiment: &Experiment,
    ) -> Result<(), TrackerError> {
        let node = LineageNode {
            id: format!("experiment:{}", experiment_id.0),
            node_type: LineageNodeType::Experiment,
            name: experiment.name.clone(),
            version: None,
            metadata: serde_json::to_value(&experiment.metadata)
                .ok()
                .and_then(|v| v.as_object().cloned())
                .map(|m| m.into_iter().collect())
                .unwrap_or_default(),
            created_at: experiment.created_at,
        };

        self.store.record_node(node).await
    }

    pub async fn record_run_start(
        &self,
        run_id: RunId,
        experiment_id: ExperimentId,
        run: &ExperimentRun,
    ) -> Result<(), TrackerError> {
        // Create run node
        let run_node = LineageNode {
            id: format!("run:{}", run_id.0),
            node_type: LineageNodeType::Run,
            name: run.name.clone().unwrap_or_else(|| format!("Run #{}", run.run_number)),
            version: Some(run.run_number.to_string()),
            metadata: HashMap::new(),
            created_at: run.created_at,
        };
        self.store.record_node(run_node).await?;

        // Create edge from experiment to run
        let edge = LineageEdge {
            source_id: format!("experiment:{}", experiment_id.0),
            target_id: format!("run:{}", run_id.0),
            edge_type: LineageEdgeType::Contains,
            metadata: HashMap::new(),
            created_at: Utc::now(),
        };
        self.store.record_edge(edge).await
    }

    pub async fn record_run_completion(
        &self,
        run_id: RunId,
        run: &ExperimentRun,
    ) -> Result<(), TrackerError> {
        // Update run node with completion metadata
        let mut metadata = HashMap::new();
        metadata.insert(
            "status".to_string(),
            serde_json::Value::String(format!("{:?}", run.status)),
        );
        if let (Some(start), Some(end)) = (run.started_at, run.ended_at) {
            let duration = (end - start).num_seconds();
            metadata.insert(
                "duration_seconds".to_string(),
                serde_json::Value::Number(duration.into()),
            );
        }

        let node = LineageNode {
            id: format!("run:{}", run_id.0),
            node_type: LineageNodeType::Run,
            name: run.name.clone().unwrap_or_else(|| format!("Run #{}", run.run_number)),
            version: Some(run.run_number.to_string()),
            metadata,
            created_at: run.created_at,
        };

        self.store.record_node(node).await
    }

    pub async fn record_run_failure(
        &self,
        run_id: RunId,
        run: &ExperimentRun,
    ) -> Result<(), TrackerError> {
        let mut metadata = HashMap::new();
        metadata.insert(
            "status".to_string(),
            serde_json::Value::String("failed".to_string()),
        );
        if let Some(ref error) = run.error {
            metadata.insert(
                "error_type".to_string(),
                serde_json::Value::String(error.error_type.clone()),
            );
            metadata.insert(
                "error_message".to_string(),
                serde_json::Value::String(error.message.clone()),
            );
        }

        let node = LineageNode {
            id: format!("run:{}", run_id.0),
            node_type: LineageNodeType::Run,
            name: run.name.clone().unwrap_or_else(|| format!("Run #{}", run.run_number)),
            version: Some(run.run_number.to_string()),
            metadata,
            created_at: run.created_at,
        };

        self.store.record_node(node).await
    }

    pub async fn record_artifact(
        &self,
        run_id: RunId,
        artifact: &ArtifactRef,
    ) -> Result<(), TrackerError> {
        // Create artifact node
        let artifact_node = LineageNode {
            id: format!("artifact:{}", artifact.id.0),
            node_type: LineageNodeType::Artifact,
            name: artifact.name.clone(),
            version: None,
            metadata: artifact
                .metadata
                .iter()
                .map(|(k, v)| (k.clone(), serde_json::Value::String(v.clone())))
                .collect(),
            created_at: artifact.created_at,
        };
        self.store.record_node(artifact_node).await?;

        // Create edge from run to artifact
        let edge = LineageEdge {
            source_id: format!("run:{}", run_id.0),
            target_id: format!("artifact:{}", artifact.id.0),
            edge_type: LineageEdgeType::Produces,
            metadata: HashMap::new(),
            created_at: Utc::now(),
        };
        self.store.record_edge(edge).await
    }

    pub async fn record_dataset_usage(
        &self,
        run_id: RunId,
        dataset_ref: &DatasetRef,
    ) -> Result<(), TrackerError> {
        let edge = LineageEdge {
            source_id: format!("run:{}", run_id.0),
            target_id: format!("dataset:{}", dataset_ref.dataset_id.0),
            edge_type: LineageEdgeType::UsesDataset,
            metadata: {
                let mut m = HashMap::new();
                m.insert(
                    "version_selector".to_string(),
                    serde_json::to_value(&dataset_ref.version).unwrap_or_default(),
                );
                if let Some(ref split) = dataset_ref.split {
                    m.insert(
                        "split".to_string(),
                        serde_json::Value::String(format!("{:?}", split)),
                    );
                }
                m
            },
            created_at: Utc::now(),
        };
        self.store.record_edge(edge).await
    }

    pub async fn get_experiment_lineage(
        &self,
        experiment_id: ExperimentId,
    ) -> Result<LineageGraph, TrackerError> {
        self.store
            .get_graph(&format!("experiment:{}", experiment_id.0))
            .await
    }
}
```

### 2.5 Event System

```rust
/// Event types for experiment tracking
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(tag = "type", rename_all = "snake_case")]
pub enum Event {
    ExperimentCreated {
        experiment_id: ExperimentId,
        owner_id: UserId,
        timestamp: DateTime<Utc>,
    },
    ExperimentUpdated {
        experiment_id: ExperimentId,
        updated_fields: Vec<String>,
        timestamp: DateTime<Utc>,
    },
    ExperimentArchived {
        experiment_id: ExperimentId,
        timestamp: DateTime<Utc>,
    },
    RunStarted {
        run_id: RunId,
        experiment_id: ExperimentId,
        timestamp: DateTime<Utc>,
    },
    RunCompleted {
        run_id: RunId,
        experiment_id: ExperimentId,
        status: RunStatus,
        duration: chrono::Duration,
        timestamp: DateTime<Utc>,
    },
    RunFailed {
        run_id: RunId,
        experiment_id: ExperimentId,
        error_type: Option<String>,
        timestamp: DateTime<Utc>,
    },
    MetricsLogged {
        run_id: RunId,
        count: usize,
        timestamp: DateTime<Utc>,
    },
    ArtifactLogged {
        run_id: RunId,
        artifact_id: ArtifactId,
        artifact_type: ArtifactType,
        timestamp: DateTime<Utc>,
    },
}

/// Event publisher trait
#[async_trait]
pub trait EventPublisher: Send + Sync {
    async fn publish(&self, event: Event) -> Result<(), TrackerError>;
    async fn publish_batch(&self, events: Vec<Event>) -> Result<(), TrackerError>;
}

/// Event subscriber trait
#[async_trait]
pub trait EventSubscriber: Send + Sync {
    async fn subscribe(&self, filter: EventFilter) -> Result<EventStream, TrackerError>;
}

pub type EventStream = tokio::sync::mpsc::Receiver<Event>;

#[derive(Debug, Clone)]
pub struct EventFilter {
    pub event_types: Option<Vec<String>>,
    pub experiment_ids: Option<Vec<ExperimentId>>,
    pub run_ids: Option<Vec<RunId>>,
    pub since: Option<DateTime<Utc>>,
}
```

### 2.6 Error Types

```rust
/// Tracker error types
#[derive(Debug, thiserror::Error)]
pub enum TrackerError {
    #[error("Not found: {message}")]
    NotFound { message: String },

    #[error("Validation error: {message}")]
    ValidationError { message: String },

    #[error("Invalid state: {message}")]
    InvalidState { message: String },

    #[error("Storage error: {source}")]
    Storage {
        #[from]
        source: Box<dyn std::error::Error + Send + Sync>,
    },

    #[error("Artifact too large: {size} bytes exceeds max {max_size} bytes")]
    ArtifactTooLarge { size: u64, max_size: u64 },

    #[error("Command failed: {command} - {stderr}")]
    CommandFailed { command: String, stderr: String },

    #[error("Serialization error: {0}")]
    Serialization(#[from] serde_json::Error),

    #[error("IO error: {0}")]
    Io(#[from] std::io::Error),

    #[error("TOML parse error: {0}")]
    TomlParse(#[from] toml::de::Error),

    #[error("Unauthorized: {message}")]
    Unauthorized { message: String },

    #[error("Conflict: {message}")]
    Conflict { message: String },

    #[error("Internal error: {message}")]
    Internal { message: String },
}

impl TrackerError {
    pub fn is_retryable(&self) -> bool {
        matches!(
            self,
            TrackerError::Storage { .. } | TrackerError::Internal { .. }
        )
    }

    pub fn error_code(&self) -> &'static str {
        match self {
            TrackerError::NotFound { .. } => "NOT_FOUND",
            TrackerError::ValidationError { .. } => "VALIDATION_ERROR",
            TrackerError::InvalidState { .. } => "INVALID_STATE",
            TrackerError::Storage { .. } => "STORAGE_ERROR",
            TrackerError::ArtifactTooLarge { .. } => "ARTIFACT_TOO_LARGE",
            TrackerError::CommandFailed { .. } => "COMMAND_FAILED",
            TrackerError::Serialization(_) => "SERIALIZATION_ERROR",
            TrackerError::Io(_) => "IO_ERROR",
            TrackerError::TomlParse(_) => "TOML_PARSE_ERROR",
            TrackerError::Unauthorized { .. } => "UNAUTHORIZED",
            TrackerError::Conflict { .. } => "CONFLICT",
            TrackerError::Internal { .. } => "INTERNAL_ERROR",
        }
    }
}
```

---

## Document Metadata

| Field | Value |
|-------|-------|
| **Version** | 1.0.0 |
| **Status** | Draft |
| **SPARC Phase** | Pseudocode (Part 1 of 3) |
| **Created** | 2025-11-28 |
| **Ecosystem** | LLM DevOps |
| **Next Part** | Pseudocode Part 2: Metrics & Datasets |

---

*This pseudocode document is part of the SPARC methodology. Part 2 covers Metric Benchmarking and Dataset Versioning. Part 3 covers Integration APIs, Reproducibility Engine, and Workflow Orchestration.*
---

## 3. Metric Benchmarking Framework

### 3.1 Metric Trait and Built-in Metrics

```rust
//! Metric benchmarking framework for LLM evaluation
//! Provides extensible metric definitions with statistical rigor

use async_trait::async_trait;
use std::sync::Arc;

/// Core metric trait - all metrics implement this interface
#[async_trait]
pub trait Metric: Send + Sync {
    /// Unique metric name
    fn name(&self) -> &str;

    /// Metric version for reproducibility
    fn version(&self) -> SemanticVersion;

    /// Compute metric value from input
    async fn compute(&self, input: MetricInput) -> Result<MetricOutput, MetricError>;

    /// Metric properties for validation and aggregation
    fn properties(&self) -> MetricProperties;

    /// Optional batch computation for efficiency
    async fn compute_batch(&self, inputs: Vec<MetricInput>) -> Result<Vec<MetricOutput>, MetricError> {
        let mut results = Vec::with_capacity(inputs.len());
        for input in inputs {
            results.push(self.compute(input).await?);
        }
        Ok(results)
    }

    /// Validate metric configuration
    fn validate_config(&self, config: &HashMap<String, serde_json::Value>) -> Result<(), MetricError> {
        Ok(())
    }
}

/// Input to metric computation
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MetricInput {
    pub prompt: String,
    pub response: String,
    pub expected: Option<String>,
    pub context: HashMap<String, serde_json::Value>,
    pub model_info: Option<ModelInfo>,
    pub latency_ms: Option<u64>,
    pub token_counts: Option<TokenCounts>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelInfo {
    pub provider: String,
    pub model_id: String,
    pub parameters: HashMap<String, serde_json::Value>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TokenCounts {
    pub prompt_tokens: u32,
    pub completion_tokens: u32,
    pub total_tokens: u32,
}

/// Output from metric computation
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MetricOutput {
    pub value: MetricValue,
    pub confidence: Option<f64>,
    pub breakdown: Option<HashMap<String, f64>>,
    pub explanation: Option<String>,
    pub computation_time_ms: u64,
}

/// Metric value types
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(untagged)]
pub enum MetricValue {
    Scalar(f64),
    Boolean(bool),
    Categorical(String),
    Distribution(DistributionValue),
    MultiLabel(Vec<String>),
    Structured(HashMap<String, serde_json::Value>),
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DistributionValue {
    pub mean: f64,
    pub std: f64,
    pub min: f64,
    pub max: f64,
    pub percentiles: HashMap<String, f64>,
}

/// Metric properties for validation and aggregation
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MetricProperties {
    pub metric_type: MetricType,
    pub value_range: Option<ValueRange>,
    pub higher_is_better: bool,
    pub requires_reference: bool,
    pub supports_batch: bool,
    pub deterministic: bool,
    pub aggregation_method: AggregationMethod,
    pub tags: Vec<String>,
}

#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum MetricType {
    Quality,        // Response quality metrics
    Safety,         // Safety and alignment metrics
    Performance,    // Latency, throughput metrics
    Cost,           // Token usage, API cost metrics
    Factuality,     // Factual accuracy metrics
    Coherence,      // Text coherence metrics
    Relevance,      // Response relevance metrics
    Custom,         // User-defined metrics
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ValueRange {
    pub min: f64,
    pub max: f64,
    pub inclusive: bool,
}

#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum AggregationMethod {
    Mean,
    Median,
    WeightedMean,
    Max,
    Min,
    Sum,
    Mode,
    Custom,
}

/// Built-in accuracy metric (exact match)
pub struct ExactMatchMetric {
    case_sensitive: bool,
    strip_whitespace: bool,
}

impl ExactMatchMetric {
    pub fn new(case_sensitive: bool, strip_whitespace: bool) -> Self {
        Self {
            case_sensitive,
            strip_whitespace,
        }
    }

    fn normalize(&self, text: &str) -> String {
        let mut result = text.to_string();
        if self.strip_whitespace {
            result = result.trim().to_string();
        }
        if !self.case_sensitive {
            result = result.to_lowercase();
        }
        result
    }
}

#[async_trait]
impl Metric for ExactMatchMetric {
    fn name(&self) -> &str {
        "exact_match"
    }

    fn version(&self) -> SemanticVersion {
        SemanticVersion::new(1, 0, 0)
    }

    async fn compute(&self, input: MetricInput) -> Result<MetricOutput, MetricError> {
        let start = std::time::Instant::now();

        let expected = input.expected.ok_or(MetricError::MissingReference {
            metric: "exact_match".to_string(),
        })?;

        let normalized_response = self.normalize(&input.response);
        let normalized_expected = self.normalize(&expected);

        let matches = normalized_response == normalized_expected;

        Ok(MetricOutput {
            value: MetricValue::Boolean(matches),
            confidence: Some(1.0), // Deterministic
            breakdown: None,
            explanation: Some(if matches {
                "Response exactly matches expected".to_string()
            } else {
                "Response does not match expected".to_string()
            }),
            computation_time_ms: start.elapsed().as_millis() as u64,
        })
    }

    fn properties(&self) -> MetricProperties {
        MetricProperties {
            metric_type: MetricType::Quality,
            value_range: Some(ValueRange {
                min: 0.0,
                max: 1.0,
                inclusive: true,
            }),
            higher_is_better: true,
            requires_reference: true,
            supports_batch: true,
            deterministic: true,
            aggregation_method: AggregationMethod::Mean,
            tags: vec!["accuracy".to_string(), "exact".to_string()],
        }
    }
}

/// BLEU score metric for text similarity
pub struct BleuMetric {
    max_ngram: usize,
    smoothing: SmoothingMethod,
}

#[derive(Debug, Clone, Copy)]
pub enum SmoothingMethod {
    None,
    Add1,
    AddK(f64),
    Exponential,
}

impl BleuMetric {
    pub fn new(max_ngram: usize, smoothing: SmoothingMethod) -> Self {
        Self { max_ngram, smoothing }
    }

    fn tokenize(&self, text: &str) -> Vec<String> {
        text.split_whitespace()
            .map(|s| s.to_lowercase())
            .collect()
    }

    fn get_ngrams(&self, tokens: &[String], n: usize) -> HashMap<Vec<String>, usize> {
        let mut ngrams = HashMap::new();
        if tokens.len() >= n {
            for window in tokens.windows(n) {
                *ngrams.entry(window.to_vec()).or_insert(0) += 1;
            }
        }
        ngrams
    }

    fn modified_precision(&self, candidate: &[String], reference: &[String], n: usize) -> f64 {
        let candidate_ngrams = self.get_ngrams(candidate, n);
        let reference_ngrams = self.get_ngrams(reference, n);

        let mut clipped_count = 0;
        let mut total_count = 0;

        for (ngram, count) in &candidate_ngrams {
            let ref_count = reference_ngrams.get(ngram).copied().unwrap_or(0);
            clipped_count += (*count).min(ref_count);
            total_count += count;
        }

        if total_count == 0 {
            return 0.0;
        }

        match self.smoothing {
            SmoothingMethod::None => clipped_count as f64 / total_count as f64,
            SmoothingMethod::Add1 => (clipped_count as f64 + 1.0) / (total_count as f64 + 1.0),
            SmoothingMethod::AddK(k) => (clipped_count as f64 + k) / (total_count as f64 + k),
            SmoothingMethod::Exponential => {
                if clipped_count == 0 {
                    1.0 / (2_f64.powi(self.max_ngram as i32 - n as i32 + 1))
                } else {
                    clipped_count as f64 / total_count as f64
                }
            }
        }
    }

    fn brevity_penalty(&self, candidate_len: usize, reference_len: usize) -> f64 {
        if candidate_len >= reference_len {
            1.0
        } else {
            (1.0 - reference_len as f64 / candidate_len as f64).exp()
        }
    }
}

#[async_trait]
impl Metric for BleuMetric {
    fn name(&self) -> &str {
        "bleu"
    }

    fn version(&self) -> SemanticVersion {
        SemanticVersion::new(1, 0, 0)
    }

    async fn compute(&self, input: MetricInput) -> Result<MetricOutput, MetricError> {
        let start = std::time::Instant::now();

        let expected = input.expected.ok_or(MetricError::MissingReference {
            metric: "bleu".to_string(),
        })?;

        let candidate_tokens = self.tokenize(&input.response);
        let reference_tokens = self.tokenize(&expected);

        if candidate_tokens.is_empty() || reference_tokens.is_empty() {
            return Ok(MetricOutput {
                value: MetricValue::Scalar(0.0),
                confidence: Some(1.0),
                breakdown: None,
                explanation: Some("Empty input".to_string()),
                computation_time_ms: start.elapsed().as_millis() as u64,
            });
        }

        // Compute modified precisions for each n-gram level
        let mut log_precisions = 0.0;
        let mut breakdown = HashMap::new();

        for n in 1..=self.max_ngram {
            let precision = self.modified_precision(&candidate_tokens, &reference_tokens, n);
            breakdown.insert(format!("precision_{}", n), precision);

            if precision > 0.0 {
                log_precisions += precision.ln() / self.max_ngram as f64;
            } else {
                log_precisions = f64::NEG_INFINITY;
                break;
            }
        }

        let brevity_penalty =
            self.brevity_penalty(candidate_tokens.len(), reference_tokens.len());
        breakdown.insert("brevity_penalty".to_string(), brevity_penalty);

        let bleu = if log_precisions.is_finite() {
            brevity_penalty * log_precisions.exp()
        } else {
            0.0
        };

        Ok(MetricOutput {
            value: MetricValue::Scalar(bleu),
            confidence: Some(1.0),
            breakdown: Some(breakdown),
            explanation: Some(format!(
                "BLEU-{} score: {:.4} (BP: {:.4})",
                self.max_ngram, bleu, brevity_penalty
            )),
            computation_time_ms: start.elapsed().as_millis() as u64,
        })
    }

    fn properties(&self) -> MetricProperties {
        MetricProperties {
            metric_type: MetricType::Quality,
            value_range: Some(ValueRange {
                min: 0.0,
                max: 1.0,
                inclusive: true,
            }),
            higher_is_better: true,
            requires_reference: true,
            supports_batch: true,
            deterministic: true,
            aggregation_method: AggregationMethod::Mean,
            tags: vec!["similarity".to_string(), "translation".to_string()],
        }
    }
}

/// ROUGE metric for summarization evaluation
pub struct RougeMetric {
    rouge_type: RougeType,
}

#[derive(Debug, Clone, Copy)]
pub enum RougeType {
    RougeL,  // Longest common subsequence
    Rouge1,  // Unigram overlap
    Rouge2,  // Bigram overlap
    RougeN(usize),
}

impl RougeMetric {
    pub fn new(rouge_type: RougeType) -> Self {
        Self { rouge_type }
    }

    fn tokenize(&self, text: &str) -> Vec<String> {
        text.split_whitespace()
            .map(|s| s.to_lowercase())
            .collect()
    }

    fn lcs_length(&self, a: &[String], b: &[String]) -> usize {
        let m = a.len();
        let n = b.len();
        let mut dp = vec![vec![0; n + 1]; m + 1];

        for i in 1..=m {
            for j in 1..=n {
                if a[i - 1] == b[j - 1] {
                    dp[i][j] = dp[i - 1][j - 1] + 1;
                } else {
                    dp[i][j] = dp[i - 1][j].max(dp[i][j - 1]);
                }
            }
        }

        dp[m][n]
    }

    fn ngram_overlap(&self, candidate: &[String], reference: &[String], n: usize) -> (f64, f64, f64) {
        let get_ngrams = |tokens: &[String]| -> HashMap<Vec<String>, usize> {
            let mut ngrams = HashMap::new();
            if tokens.len() >= n {
                for window in tokens.windows(n) {
                    *ngrams.entry(window.to_vec()).or_insert(0) += 1;
                }
            }
            ngrams
        };

        let candidate_ngrams = get_ngrams(candidate);
        let reference_ngrams = get_ngrams(reference);

        let mut overlap = 0;
        for (ngram, count) in &candidate_ngrams {
            if let Some(&ref_count) = reference_ngrams.get(ngram) {
                overlap += (*count).min(ref_count);
            }
        }

        let candidate_total: usize = candidate_ngrams.values().sum();
        let reference_total: usize = reference_ngrams.values().sum();

        let precision = if candidate_total > 0 {
            overlap as f64 / candidate_total as f64
        } else {
            0.0
        };

        let recall = if reference_total > 0 {
            overlap as f64 / reference_total as f64
        } else {
            0.0
        };

        let f1 = if precision + recall > 0.0 {
            2.0 * precision * recall / (precision + recall)
        } else {
            0.0
        };

        (precision, recall, f1)
    }
}

#[async_trait]
impl Metric for RougeMetric {
    fn name(&self) -> &str {
        match self.rouge_type {
            RougeType::RougeL => "rouge_l",
            RougeType::Rouge1 => "rouge_1",
            RougeType::Rouge2 => "rouge_2",
            RougeType::RougeN(n) => "rouge_n",
        }
    }

    fn version(&self) -> SemanticVersion {
        SemanticVersion::new(1, 0, 0)
    }

    async fn compute(&self, input: MetricInput) -> Result<MetricOutput, MetricError> {
        let start = std::time::Instant::now();

        let expected = input.expected.ok_or(MetricError::MissingReference {
            metric: self.name().to_string(),
        })?;

        let candidate_tokens = self.tokenize(&input.response);
        let reference_tokens = self.tokenize(&expected);

        if candidate_tokens.is_empty() || reference_tokens.is_empty() {
            return Ok(MetricOutput {
                value: MetricValue::Scalar(0.0),
                confidence: Some(1.0),
                breakdown: None,
                explanation: Some("Empty input".to_string()),
                computation_time_ms: start.elapsed().as_millis() as u64,
            });
        }

        let (precision, recall, f1) = match self.rouge_type {
            RougeType::RougeL => {
                let lcs = self.lcs_length(&candidate_tokens, &reference_tokens);
                let p = lcs as f64 / candidate_tokens.len() as f64;
                let r = lcs as f64 / reference_tokens.len() as f64;
                let f = if p + r > 0.0 { 2.0 * p * r / (p + r) } else { 0.0 };
                (p, r, f)
            }
            RougeType::Rouge1 => self.ngram_overlap(&candidate_tokens, &reference_tokens, 1),
            RougeType::Rouge2 => self.ngram_overlap(&candidate_tokens, &reference_tokens, 2),
            RougeType::RougeN(n) => self.ngram_overlap(&candidate_tokens, &reference_tokens, n),
        };

        let mut breakdown = HashMap::new();
        breakdown.insert("precision".to_string(), precision);
        breakdown.insert("recall".to_string(), recall);
        breakdown.insert("f1".to_string(), f1);

        Ok(MetricOutput {
            value: MetricValue::Scalar(f1),
            confidence: Some(1.0),
            breakdown: Some(breakdown),
            explanation: Some(format!(
                "{}: F1={:.4}, P={:.4}, R={:.4}",
                self.name().to_uppercase(),
                f1,
                precision,
                recall
            )),
            computation_time_ms: start.elapsed().as_millis() as u64,
        })
    }

    fn properties(&self) -> MetricProperties {
        MetricProperties {
            metric_type: MetricType::Quality,
            value_range: Some(ValueRange {
                min: 0.0,
                max: 1.0,
                inclusive: true,
            }),
            higher_is_better: true,
            requires_reference: true,
            supports_batch: true,
            deterministic: true,
            aggregation_method: AggregationMethod::Mean,
            tags: vec!["similarity".to_string(), "summarization".to_string()],
        }
    }
}

/// Latency metric for performance evaluation
pub struct LatencyMetric {
    unit: TimeUnit,
}

#[derive(Debug, Clone, Copy)]
pub enum TimeUnit {
    Milliseconds,
    Seconds,
}

#[async_trait]
impl Metric for LatencyMetric {
    fn name(&self) -> &str {
        "latency"
    }

    fn version(&self) -> SemanticVersion {
        SemanticVersion::new(1, 0, 0)
    }

    async fn compute(&self, input: MetricInput) -> Result<MetricOutput, MetricError> {
        let start = std::time::Instant::now();

        let latency_ms = input.latency_ms.ok_or(MetricError::MissingField {
            field: "latency_ms".to_string(),
        })?;

        let value = match self.unit {
            TimeUnit::Milliseconds => latency_ms as f64,
            TimeUnit::Seconds => latency_ms as f64 / 1000.0,
        };

        Ok(MetricOutput {
            value: MetricValue::Scalar(value),
            confidence: Some(1.0),
            breakdown: None,
            explanation: Some(format!("Latency: {:.2}ms", latency_ms)),
            computation_time_ms: start.elapsed().as_millis() as u64,
        })
    }

    fn properties(&self) -> MetricProperties {
        MetricProperties {
            metric_type: MetricType::Performance,
            value_range: Some(ValueRange {
                min: 0.0,
                max: f64::MAX,
                inclusive: true,
            }),
            higher_is_better: false, // Lower latency is better
            requires_reference: false,
            supports_batch: true,
            deterministic: true,
            aggregation_method: AggregationMethod::Mean,
            tags: vec!["performance".to_string(), "latency".to_string()],
        }
    }
}

/// Token cost metric
pub struct TokenCostMetric {
    cost_per_input_token: f64,
    cost_per_output_token: f64,
}

impl TokenCostMetric {
    pub fn new(cost_per_input_token: f64, cost_per_output_token: f64) -> Self {
        Self {
            cost_per_input_token,
            cost_per_output_token,
        }
    }
}

#[async_trait]
impl Metric for TokenCostMetric {
    fn name(&self) -> &str {
        "token_cost"
    }

    fn version(&self) -> SemanticVersion {
        SemanticVersion::new(1, 0, 0)
    }

    async fn compute(&self, input: MetricInput) -> Result<MetricOutput, MetricError> {
        let start = std::time::Instant::now();

        let tokens = input.token_counts.ok_or(MetricError::MissingField {
            field: "token_counts".to_string(),
        })?;

        let input_cost = tokens.prompt_tokens as f64 * self.cost_per_input_token;
        let output_cost = tokens.completion_tokens as f64 * self.cost_per_output_token;
        let total_cost = input_cost + output_cost;

        let mut breakdown = HashMap::new();
        breakdown.insert("input_cost".to_string(), input_cost);
        breakdown.insert("output_cost".to_string(), output_cost);
        breakdown.insert("input_tokens".to_string(), tokens.prompt_tokens as f64);
        breakdown.insert("output_tokens".to_string(), tokens.completion_tokens as f64);

        Ok(MetricOutput {
            value: MetricValue::Scalar(total_cost),
            confidence: Some(1.0),
            breakdown: Some(breakdown),
            explanation: Some(format!(
                "Cost: ${:.6} ({} input + {} output tokens)",
                total_cost, tokens.prompt_tokens, tokens.completion_tokens
            )),
            computation_time_ms: start.elapsed().as_millis() as u64,
        })
    }

    fn properties(&self) -> MetricProperties {
        MetricProperties {
            metric_type: MetricType::Cost,
            value_range: Some(ValueRange {
                min: 0.0,
                max: f64::MAX,
                inclusive: true,
            }),
            higher_is_better: false,
            requires_reference: false,
            supports_batch: true,
            deterministic: true,
            aggregation_method: AggregationMethod::Sum,
            tags: vec!["cost".to_string(), "tokens".to_string()],
        }
    }
}

/// LLM-as-judge metric for subjective evaluation
pub struct LlmJudgeMetric {
    judge_client: Arc<dyn LlmClient>,
    evaluation_prompt: String,
    scoring_rubric: ScoringRubric,
    model_config: ModelConfig,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ScoringRubric {
    pub criteria: Vec<EvaluationCriterion>,
    pub scale: ScoreScale,
    pub aggregation: AggregationMethod,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EvaluationCriterion {
    pub name: String,
    pub description: String,
    pub weight: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ScoreScale {
    pub min: f64,
    pub max: f64,
    pub labels: Option<HashMap<i32, String>>, // e.g., 1: "Poor", 5: "Excellent"
}

#[async_trait]
pub trait LlmClient: Send + Sync {
    async fn complete(&self, prompt: &str, config: &ModelConfig) -> Result<LlmResponse, MetricError>;
}

#[derive(Debug, Clone)]
pub struct LlmResponse {
    pub content: String,
    pub usage: TokenCounts,
    pub latency_ms: u64,
}

impl LlmJudgeMetric {
    pub fn new(
        judge_client: Arc<dyn LlmClient>,
        evaluation_prompt: String,
        scoring_rubric: ScoringRubric,
        model_config: ModelConfig,
    ) -> Self {
        Self {
            judge_client,
            evaluation_prompt,
            scoring_rubric,
            model_config,
        }
    }

    fn build_judge_prompt(&self, input: &MetricInput) -> String {
        let criteria_text = self
            .scoring_rubric
            .criteria
            .iter()
            .map(|c| format!("- {}: {}", c.name, c.description))
            .collect::<Vec<_>>()
            .join("\n");

        format!(
            "{}\n\nCriteria:\n{}\n\nPrompt: {}\n\nResponse: {}\n\nProvide scores as JSON.",
            self.evaluation_prompt, criteria_text, input.prompt, input.response
        )
    }

    fn parse_scores(&self, response: &str) -> Result<HashMap<String, f64>, MetricError> {
        // Extract JSON from response
        let json_start = response.find('{').ok_or(MetricError::ParseError {
            message: "No JSON found in judge response".to_string(),
        })?;
        let json_end = response.rfind('}').ok_or(MetricError::ParseError {
            message: "No JSON found in judge response".to_string(),
        })? + 1;

        let json_str = &response[json_start..json_end];
        let scores: HashMap<String, f64> =
            serde_json::from_str(json_str).map_err(|e| MetricError::ParseError {
                message: format!("Failed to parse scores: {}", e),
            })?;

        Ok(scores)
    }
}

#[async_trait]
impl Metric for LlmJudgeMetric {
    fn name(&self) -> &str {
        "llm_judge"
    }

    fn version(&self) -> SemanticVersion {
        SemanticVersion::new(1, 0, 0)
    }

    async fn compute(&self, input: MetricInput) -> Result<MetricOutput, MetricError> {
        let start = std::time::Instant::now();

        let judge_prompt = self.build_judge_prompt(&input);
        let response = self.judge_client.complete(&judge_prompt, &self.model_config).await?;

        let scores = self.parse_scores(&response.content)?;

        // Compute weighted average
        let mut weighted_sum = 0.0;
        let mut total_weight = 0.0;

        for criterion in &self.scoring_rubric.criteria {
            if let Some(&score) = scores.get(&criterion.name) {
                // Normalize to 0-1 scale
                let normalized = (score - self.scoring_rubric.scale.min)
                    / (self.scoring_rubric.scale.max - self.scoring_rubric.scale.min);
                weighted_sum += normalized * criterion.weight;
                total_weight += criterion.weight;
            }
        }

        let final_score = if total_weight > 0.0 {
            weighted_sum / total_weight
        } else {
            0.0
        };

        Ok(MetricOutput {
            value: MetricValue::Scalar(final_score),
            confidence: Some(0.8), // LLM judgments have some uncertainty
            breakdown: Some(scores),
            explanation: Some(format!("LLM judge score: {:.2}", final_score)),
            computation_time_ms: start.elapsed().as_millis() as u64,
        })
    }

    fn properties(&self) -> MetricProperties {
        MetricProperties {
            metric_type: MetricType::Quality,
            value_range: Some(ValueRange {
                min: 0.0,
                max: 1.0,
                inclusive: true,
            }),
            higher_is_better: true,
            requires_reference: false,
            supports_batch: false,
            deterministic: false,
            aggregation_method: AggregationMethod::Mean,
            tags: vec!["subjective".to_string(), "llm-judge".to_string()],
        }
    }
}
```

### 3.2 Benchmark Runner

```rust
/// Benchmark runner for executing metric evaluations
pub struct BenchmarkRunner {
    metric_registry: Arc<MetricRegistry>,
    executor: Arc<BenchmarkExecutor>,
    result_store: Arc<dyn BenchmarkResultStore>,
    event_publisher: Arc<dyn EventPublisher>,
    config: BenchmarkConfig,
}

#[derive(Debug, Clone)]
pub struct BenchmarkConfig {
    pub max_concurrent_evaluations: usize,
    pub timeout_per_evaluation: std::time::Duration,
    pub retry_policy: RetryPolicy,
    pub checkpoint_interval: Option<std::time::Duration>,
    pub progress_reporting: bool,
}

impl Default for BenchmarkConfig {
    fn default() -> Self {
        Self {
            max_concurrent_evaluations: 10,
            timeout_per_evaluation: std::time::Duration::from_secs(60),
            retry_policy: RetryPolicy::default(),
            checkpoint_interval: Some(std::time::Duration::from_secs(300)),
            progress_reporting: true,
        }
    }
}

#[derive(Debug, Clone)]
pub struct RetryPolicy {
    pub max_retries: u32,
    pub initial_delay: std::time::Duration,
    pub max_delay: std::time::Duration,
    pub exponential_base: f64,
}

impl Default for RetryPolicy {
    fn default() -> Self {
        Self {
            max_retries: 3,
            initial_delay: std::time::Duration::from_millis(100),
            max_delay: std::time::Duration::from_secs(10),
            exponential_base: 2.0,
        }
    }
}

/// Benchmark suite definition
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BenchmarkSuite {
    pub id: Uuid,
    pub name: String,
    pub description: Option<String>,
    pub metrics: Vec<MetricConfig>,
    pub test_cases: Vec<TestCase>,
    pub aggregation_config: AggregationConfig,
    pub created_at: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TestCase {
    pub id: Uuid,
    pub name: Option<String>,
    pub prompt: String,
    pub expected: Option<String>,
    pub context: HashMap<String, serde_json::Value>,
    pub tags: Vec<String>,
    pub weight: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AggregationConfig {
    pub method: AggregationMethod,
    pub weighted_by_test_case: bool,
    pub include_confidence_intervals: bool,
    pub bootstrap_samples: Option<usize>,
}

/// Model reference for benchmarking
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelRef {
    pub model_id: String,
    pub provider: ModelProvider,
    pub endpoint: Option<String>,
    pub parameters: ModelParameters,
}

impl BenchmarkRunner {
    pub fn new(
        metric_registry: Arc<MetricRegistry>,
        executor: Arc<BenchmarkExecutor>,
        result_store: Arc<dyn BenchmarkResultStore>,
        event_publisher: Arc<dyn EventPublisher>,
        config: BenchmarkConfig,
    ) -> Self {
        Self {
            metric_registry,
            executor,
            result_store,
            event_publisher,
            config,
        }
    }

    /// Run a benchmark suite against a model
    pub async fn run_benchmark(
        &self,
        model: ModelRef,
        suite: BenchmarkSuite,
        run_id: Option<RunId>,
    ) -> Result<BenchmarkResults, MetricError> {
        let benchmark_id = Uuid::new_v4();
        let start_time = Utc::now();

        // Publish start event
        self.event_publisher
            .publish(Event::BenchmarkStarted {
                benchmark_id,
                model_id: model.model_id.clone(),
                suite_name: suite.name.clone(),
                timestamp: start_time,
            })
            .await
            .ok();

        // Resolve metrics from registry
        let metrics = self.resolve_metrics(&suite.metrics).await?;

        // Create evaluation tasks
        let mut evaluation_results = Vec::with_capacity(suite.test_cases.len() * metrics.len());
        let semaphore = Arc::new(tokio::sync::Semaphore::new(
            self.config.max_concurrent_evaluations,
        ));

        // Process test cases with concurrency control
        let mut tasks = Vec::new();

        for test_case in &suite.test_cases {
            for (metric_config, metric) in metrics.iter() {
                let permit = semaphore.clone().acquire_owned().await.unwrap();
                let model = model.clone();
                let test_case = test_case.clone();
                let metric = metric.clone();
                let metric_config = metric_config.clone();
                let executor = self.executor.clone();
                let timeout = self.config.timeout_per_evaluation;
                let retry_policy = self.config.retry_policy.clone();

                let task = tokio::spawn(async move {
                    let result = Self::run_single_evaluation(
                        executor,
                        model,
                        test_case,
                        metric,
                        metric_config,
                        timeout,
                        retry_policy,
                    )
                    .await;
                    drop(permit);
                    result
                });

                tasks.push(task);
            }
        }

        // Collect results
        for task in tasks {
            match task.await {
                Ok(Ok(result)) => evaluation_results.push(result),
                Ok(Err(e)) => {
                    tracing::warn!("Evaluation failed: {}", e);
                    // Continue with other evaluations
                }
                Err(e) => {
                    tracing::error!("Task panicked: {}", e);
                }
            }
        }

        // Aggregate results
        let aggregated = self.aggregate_results(&evaluation_results, &suite.aggregation_config)?;

        let end_time = Utc::now();
        let results = BenchmarkResults {
            id: benchmark_id,
            model: model.clone(),
            suite_id: suite.id,
            suite_name: suite.name.clone(),
            run_id,
            individual_results: evaluation_results,
            aggregated_metrics: aggregated.metrics,
            statistical_summary: aggregated.summary,
            started_at: start_time,
            completed_at: end_time,
            duration: end_time - start_time,
        };

        // Store results
        self.result_store.save(&results).await?;

        // Publish completion event
        self.event_publisher
            .publish(Event::BenchmarkCompleted {
                benchmark_id,
                model_id: model.model_id,
                duration: results.duration,
                timestamp: end_time,
            })
            .await
            .ok();

        Ok(results)
    }

    /// Compare multiple models on the same benchmark
    pub async fn compare_models(
        &self,
        models: Vec<ModelRef>,
        suite: BenchmarkSuite,
    ) -> Result<ModelComparison, MetricError> {
        if models.len() < 2 {
            return Err(MetricError::ValidationError {
                message: "At least 2 models required for comparison".to_string(),
            });
        }

        let mut model_results = Vec::with_capacity(models.len());

        for model in models {
            let results = self.run_benchmark(model, suite.clone(), None).await?;
            model_results.push(results);
        }

        // Build comparison matrix
        let metric_names: Vec<_> = model_results
            .first()
            .map(|r| r.aggregated_metrics.keys().cloned().collect())
            .unwrap_or_default();

        let mut comparison_matrix = HashMap::new();

        for metric_name in &metric_names {
            let values: Vec<_> = model_results
                .iter()
                .map(|r| r.aggregated_metrics.get(metric_name).cloned())
                .collect();

            // Run pairwise statistical tests
            let mut pairwise_tests = Vec::new();
            for i in 0..model_results.len() {
                for j in (i + 1)..model_results.len() {
                    if let (Some(a), Some(b)) = (&values[i], &values[j]) {
                        let test = self.run_significance_test(
                            &model_results[i],
                            &model_results[j],
                            metric_name,
                        )?;
                        pairwise_tests.push(PairwiseTest {
                            model_a_index: i,
                            model_b_index: j,
                            test_result: test,
                        });
                    }
                }
            }

            comparison_matrix.insert(
                metric_name.clone(),
                MetricComparisonEntry {
                    values,
                    pairwise_tests,
                    best_model_index: None, // Computed after all tests
                },
            );
        }

        // Determine winners for each metric
        for (metric_name, entry) in &mut comparison_matrix {
            let properties = self
                .metric_registry
                .get_properties(metric_name)
                .await
                .ok();

            let higher_is_better = properties
                .map(|p| p.higher_is_better)
                .unwrap_or(true);

            entry.best_model_index = entry
                .values
                .iter()
                .enumerate()
                .filter_map(|(i, v)| v.as_ref().map(|agg| (i, agg.mean)))
                .max_by(|(_, a), (_, b)| {
                    if higher_is_better {
                        a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal)
                    } else {
                        b.partial_cmp(a).unwrap_or(std::cmp::Ordering::Equal)
                    }
                })
                .map(|(i, _)| i);
        }

        Ok(ModelComparison {
            models: model_results.iter().map(|r| r.model.clone()).collect(),
            suite_id: suite.id,
            suite_name: suite.name,
            comparison_matrix,
            generated_at: Utc::now(),
        })
    }

    async fn run_single_evaluation(
        executor: Arc<BenchmarkExecutor>,
        model: ModelRef,
        test_case: TestCase,
        metric: Arc<dyn Metric>,
        metric_config: MetricConfig,
        timeout: std::time::Duration,
        retry_policy: RetryPolicy,
    ) -> Result<EvaluationResult, MetricError> {
        let mut attempts = 0;
        let mut last_error = None;

        while attempts <= retry_policy.max_retries {
            let result = tokio::time::timeout(timeout, async {
                // Get model response
                let response = executor.get_model_response(&model, &test_case).await?;

                // Build metric input
                let input = MetricInput {
                    prompt: test_case.prompt.clone(),
                    response: response.content.clone(),
                    expected: test_case.expected.clone(),
                    context: test_case.context.clone(),
                    model_info: Some(ModelInfo {
                        provider: model.provider.to_string(),
                        model_id: model.model_id.clone(),
                        parameters: HashMap::new(),
                    }),
                    latency_ms: Some(response.latency_ms),
                    token_counts: Some(response.usage),
                };

                // Compute metric
                let output = metric.compute(input).await?;

                Ok::<_, MetricError>(EvaluationResult {
                    test_case_id: test_case.id,
                    metric_name: metric.name().to_string(),
                    metric_version: metric.version(),
                    output,
                    model_response: response.content,
                    evaluated_at: Utc::now(),
                })
            })
            .await;

            match result {
                Ok(Ok(eval_result)) => return Ok(eval_result),
                Ok(Err(e)) => {
                    last_error = Some(e);
                    attempts += 1;
                }
                Err(_) => {
                    last_error = Some(MetricError::Timeout);
                    attempts += 1;
                }
            }

            if attempts <= retry_policy.max_retries {
                let delay = std::cmp::min(
                    retry_policy.initial_delay
                        * (retry_policy.exponential_base.powi(attempts as i32 - 1) as u32),
                    retry_policy.max_delay,
                );
                tokio::time::sleep(delay).await;
            }
        }

        Err(last_error.unwrap_or(MetricError::Unknown))
    }

    async fn resolve_metrics(
        &self,
        configs: &[MetricConfig],
    ) -> Result<Vec<(MetricConfig, Arc<dyn Metric>)>, MetricError> {
        let mut resolved = Vec::with_capacity(configs.len());

        for config in configs {
            let metric = self
                .metric_registry
                .get(config.metric_id, &config.version)
                .await?;
            resolved.push((config.clone(), metric));
        }

        Ok(resolved)
    }

    fn aggregate_results(
        &self,
        results: &[EvaluationResult],
        config: &AggregationConfig,
    ) -> Result<AggregatedResults, MetricError> {
        let mut metrics_by_name: HashMap<String, Vec<f64>> = HashMap::new();

        for result in results {
            if let MetricValue::Scalar(value) = result.output.value {
                metrics_by_name
                    .entry(result.metric_name.clone())
                    .or_default()
                    .push(value);
            }
        }

        let mut aggregated_metrics = HashMap::new();

        for (name, values) in &metrics_by_name {
            let agg = compute_aggregations(&values, config)?;
            aggregated_metrics.insert(name.clone(), agg);
        }

        let summary = StatisticalSummary {
            total_evaluations: results.len(),
            successful_evaluations: results.len(), // TODO: track failures
            failed_evaluations: 0,
            metrics_computed: aggregated_metrics.len(),
        };

        Ok(AggregatedResults {
            metrics: aggregated_metrics,
            summary,
        })
    }

    fn run_significance_test(
        &self,
        results_a: &BenchmarkResults,
        results_b: &BenchmarkResults,
        metric_name: &str,
    ) -> Result<StatisticalTestResult, MetricError> {
        let values_a: Vec<f64> = results_a
            .individual_results
            .iter()
            .filter(|r| r.metric_name == metric_name)
            .filter_map(|r| match r.output.value {
                MetricValue::Scalar(v) => Some(v),
                _ => None,
            })
            .collect();

        let values_b: Vec<f64> = results_b
            .individual_results
            .iter()
            .filter(|r| r.metric_name == metric_name)
            .filter_map(|r| match r.output.value {
                MetricValue::Scalar(v) => Some(v),
                _ => None,
            })
            .collect();

        // Perform Mann-Whitney U test (non-parametric)
        let test_result = mann_whitney_u_test(&values_a, &values_b)?;

        Ok(test_result)
    }
}

/// Benchmark executor for getting model responses
pub struct BenchmarkExecutor {
    clients: HashMap<String, Arc<dyn LlmClient>>,
}

impl BenchmarkExecutor {
    pub fn new() -> Self {
        Self {
            clients: HashMap::new(),
        }
    }

    pub fn register_client(&mut self, provider: &str, client: Arc<dyn LlmClient>) {
        self.clients.insert(provider.to_string(), client);
    }

    pub async fn get_model_response(
        &self,
        model: &ModelRef,
        test_case: &TestCase,
    ) -> Result<LlmResponse, MetricError> {
        let client = self
            .clients
            .get(&model.provider.to_string())
            .ok_or_else(|| MetricError::ProviderNotFound {
                provider: model.provider.to_string(),
            })?;

        let config = ModelConfig {
            model_id: model.model_id.clone(),
            provider: model.provider.clone(),
            variant: None,
            endpoint: model.endpoint.clone(),
            parameters: model.parameters.clone(),
            credentials_ref: None,
        };

        client.complete(&test_case.prompt, &config).await
    }
}

/// Result types
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BenchmarkResults {
    pub id: Uuid,
    pub model: ModelRef,
    pub suite_id: Uuid,
    pub suite_name: String,
    pub run_id: Option<RunId>,
    pub individual_results: Vec<EvaluationResult>,
    pub aggregated_metrics: HashMap<String, AggregatedMetric>,
    pub statistical_summary: StatisticalSummary,
    pub started_at: DateTime<Utc>,
    pub completed_at: DateTime<Utc>,
    pub duration: chrono::Duration,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EvaluationResult {
    pub test_case_id: Uuid,
    pub metric_name: String,
    pub metric_version: SemanticVersion,
    pub output: MetricOutput,
    pub model_response: String,
    pub evaluated_at: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AggregatedMetric {
    pub mean: f64,
    pub std: f64,
    pub median: f64,
    pub min: f64,
    pub max: f64,
    pub count: usize,
    pub confidence_interval: Option<(f64, f64)>,
    pub percentiles: HashMap<String, f64>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StatisticalSummary {
    pub total_evaluations: usize,
    pub successful_evaluations: usize,
    pub failed_evaluations: usize,
    pub metrics_computed: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelComparison {
    pub models: Vec<ModelRef>,
    pub suite_id: Uuid,
    pub suite_name: String,
    pub comparison_matrix: HashMap<String, MetricComparisonEntry>,
    pub generated_at: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MetricComparisonEntry {
    pub values: Vec<Option<AggregatedMetric>>,
    pub pairwise_tests: Vec<PairwiseTest>,
    pub best_model_index: Option<usize>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PairwiseTest {
    pub model_a_index: usize,
    pub model_b_index: usize,
    pub test_result: StatisticalTestResult,
}

struct AggregatedResults {
    metrics: HashMap<String, AggregatedMetric>,
    summary: StatisticalSummary,
}

fn compute_aggregations(
    values: &[f64],
    config: &AggregationConfig,
) -> Result<AggregatedMetric, MetricError> {
    if values.is_empty() {
        return Err(MetricError::InsufficientData);
    }

    let n = values.len();
    let sum: f64 = values.iter().sum();
    let mean = sum / n as f64;

    let variance: f64 = values.iter().map(|v| (v - mean).powi(2)).sum::<f64>() / n as f64;
    let std = variance.sqrt();

    let mut sorted = values.to_vec();
    sorted.sort_by(|a, b| a.partial_cmp(b).unwrap());

    let median = if n % 2 == 0 {
        (sorted[n / 2 - 1] + sorted[n / 2]) / 2.0
    } else {
        sorted[n / 2]
    };

    let percentiles = compute_percentiles(&sorted);

    let confidence_interval = if config.include_confidence_intervals {
        let bootstrap_samples = config.bootstrap_samples.unwrap_or(1000);
        Some(bootstrap_confidence_interval(values, bootstrap_samples, 0.95)?)
    } else {
        None
    };

    Ok(AggregatedMetric {
        mean,
        std,
        median,
        min: *sorted.first().unwrap(),
        max: *sorted.last().unwrap(),
        count: n,
        confidence_interval,
        percentiles,
    })
}

fn compute_percentiles(sorted: &[f64]) -> HashMap<String, f64> {
    let mut percentiles = HashMap::new();
    let n = sorted.len();

    for p in [25, 50, 75, 90, 95, 99] {
        let idx = (p as f64 / 100.0 * n as f64).ceil() as usize - 1;
        let idx = idx.min(n - 1);
        percentiles.insert(format!("p{}", p), sorted[idx]);
    }

    percentiles
}

fn bootstrap_confidence_interval(
    values: &[f64],
    n_samples: usize,
    confidence: f64,
) -> Result<(f64, f64), MetricError> {
    use rand::prelude::*;

    let mut rng = rand::thread_rng();
    let n = values.len();
    let mut bootstrap_means = Vec::with_capacity(n_samples);

    for _ in 0..n_samples {
        let sample_sum: f64 = (0..n)
            .map(|_| values[rng.gen_range(0..n)])
            .sum();
        bootstrap_means.push(sample_sum / n as f64);
    }

    bootstrap_means.sort_by(|a, b| a.partial_cmp(b).unwrap());

    let alpha = 1.0 - confidence;
    let lower_idx = (alpha / 2.0 * n_samples as f64).floor() as usize;
    let upper_idx = ((1.0 - alpha / 2.0) * n_samples as f64).ceil() as usize - 1;

    Ok((bootstrap_means[lower_idx], bootstrap_means[upper_idx]))
}

fn mann_whitney_u_test(a: &[f64], b: &[f64]) -> Result<StatisticalTestResult, MetricError> {
    // Combine and rank
    let mut combined: Vec<(f64, bool)> = a.iter().map(|&v| (v, true)).collect();
    combined.extend(b.iter().map(|&v| (v, false)));
    combined.sort_by(|x, y| x.0.partial_cmp(&y.0).unwrap());

    // Assign ranks (handling ties)
    let mut ranks: Vec<(f64, bool)> = Vec::with_capacity(combined.len());
    let mut i = 0;
    while i < combined.len() {
        let mut j = i;
        while j < combined.len() && combined[j].0 == combined[i].0 {
            j += 1;
        }
        let avg_rank = (i + j + 1) as f64 / 2.0;
        for k in i..j {
            ranks.push((avg_rank, combined[k].1));
        }
        i = j;
    }

    // Calculate U statistic
    let r1: f64 = ranks.iter().filter(|(_, is_a)| *is_a).map(|(r, _)| r).sum();
    let n1 = a.len() as f64;
    let n2 = b.len() as f64;
    let u1 = r1 - (n1 * (n1 + 1.0)) / 2.0;
    let u2 = n1 * n2 - u1;
    let u = u1.min(u2);

    // Normal approximation for large samples
    let mean_u = n1 * n2 / 2.0;
    let std_u = ((n1 * n2 * (n1 + n2 + 1.0)) / 12.0).sqrt();
    let z = (u - mean_u) / std_u;

    // Two-tailed p-value (using normal approximation)
    let p_value = 2.0 * (1.0 - normal_cdf(z.abs()));

    // Effect size (rank-biserial correlation)
    let effect_size = 1.0 - (2.0 * u) / (n1 * n2);

    Ok(StatisticalTestResult {
        test_type: StatisticalTest::WilcoxonRankSum,
        statistic: u,
        p_value,
        significant: p_value < 0.05,
        effect_size: Some(effect_size),
        confidence_interval: None,
    })
}

fn normal_cdf(x: f64) -> f64 {
    // Approximation of the standard normal CDF
    0.5 * (1.0 + erf(x / std::f64::consts::SQRT_2))
}

fn erf(x: f64) -> f64 {
    // Approximation of the error function
    let a1 = 0.254829592;
    let a2 = -0.284496736;
    let a3 = 1.421413741;
    let a4 = -1.453152027;
    let a5 = 1.061405429;
    let p = 0.3275911;

    let sign = if x < 0.0 { -1.0 } else { 1.0 };
    let x = x.abs();
    let t = 1.0 / (1.0 + p * x);
    let y = 1.0 - (((((a5 * t + a4) * t) + a3) * t + a2) * t + a1) * t * (-x * x).exp();

    sign * y
}
```

### 3.3 Metric Registry

```rust
/// Registry for managing metric definitions
pub struct MetricRegistry {
    metrics: Arc<RwLock<HashMap<MetricId, Vec<RegisteredMetric>>>>,
    store: Arc<dyn MetricDefinitionStore>,
}

#[derive(Clone)]
struct RegisteredMetric {
    id: MetricId,
    version: SemanticVersion,
    definition: MetricDefinition,
    implementation: Arc<dyn Metric>,
    registered_at: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MetricDefinition {
    pub id: MetricId,
    pub name: String,
    pub description: String,
    pub version: SemanticVersion,
    pub properties: MetricProperties,
    pub parameters_schema: Option<serde_json::Value>,
    pub created_by: UserId,
    pub created_at: DateTime<Utc>,
    pub deprecated: bool,
    pub deprecation_message: Option<String>,
}

#[async_trait]
pub trait MetricDefinitionStore: Send + Sync {
    async fn save(&self, definition: &MetricDefinition) -> Result<(), MetricError>;
    async fn get(&self, id: MetricId, version: &SemanticVersion) -> Result<MetricDefinition, MetricError>;
    async fn list(&self, filter: MetricFilter) -> Result<Vec<MetricDefinition>, MetricError>;
    async fn get_latest_version(&self, id: MetricId) -> Result<MetricDefinition, MetricError>;
}

#[derive(Debug, Clone, Default)]
pub struct MetricFilter {
    pub metric_type: Option<MetricType>,
    pub tags: Option<Vec<String>>,
    pub include_deprecated: bool,
}

impl MetricRegistry {
    pub fn new(store: Arc<dyn MetricDefinitionStore>) -> Self {
        Self {
            metrics: Arc::new(RwLock::new(HashMap::new())),
            store,
        }
    }

    /// Register a new metric implementation
    pub async fn register(
        &self,
        definition: MetricDefinition,
        implementation: Arc<dyn Metric>,
    ) -> Result<(), MetricError> {
        // Validate implementation matches definition
        if implementation.name() != definition.name {
            return Err(MetricError::ValidationError {
                message: format!(
                    "Implementation name '{}' doesn't match definition name '{}'",
                    implementation.name(),
                    definition.name
                ),
            });
        }

        if implementation.version() != definition.version {
            return Err(MetricError::ValidationError {
                message: format!(
                    "Implementation version '{}' doesn't match definition version '{}'",
                    implementation.version(),
                    definition.version
                ),
            });
        }

        // Save definition to store
        self.store.save(&definition).await?;

        // Register in memory
        let registered = RegisteredMetric {
            id: definition.id,
            version: definition.version.clone(),
            definition,
            implementation,
            registered_at: Utc::now(),
        };

        let mut metrics = self.metrics.write().await;
        metrics
            .entry(registered.id)
            .or_insert_with(Vec::new)
            .push(registered);

        Ok(())
    }

    /// Get a metric by ID and version
    pub async fn get(
        &self,
        id: MetricId,
        version: &SemanticVersion,
    ) -> Result<Arc<dyn Metric>, MetricError> {
        let metrics = self.metrics.read().await;

        let versions = metrics.get(&id).ok_or(MetricError::NotFound {
            metric: format!("{:?}", id),
        })?;

        let registered = versions
            .iter()
            .find(|m| m.version == *version)
            .ok_or(MetricError::VersionNotFound {
                metric: format!("{:?}", id),
                version: version.to_string(),
            })?;

        Ok(registered.implementation.clone())
    }

    /// Get the latest version of a metric
    pub async fn get_latest(&self, id: MetricId) -> Result<Arc<dyn Metric>, MetricError> {
        let metrics = self.metrics.read().await;

        let versions = metrics.get(&id).ok_or(MetricError::NotFound {
            metric: format!("{:?}", id),
        })?;

        let registered = versions
            .iter()
            .max_by(|a, b| a.version.cmp(&b.version))
            .ok_or(MetricError::NotFound {
                metric: format!("{:?}", id),
            })?;

        Ok(registered.implementation.clone())
    }

    /// Get metric properties
    pub async fn get_properties(&self, name: &str) -> Result<MetricProperties, MetricError> {
        let metrics = self.metrics.read().await;

        for versions in metrics.values() {
            if let Some(registered) = versions.iter().find(|m| m.definition.name == name) {
                return Ok(registered.definition.properties.clone());
            }
        }

        Err(MetricError::NotFound {
            metric: name.to_string(),
        })
    }

    /// List all registered metrics
    pub async fn list(&self, filter: MetricFilter) -> Result<Vec<MetricDefinition>, MetricError> {
        self.store.list(filter).await
    }

    /// Register built-in metrics
    pub async fn register_builtins(&self, user_id: UserId) -> Result<(), MetricError> {
        // Exact match
        let exact_match = ExactMatchMetric::new(false, true);
        self.register(
            MetricDefinition {
                id: MetricId(Uuid::new_v4()),
                name: "exact_match".to_string(),
                description: "Exact string match between response and expected".to_string(),
                version: SemanticVersion::new(1, 0, 0),
                properties: exact_match.properties(),
                parameters_schema: None,
                created_by: user_id,
                created_at: Utc::now(),
                deprecated: false,
                deprecation_message: None,
            },
            Arc::new(exact_match),
        )
        .await?;

        // BLEU
        let bleu = BleuMetric::new(4, SmoothingMethod::Add1);
        self.register(
            MetricDefinition {
                id: MetricId(Uuid::new_v4()),
                name: "bleu".to_string(),
                description: "BLEU score for text similarity".to_string(),
                version: SemanticVersion::new(1, 0, 0),
                properties: bleu.properties(),
                parameters_schema: None,
                created_by: user_id,
                created_at: Utc::now(),
                deprecated: false,
                deprecation_message: None,
            },
            Arc::new(bleu),
        )
        .await?;

        // ROUGE-L
        let rouge_l = RougeMetric::new(RougeType::RougeL);
        self.register(
            MetricDefinition {
                id: MetricId(Uuid::new_v4()),
                name: "rouge_l".to_string(),
                description: "ROUGE-L score using longest common subsequence".to_string(),
                version: SemanticVersion::new(1, 0, 0),
                properties: rouge_l.properties(),
                parameters_schema: None,
                created_by: user_id,
                created_at: Utc::now(),
                deprecated: false,
                deprecation_message: None,
            },
            Arc::new(rouge_l),
        )
        .await?;

        // Latency
        let latency = LatencyMetric {
            unit: TimeUnit::Milliseconds,
        };
        self.register(
            MetricDefinition {
                id: MetricId(Uuid::new_v4()),
                name: "latency".to_string(),
                description: "Response latency in milliseconds".to_string(),
                version: SemanticVersion::new(1, 0, 0),
                properties: latency.properties(),
                parameters_schema: None,
                created_by: user_id,
                created_at: Utc::now(),
                deprecated: false,
                deprecation_message: None,
            },
            Arc::new(latency),
        )
        .await?;

        Ok(())
    }
}

/// Metric error types
#[derive(Debug, thiserror::Error)]
pub enum MetricError {
    #[error("Metric not found: {metric}")]
    NotFound { metric: String },

    #[error("Version not found for metric {metric}: {version}")]
    VersionNotFound { metric: String, version: String },

    #[error("Missing reference for metric {metric}")]
    MissingReference { metric: String },

    #[error("Missing field: {field}")]
    MissingField { field: String },

    #[error("Validation error: {message}")]
    ValidationError { message: String },

    #[error("Parse error: {message}")]
    ParseError { message: String },

    #[error("Provider not found: {provider}")]
    ProviderNotFound { provider: String },

    #[error("Computation timeout")]
    Timeout,

    #[error("Insufficient data for computation")]
    InsufficientData,

    #[error("Storage error: {0}")]
    Storage(#[from] Box<dyn std::error::Error + Send + Sync>),

    #[error("Unknown error")]
    Unknown,
}

#[async_trait]
pub trait BenchmarkResultStore: Send + Sync {
    async fn save(&self, results: &BenchmarkResults) -> Result<(), MetricError>;
    async fn get(&self, id: Uuid) -> Result<BenchmarkResults, MetricError>;
    async fn list_for_model(&self, model_id: &str) -> Result<Vec<BenchmarkResults>, MetricError>;
    async fn list_for_suite(&self, suite_id: Uuid) -> Result<Vec<BenchmarkResults>, MetricError>;
}

impl std::fmt::Display for ModelProvider {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            ModelProvider::OpenAI => write!(f, "openai"),
            ModelProvider::Anthropic => write!(f, "anthropic"),
            ModelProvider::Google => write!(f, "google"),
            ModelProvider::Cohere => write!(f, "cohere"),
            ModelProvider::HuggingFace => write!(f, "huggingface"),
            ModelProvider::AzureOpenAI => write!(f, "azure_openai"),
            ModelProvider::AWSBedrock => write!(f, "aws_bedrock"),
            ModelProvider::Custom { name } => write!(f, "custom:{}", name),
        }
    }
}

// Additional event types for benchmarking
impl Event {
    pub fn benchmark_started(
        benchmark_id: Uuid,
        model_id: String,
        suite_name: String,
    ) -> Self {
        Event::BenchmarkStarted {
            benchmark_id,
            model_id,
            suite_name,
            timestamp: Utc::now(),
        }
    }

    pub fn benchmark_completed(
        benchmark_id: Uuid,
        model_id: String,
        duration: chrono::Duration,
    ) -> Self {
        Event::BenchmarkCompleted {
            benchmark_id,
            model_id,
            duration,
            timestamp: Utc::now(),
        }
    }
}

// Extend Event enum
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(tag = "type", rename_all = "snake_case")]
pub enum Event {
    // ... previous variants ...
    BenchmarkStarted {
        benchmark_id: Uuid,
        model_id: String,
        suite_name: String,
        timestamp: DateTime<Utc>,
    },
    BenchmarkCompleted {
        benchmark_id: Uuid,
        model_id: String,
        duration: chrono::Duration,
        timestamp: DateTime<Utc>,
    },
    BenchmarkFailed {
        benchmark_id: Uuid,
        model_id: String,
        error: String,
        timestamp: DateTime<Utc>,
    },
}
```

---

## 4. Dataset Versioning System

### 4.1 Dataset Manager

```rust
//! Dataset versioning system with content-addressable storage
//! Provides immutable versioning, lineage tracking, and efficient storage

use async_trait::async_trait;
use std::sync::Arc;
use tokio::sync::RwLock;

/// Dataset manager for versioned dataset operations
pub struct DatasetManager {
    content_store: Arc<ContentStore>,
    metadata_store: Arc<dyn DatasetMetadataStore>,
    lineage_graph: Arc<RwLock<DatasetLineageGraph>>,
    vault_client: Arc<dyn DataVaultClient>,
    config: DatasetConfig,
}

#[derive(Debug, Clone)]
pub struct DatasetConfig {
    pub chunk_size: usize,
    pub compression: CompressionType,
    pub deduplication: bool,
    pub max_inline_size: usize,
    pub default_retention_days: Option<u32>,
}

impl Default for DatasetConfig {
    fn default() -> Self {
        Self {
            chunk_size: 64 * 1024 * 1024, // 64MB chunks
            compression: CompressionType::Zstd,
            deduplication: true,
            max_inline_size: 1024 * 1024, // 1MB inline threshold
            default_retention_days: None,
        }
    }
}

#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum CompressionType {
    None,
    Gzip,
    Zstd,
    Lz4,
    Snappy,
}

/// Dataset entity
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Dataset {
    pub id: DatasetId,
    pub name: String,
    pub description: Option<String>,
    pub schema: DatasetSchema,
    pub tags: Vec<String>,
    pub owner_id: UserId,
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
    pub metadata: HashMap<String, serde_json::Value>,
    pub governance: GovernanceConfig,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DatasetSchema {
    pub format: DataFormat,
    pub columns: Vec<ColumnDefinition>,
    pub primary_key: Option<Vec<String>>,
    pub partition_columns: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum DataFormat {
    Csv,
    Json,
    Jsonl,
    Parquet,
    Arrow,
    Custom(String),
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ColumnDefinition {
    pub name: String,
    pub data_type: DataType,
    pub nullable: bool,
    pub description: Option<String>,
    pub constraints: Vec<ColumnConstraint>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum DataType {
    String,
    Integer,
    Float,
    Boolean,
    Timestamp,
    Date,
    Binary,
    Array(Box<DataType>),
    Map { key: Box<DataType>, value: Box<DataType> },
    Struct(Vec<ColumnDefinition>),
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ColumnConstraint {
    NotNull,
    Unique,
    MinLength(usize),
    MaxLength(usize),
    Pattern(String),
    Range { min: Option<f64>, max: Option<f64> },
    Enum(Vec<String>),
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GovernanceConfig {
    pub classification: DataClassification,
    pub retention_days: Option<u32>,
    pub access_control: AccessControl,
    pub pii_columns: Vec<String>,
    pub audit_enabled: bool,
}

#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum DataClassification {
    Public,
    Internal,
    Confidential,
    Restricted,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AccessControl {
    pub read_roles: Vec<String>,
    pub write_roles: Vec<String>,
    pub admin_roles: Vec<String>,
}

/// Dataset version
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DatasetVersion {
    pub id: DatasetVersionId,
    pub dataset_id: DatasetId,
    pub version_number: u64,
    pub content_hash: ContentHash,
    pub parent_version_id: Option<DatasetVersionId>,
    pub transform: Option<Transform>,
    pub statistics: DatasetStatistics,
    pub manifest: DataManifest,
    pub created_at: DateTime<Utc>,
    pub created_by: UserId,
    pub message: Option<String>,
    pub tags: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DatasetStatistics {
    pub row_count: u64,
    pub size_bytes: u64,
    pub column_statistics: HashMap<String, ColumnStatistics>,
    pub computed_at: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ColumnStatistics {
    pub null_count: u64,
    pub distinct_count: Option<u64>,
    pub min_value: Option<serde_json::Value>,
    pub max_value: Option<serde_json::Value>,
    pub mean: Option<f64>,
    pub std: Option<f64>,
    pub histogram: Option<Histogram>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DataManifest {
    pub chunks: Vec<ChunkInfo>,
    pub total_size: u64,
    pub compression: CompressionType,
    pub format: DataFormat,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ChunkInfo {
    pub chunk_id: String,
    pub content_hash: ContentHash,
    pub offset: u64,
    pub size_bytes: u64,
    pub row_count: u64,
    pub storage_uri: String,
}

/// Transform that created a version
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Transform {
    pub transform_type: TransformType,
    pub parameters: HashMap<String, serde_json::Value>,
    pub source_versions: Vec<DatasetVersionId>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum TransformType {
    Import,
    Filter,
    Sample,
    Split,
    Join,
    Aggregate,
    Augment,
    Clean,
    Custom(String),
}

impl DatasetManager {
    pub fn new(
        content_store: Arc<ContentStore>,
        metadata_store: Arc<dyn DatasetMetadataStore>,
        vault_client: Arc<dyn DataVaultClient>,
        config: DatasetConfig,
    ) -> Self {
        Self {
            content_store,
            metadata_store,
            lineage_graph: Arc::new(RwLock::new(DatasetLineageGraph::new())),
            vault_client,
            config,
        }
    }

    /// Register a new dataset
    pub async fn register_dataset(
        &self,
        request: RegisterDatasetRequest,
    ) -> Result<Dataset, DatasetError> {
        // Validate schema
        self.validate_schema(&request.schema)?;

        let id = DatasetId(Uuid::new_v4());
        let now = Utc::now();

        let dataset = Dataset {
            id,
            name: request.name,
            description: request.description,
            schema: request.schema,
            tags: request.tags.unwrap_or_default(),
            owner_id: request.owner_id,
            created_at: now,
            updated_at: now,
            metadata: request.metadata.unwrap_or_default(),
            governance: request.governance.unwrap_or_else(|| GovernanceConfig {
                classification: DataClassification::Internal,
                retention_days: self.config.default_retention_days,
                access_control: AccessControl {
                    read_roles: vec!["researcher".to_string()],
                    write_roles: vec!["data_engineer".to_string()],
                    admin_roles: vec!["admin".to_string()],
                },
                pii_columns: Vec::new(),
                audit_enabled: true,
            }),
        };

        // Save to metadata store
        self.metadata_store.save_dataset(&dataset).await?;

        // Initialize lineage node
        let mut lineage = self.lineage_graph.write().await;
        lineage.add_dataset_node(dataset.id);

        Ok(dataset)
    }

    /// Create a new version of a dataset
    pub async fn create_version(
        &self,
        dataset_id: DatasetId,
        request: CreateVersionRequest,
    ) -> Result<DatasetVersion, DatasetError> {
        // Get dataset
        let dataset = self.metadata_store.get_dataset(dataset_id).await?;

        // Verify access
        self.vault_client
            .verify_access(dataset_id, &request.user_id, AccessType::Write)
            .await?;

        // Get next version number
        let version_number = self
            .metadata_store
            .get_next_version_number(dataset_id)
            .await?;

        // Store content
        let (manifest, content_hash) = match request.source {
            DataSource::Upload(data) => self.store_uploaded_data(data, &dataset.schema).await?,
            DataSource::Stream(stream) => {
                self.store_streamed_data(stream, &dataset.schema).await?
            }
            DataSource::Transform { source_version, transform } => {
                self.apply_transform(source_version, transform).await?
            }
            DataSource::External(uri) => self.import_external(uri, &dataset.schema).await?,
        };

        // Compute statistics
        let statistics = self.compute_statistics(&manifest, &dataset.schema).await?;

        let version_id = DatasetVersionId(Uuid::new_v4());
        let now = Utc::now();

        let version = DatasetVersion {
            id: version_id,
            dataset_id,
            version_number,
            content_hash,
            parent_version_id: request.parent_version_id,
            transform: request.transform,
            statistics,
            manifest,
            created_at: now,
            created_by: request.user_id,
            message: request.message,
            tags: request.tags.unwrap_or_default(),
        };

        // Save version
        self.metadata_store.save_version(&version).await?;

        // Update lineage
        let mut lineage = self.lineage_graph.write().await;
        lineage.add_version_node(version_id, dataset_id, request.parent_version_id);

        Ok(version)
    }

    /// Get a dataset version by reference
    pub async fn get_version(
        &self,
        dataset_id: DatasetId,
        selector: DatasetVersionSelector,
    ) -> Result<DatasetVersion, DatasetError> {
        match selector {
            DatasetVersionSelector::Latest => {
                self.metadata_store.get_latest_version(dataset_id).await
            }
            DatasetVersionSelector::Specific(version_id) => {
                self.metadata_store.get_version(version_id).await
            }
            DatasetVersionSelector::Tag(tag) => {
                self.metadata_store.get_version_by_tag(dataset_id, &tag).await
            }
            DatasetVersionSelector::ContentHash(hash) => {
                self.metadata_store.get_version_by_hash(dataset_id, &hash).await
            }
        }
    }

    /// Stream dataset content
    pub async fn stream_data(
        &self,
        version_id: DatasetVersionId,
        options: StreamOptions,
    ) -> Result<DataStream, DatasetError> {
        let version = self.metadata_store.get_version(version_id).await?;

        // Build stream from chunks
        let chunks = if let Some(ref columns) = options.columns {
            // Column projection - need to read only relevant chunks
            self.get_projected_chunks(&version.manifest, columns)?
        } else {
            version.manifest.chunks.clone()
        };

        let stream = self.content_store.stream_chunks(chunks, options).await?;

        Ok(stream)
    }

    /// Create dataset splits (train/val/test)
    pub async fn create_splits(
        &self,
        version_id: DatasetVersionId,
        config: SplitConfig,
    ) -> Result<DatasetSplits, DatasetError> {
        let version = self.metadata_store.get_version(version_id).await?;
        let dataset = self.metadata_store.get_dataset(version.dataset_id).await?;

        // Validate ratios
        let total: f64 = config.ratios.values().sum();
        if (total - 1.0).abs() > 0.001 {
            return Err(DatasetError::ValidationError {
                message: format!("Split ratios must sum to 1.0, got {}", total),
            });
        }

        // Load indices
        let total_rows = version.statistics.row_count;
        let mut indices: Vec<usize> = (0..total_rows as usize).collect();

        // Shuffle if needed
        if config.shuffle {
            use rand::prelude::*;
            let mut rng = match config.seed {
                Some(seed) => rand::rngs::StdRng::seed_from_u64(seed),
                None => rand::rngs::StdRng::from_entropy(),
            };
            indices.shuffle(&mut rng);
        }

        // Stratified splitting if requested
        let split_indices = if let Some(ref stratify_column) = config.stratify_by {
            self.stratified_split(&version, stratify_column, &config.ratios, config.seed)
                .await?
        } else {
            self.random_split(&indices, &config.ratios)
        };

        // Create split versions
        let mut splits = HashMap::new();

        for (split_name, indices) in split_indices {
            let split_version = self
                .create_split_version(
                    &version,
                    &dataset,
                    &split_name,
                    &indices,
                    config.user_id,
                )
                .await?;
            splits.insert(split_name, split_version.id);
        }

        Ok(DatasetSplits {
            source_version_id: version_id,
            splits,
            config,
            created_at: Utc::now(),
        })
    }

    /// Get dataset lineage
    pub async fn get_lineage(
        &self,
        dataset_id: DatasetId,
    ) -> Result<DatasetLineage, DatasetError> {
        let lineage = self.lineage_graph.read().await;
        lineage.get_dataset_lineage(dataset_id)
    }

    /// Compare two dataset versions
    pub async fn compare_versions(
        &self,
        version_a: DatasetVersionId,
        version_b: DatasetVersionId,
    ) -> Result<VersionComparison, DatasetError> {
        let a = self.metadata_store.get_version(version_a).await?;
        let b = self.metadata_store.get_version(version_b).await?;

        // Compare statistics
        let stats_diff = self.compare_statistics(&a.statistics, &b.statistics);

        // Compare schemas if from same dataset
        let schema_diff = if a.dataset_id == b.dataset_id {
            None
        } else {
            let dataset_a = self.metadata_store.get_dataset(a.dataset_id).await?;
            let dataset_b = self.metadata_store.get_dataset(b.dataset_id).await?;
            Some(self.compare_schemas(&dataset_a.schema, &dataset_b.schema))
        };

        // Compute content diff (sampling for large datasets)
        let content_diff = self.compute_content_diff(&a, &b).await?;

        Ok(VersionComparison {
            version_a: version_a,
            version_b: version_b,
            statistics_diff: stats_diff,
            schema_diff,
            content_diff,
            compared_at: Utc::now(),
        })
    }

    // Private helper methods

    fn validate_schema(&self, schema: &DatasetSchema) -> Result<(), DatasetError> {
        if schema.columns.is_empty() {
            return Err(DatasetError::ValidationError {
                message: "Schema must have at least one column".to_string(),
            });
        }

        // Check for duplicate column names
        let mut seen = std::collections::HashSet::new();
        for col in &schema.columns {
            if !seen.insert(&col.name) {
                return Err(DatasetError::ValidationError {
                    message: format!("Duplicate column name: {}", col.name),
                });
            }
        }

        // Validate primary key columns exist
        if let Some(ref pk) = schema.primary_key {
            for key_col in pk {
                if !schema.columns.iter().any(|c| &c.name == key_col) {
                    return Err(DatasetError::ValidationError {
                        message: format!("Primary key column not found: {}", key_col),
                    });
                }
            }
        }

        Ok(())
    }

    async fn store_uploaded_data(
        &self,
        data: Vec<u8>,
        schema: &DatasetSchema,
    ) -> Result<(DataManifest, ContentHash), DatasetError> {
        // Validate data against schema
        self.validate_data(&data, schema)?;

        // Chunk the data
        let chunks = self.chunk_data(&data)?;

        // Store chunks and build manifest
        let mut manifest_chunks = Vec::with_capacity(chunks.len());
        let mut hasher = sha2::Sha256::new();
        let mut offset = 0u64;

        for (idx, chunk) in chunks.into_iter().enumerate() {
            let chunk_hash = ContentHash::from_bytes(&chunk);
            hasher.update(chunk_hash.as_str().as_bytes());

            // Compress chunk
            let compressed = self.compress_chunk(&chunk)?;

            // Store chunk
            let storage_uri = self
                .content_store
                .store_chunk(&chunk_hash, &compressed)
                .await?;

            let chunk_info = ChunkInfo {
                chunk_id: format!("chunk_{:06}", idx),
                content_hash: chunk_hash,
                offset,
                size_bytes: chunk.len() as u64,
                row_count: 0, // Would be computed during chunking
                storage_uri,
            };

            offset += chunk.len() as u64;
            manifest_chunks.push(chunk_info);
        }

        let content_hash = ContentHash(hex::encode(hasher.finalize()));

        let manifest = DataManifest {
            chunks: manifest_chunks,
            total_size: offset,
            compression: self.config.compression,
            format: schema.format.clone(),
        };

        Ok((manifest, content_hash))
    }

    async fn store_streamed_data(
        &self,
        mut stream: DataInputStream,
        schema: &DatasetSchema,
    ) -> Result<(DataManifest, ContentHash), DatasetError> {
        let mut manifest_chunks = Vec::new();
        let mut hasher = sha2::Sha256::new();
        let mut offset = 0u64;
        let mut chunk_idx = 0;
        let mut buffer = Vec::with_capacity(self.config.chunk_size);

        while let Some(batch) = stream.next().await {
            let batch = batch?;
            buffer.extend_from_slice(&batch);

            // Flush when buffer reaches chunk size
            while buffer.len() >= self.config.chunk_size {
                let chunk: Vec<u8> = buffer.drain(..self.config.chunk_size).collect();
                let chunk_info = self
                    .store_single_chunk(&chunk, chunk_idx, offset, &mut hasher)
                    .await?;
                offset += chunk_info.size_bytes;
                chunk_idx += 1;
                manifest_chunks.push(chunk_info);
            }
        }

        // Store remaining data
        if !buffer.is_empty() {
            let chunk_info = self
                .store_single_chunk(&buffer, chunk_idx, offset, &mut hasher)
                .await?;
            offset += chunk_info.size_bytes;
            manifest_chunks.push(chunk_info);
        }

        let content_hash = ContentHash(hex::encode(hasher.finalize()));

        let manifest = DataManifest {
            chunks: manifest_chunks,
            total_size: offset,
            compression: self.config.compression,
            format: schema.format.clone(),
        };

        Ok((manifest, content_hash))
    }

    async fn store_single_chunk(
        &self,
        chunk: &[u8],
        idx: usize,
        offset: u64,
        hasher: &mut sha2::Sha256,
    ) -> Result<ChunkInfo, DatasetError> {
        use sha2::Digest;

        let chunk_hash = ContentHash::from_bytes(chunk);
        hasher.update(chunk_hash.as_str().as_bytes());

        let compressed = self.compress_chunk(chunk)?;
        let storage_uri = self
            .content_store
            .store_chunk(&chunk_hash, &compressed)
            .await?;

        Ok(ChunkInfo {
            chunk_id: format!("chunk_{:06}", idx),
            content_hash: chunk_hash,
            offset,
            size_bytes: chunk.len() as u64,
            row_count: 0,
            storage_uri,
        })
    }

    async fn apply_transform(
        &self,
        source_version: DatasetVersionId,
        transform: Transform,
    ) -> Result<(DataManifest, ContentHash), DatasetError> {
        let source = self.metadata_store.get_version(source_version).await?;

        match transform.transform_type {
            TransformType::Filter => {
                let predicate = transform
                    .parameters
                    .get("predicate")
                    .ok_or(DatasetError::MissingParameter {
                        parameter: "predicate".to_string(),
                    })?;
                self.apply_filter(&source, predicate).await
            }
            TransformType::Sample => {
                let fraction = transform
                    .parameters
                    .get("fraction")
                    .and_then(|v| v.as_f64())
                    .ok_or(DatasetError::MissingParameter {
                        parameter: "fraction".to_string(),
                    })?;
                let seed = transform.parameters.get("seed").and_then(|v| v.as_u64());
                self.apply_sample(&source, fraction, seed).await
            }
            TransformType::Split => {
                // Handled separately via create_splits
                Err(DatasetError::ValidationError {
                    message: "Use create_splits for split operations".to_string(),
                })
            }
            _ => Err(DatasetError::UnsupportedTransform {
                transform: format!("{:?}", transform.transform_type),
            }),
        }
    }

    async fn apply_filter(
        &self,
        source: &DatasetVersion,
        predicate: &serde_json::Value,
    ) -> Result<(DataManifest, ContentHash), DatasetError> {
        // Stream source data, apply filter, store results
        let stream = self
            .stream_data(source.id, StreamOptions::default())
            .await?;

        let filtered = self.filter_stream(stream, predicate).await?;

        let dataset = self.metadata_store.get_dataset(source.dataset_id).await?;
        self.store_streamed_data(filtered, &dataset.schema).await
    }

    async fn apply_sample(
        &self,
        source: &DatasetVersion,
        fraction: f64,
        seed: Option<u64>,
    ) -> Result<(DataManifest, ContentHash), DatasetError> {
        use rand::prelude::*;

        let total_rows = source.statistics.row_count;
        let sample_size = (total_rows as f64 * fraction).ceil() as usize;

        // Generate sample indices
        let mut rng = match seed {
            Some(s) => rand::rngs::StdRng::seed_from_u64(s),
            None => rand::rngs::StdRng::from_entropy(),
        };

        let mut indices: Vec<usize> = (0..total_rows as usize).collect();
        indices.shuffle(&mut rng);
        indices.truncate(sample_size);
        indices.sort();

        // Stream and filter by indices
        let stream = self
            .stream_data(source.id, StreamOptions::default())
            .await?;
        let sampled = self.sample_stream(stream, &indices).await?;

        let dataset = self.metadata_store.get_dataset(source.dataset_id).await?;
        self.store_streamed_data(sampled, &dataset.schema).await
    }

    async fn import_external(
        &self,
        uri: String,
        schema: &DatasetSchema,
    ) -> Result<(DataManifest, ContentHash), DatasetError> {
        // Fetch data from external URI
        let data = self.fetch_external_data(&uri).await?;
        self.store_uploaded_data(data, schema).await
    }

    async fn compute_statistics(
        &self,
        manifest: &DataManifest,
        schema: &DatasetSchema,
    ) -> Result<DatasetStatistics, DatasetError> {
        let mut row_count = 0u64;
        let mut column_stats: HashMap<String, ColumnStatsAccumulator> = schema
            .columns
            .iter()
            .map(|c| (c.name.clone(), ColumnStatsAccumulator::new(&c.data_type)))
            .collect();

        // Stream through chunks to compute statistics
        for chunk in &manifest.chunks {
            let data = self.content_store.read_chunk(&chunk.storage_uri).await?;
            let decompressed = self.decompress_chunk(&data)?;

            // Parse and accumulate stats
            let records = self.parse_chunk(&decompressed, &manifest.format)?;
            row_count += records.len() as u64;

            for record in records {
                for (col_name, accumulator) in &mut column_stats {
                    if let Some(value) = record.get(col_name) {
                        accumulator.add(value);
                    } else {
                        accumulator.add_null();
                    }
                }
            }
        }

        let column_statistics: HashMap<String, ColumnStatistics> = column_stats
            .into_iter()
            .map(|(name, acc)| (name, acc.finalize()))
            .collect();

        Ok(DatasetStatistics {
            row_count,
            size_bytes: manifest.total_size,
            column_statistics,
            computed_at: Utc::now(),
        })
    }

    fn chunk_data(&self, data: &[u8]) -> Result<Vec<Vec<u8>>, DatasetError> {
        let mut chunks = Vec::new();
        let mut offset = 0;

        while offset < data.len() {
            let end = std::cmp::min(offset + self.config.chunk_size, data.len());
            chunks.push(data[offset..end].to_vec());
            offset = end;
        }

        Ok(chunks)
    }

    fn compress_chunk(&self, data: &[u8]) -> Result<Vec<u8>, DatasetError> {
        match self.config.compression {
            CompressionType::None => Ok(data.to_vec()),
            CompressionType::Gzip => {
                use flate2::write::GzEncoder;
                use flate2::Compression;
                use std::io::Write;

                let mut encoder = GzEncoder::new(Vec::new(), Compression::default());
                encoder.write_all(data)?;
                Ok(encoder.finish()?)
            }
            CompressionType::Zstd => {
                let compressed = zstd::encode_all(data, 3)?;
                Ok(compressed)
            }
            CompressionType::Lz4 => {
                let compressed = lz4_flex::compress_prepend_size(data);
                Ok(compressed)
            }
            CompressionType::Snappy => {
                let mut encoder = snap::raw::Encoder::new();
                let compressed = encoder.compress_vec(data)?;
                Ok(compressed)
            }
        }
    }

    fn decompress_chunk(&self, data: &[u8]) -> Result<Vec<u8>, DatasetError> {
        match self.config.compression {
            CompressionType::None => Ok(data.to_vec()),
            CompressionType::Gzip => {
                use flate2::read::GzDecoder;
                use std::io::Read;

                let mut decoder = GzDecoder::new(data);
                let mut decompressed = Vec::new();
                decoder.read_to_end(&mut decompressed)?;
                Ok(decompressed)
            }
            CompressionType::Zstd => {
                let decompressed = zstd::decode_all(data)?;
                Ok(decompressed)
            }
            CompressionType::Lz4 => {
                let decompressed = lz4_flex::decompress_size_prepended(data)?;
                Ok(decompressed)
            }
            CompressionType::Snappy => {
                let mut decoder = snap::raw::Decoder::new();
                let decompressed = decoder.decompress_vec(data)?;
                Ok(decompressed)
            }
        }
    }

    fn validate_data(&self, _data: &[u8], _schema: &DatasetSchema) -> Result<(), DatasetError> {
        // Validate data format matches schema
        // This would involve parsing and type checking
        Ok(())
    }

    fn random_split(
        &self,
        indices: &[usize],
        ratios: &HashMap<String, f64>,
    ) -> HashMap<String, Vec<usize>> {
        let mut result = HashMap::new();
        let mut start = 0;

        let total = indices.len();
        for (name, ratio) in ratios {
            let count = (total as f64 * ratio).round() as usize;
            let end = std::cmp::min(start + count, total);
            result.insert(name.clone(), indices[start..end].to_vec());
            start = end;
        }

        result
    }

    async fn stratified_split(
        &self,
        version: &DatasetVersion,
        stratify_column: &str,
        ratios: &HashMap<String, f64>,
        seed: Option<u64>,
    ) -> Result<HashMap<String, Vec<usize>>, DatasetError> {
        use rand::prelude::*;

        // Group indices by stratification column value
        let mut groups: HashMap<String, Vec<usize>> = HashMap::new();

        // Stream data and build groups
        let stream = self
            .stream_data(version.id, StreamOptions {
                columns: Some(vec![stratify_column.to_string()]),
                ..Default::default()
            })
            .await?;

        let mut idx = 0;
        let mut stream_pin = Box::pin(stream);
        while let Some(batch) = stream_pin.next().await {
            let batch = batch?;
            for record in batch {
                let value = record
                    .get(stratify_column)
                    .map(|v| v.to_string())
                    .unwrap_or_else(|| "null".to_string());
                groups.entry(value).or_default().push(idx);
                idx += 1;
            }
        }

        // Shuffle each group
        let mut rng = match seed {
            Some(s) => rand::rngs::StdRng::seed_from_u64(s),
            None => rand::rngs::StdRng::from_entropy(),
        };

        for indices in groups.values_mut() {
            indices.shuffle(&mut rng);
        }

        // Split each group according to ratios
        let mut result: HashMap<String, Vec<usize>> = ratios
            .keys()
            .map(|k| (k.clone(), Vec::new()))
            .collect();

        for (_value, indices) in groups {
            let group_splits = self.random_split(&indices, ratios);
            for (split_name, split_indices) in group_splits {
                result.get_mut(&split_name).unwrap().extend(split_indices);
            }
        }

        Ok(result)
    }

    async fn create_split_version(
        &self,
        source: &DatasetVersion,
        dataset: &Dataset,
        split_name: &str,
        indices: &[usize],
        user_id: UserId,
    ) -> Result<DatasetVersion, DatasetError> {
        // Stream source and select only specified indices
        let stream = self
            .stream_data(source.id, StreamOptions::default())
            .await?;
        let filtered = self.select_indices_stream(stream, indices).await?;

        let (manifest, content_hash) = self
            .store_streamed_data(filtered, &dataset.schema)
            .await?;
        let statistics = self.compute_statistics(&manifest, &dataset.schema).await?;

        let version_number = self
            .metadata_store
            .get_next_version_number(dataset.id)
            .await?;

        let version = DatasetVersion {
            id: DatasetVersionId(Uuid::new_v4()),
            dataset_id: dataset.id,
            version_number,
            content_hash,
            parent_version_id: Some(source.id),
            transform: Some(Transform {
                transform_type: TransformType::Split,
                parameters: {
                    let mut p = HashMap::new();
                    p.insert("split_name".to_string(), serde_json::json!(split_name));
                    p.insert("indices_count".to_string(), serde_json::json!(indices.len()));
                    p
                },
                source_versions: vec![source.id],
            }),
            statistics,
            manifest,
            created_at: Utc::now(),
            created_by: user_id,
            message: Some(format!("Split: {}", split_name)),
            tags: vec![format!("split:{}", split_name)],
        };

        self.metadata_store.save_version(&version).await?;

        Ok(version)
    }

    fn compare_statistics(
        &self,
        a: &DatasetStatistics,
        b: &DatasetStatistics,
    ) -> StatisticsDiff {
        StatisticsDiff {
            row_count_diff: b.row_count as i64 - a.row_count as i64,
            size_diff: b.size_bytes as i64 - a.size_bytes as i64,
            column_diffs: HashMap::new(), // Would compute per-column diffs
        }
    }

    fn compare_schemas(&self, a: &DatasetSchema, b: &DatasetSchema) -> SchemaDiff {
        let a_cols: std::collections::HashSet<_> = a.columns.iter().map(|c| &c.name).collect();
        let b_cols: std::collections::HashSet<_> = b.columns.iter().map(|c| &c.name).collect();

        SchemaDiff {
            added_columns: b_cols.difference(&a_cols).map(|s| (*s).clone()).collect(),
            removed_columns: a_cols.difference(&b_cols).map(|s| (*s).clone()).collect(),
            type_changes: Vec::new(), // Would detect type changes for common columns
        }
    }

    async fn compute_content_diff(
        &self,
        _a: &DatasetVersion,
        _b: &DatasetVersion,
    ) -> Result<ContentDiff, DatasetError> {
        // For large datasets, sample and compare
        Ok(ContentDiff {
            sample_based: true,
            sample_size: 1000,
            added_rows_estimate: 0,
            removed_rows_estimate: 0,
            modified_rows_estimate: 0,
        })
    }

    fn get_projected_chunks(
        &self,
        manifest: &DataManifest,
        _columns: &[String],
    ) -> Result<Vec<ChunkInfo>, DatasetError> {
        // For columnar formats, return only relevant chunks
        // For row-based formats, return all chunks
        Ok(manifest.chunks.clone())
    }

    fn parse_chunk(
        &self,
        _data: &[u8],
        _format: &DataFormat,
    ) -> Result<Vec<HashMap<String, serde_json::Value>>, DatasetError> {
        // Parse chunk based on format
        Ok(Vec::new())
    }

    async fn filter_stream(
        &self,
        _stream: DataStream,
        _predicate: &serde_json::Value,
    ) -> Result<DataInputStream, DatasetError> {
        // Apply filter predicate to stream
        unimplemented!()
    }

    async fn sample_stream(
        &self,
        _stream: DataStream,
        _indices: &[usize],
    ) -> Result<DataInputStream, DatasetError> {
        // Sample stream at specified indices
        unimplemented!()
    }

    async fn select_indices_stream(
        &self,
        _stream: DataStream,
        _indices: &[usize],
    ) -> Result<DataInputStream, DatasetError> {
        // Select specific indices from stream
        unimplemented!()
    }

    async fn fetch_external_data(&self, _uri: &str) -> Result<Vec<u8>, DatasetError> {
        // Fetch data from external source
        unimplemented!()
    }
}

// Type aliases and helper structs
pub type DataStream = tokio::sync::mpsc::Receiver<Result<Vec<HashMap<String, serde_json::Value>>, DatasetError>>;
pub type DataInputStream = tokio::sync::mpsc::Receiver<Result<Vec<u8>, DatasetError>>;

#[derive(Debug, Clone, Default)]
pub struct StreamOptions {
    pub columns: Option<Vec<String>>,
    pub limit: Option<usize>,
    pub offset: Option<usize>,
    pub filter: Option<serde_json::Value>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SplitConfig {
    pub ratios: HashMap<String, f64>,
    pub shuffle: bool,
    pub seed: Option<u64>,
    pub stratify_by: Option<String>,
    pub user_id: UserId,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DatasetSplits {
    pub source_version_id: DatasetVersionId,
    pub splits: HashMap<String, DatasetVersionId>,
    pub config: SplitConfig,
    pub created_at: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DatasetLineage {
    pub dataset_id: DatasetId,
    pub versions: Vec<LineageNode>,
    pub edges: Vec<LineageEdge>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct VersionComparison {
    pub version_a: DatasetVersionId,
    pub version_b: DatasetVersionId,
    pub statistics_diff: StatisticsDiff,
    pub schema_diff: Option<SchemaDiff>,
    pub content_diff: ContentDiff,
    pub compared_at: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StatisticsDiff {
    pub row_count_diff: i64,
    pub size_diff: i64,
    pub column_diffs: HashMap<String, ColumnStatsDiff>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ColumnStatsDiff {
    pub null_count_diff: i64,
    pub distinct_count_diff: Option<i64>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SchemaDiff {
    pub added_columns: Vec<String>,
    pub removed_columns: Vec<String>,
    pub type_changes: Vec<TypeChange>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TypeChange {
    pub column: String,
    pub old_type: DataType,
    pub new_type: DataType,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ContentDiff {
    pub sample_based: bool,
    pub sample_size: usize,
    pub added_rows_estimate: u64,
    pub removed_rows_estimate: u64,
    pub modified_rows_estimate: u64,
}

/// Request types
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RegisterDatasetRequest {
    pub name: String,
    pub description: Option<String>,
    pub schema: DatasetSchema,
    pub tags: Option<Vec<String>>,
    pub owner_id: UserId,
    pub metadata: Option<HashMap<String, serde_json::Value>>,
    pub governance: Option<GovernanceConfig>,
}

#[derive(Debug, Clone)]
pub struct CreateVersionRequest {
    pub source: DataSource,
    pub parent_version_id: Option<DatasetVersionId>,
    pub transform: Option<Transform>,
    pub message: Option<String>,
    pub tags: Option<Vec<String>>,
    pub user_id: UserId,
}

#[derive(Debug)]
pub enum DataSource {
    Upload(Vec<u8>),
    Stream(DataInputStream),
    Transform {
        source_version: DatasetVersionId,
        transform: Transform,
    },
    External(String),
}

/// Column statistics accumulator
struct ColumnStatsAccumulator {
    data_type: DataType,
    count: u64,
    null_count: u64,
    values: Vec<serde_json::Value>,
    sum: f64,
    sum_sq: f64,
}

impl ColumnStatsAccumulator {
    fn new(data_type: &DataType) -> Self {
        Self {
            data_type: data_type.clone(),
            count: 0,
            null_count: 0,
            values: Vec::new(),
            sum: 0.0,
            sum_sq: 0.0,
        }
    }

    fn add(&mut self, value: &serde_json::Value) {
        self.count += 1;
        self.values.push(value.clone());

        if let Some(n) = value.as_f64() {
            self.sum += n;
            self.sum_sq += n * n;
        }
    }

    fn add_null(&mut self) {
        self.count += 1;
        self.null_count += 1;
    }

    fn finalize(self) -> ColumnStatistics {
        let distinct_count = if self.values.len() < 10000 {
            let unique: std::collections::HashSet<_> =
                self.values.iter().map(|v| v.to_string()).collect();
            Some(unique.len() as u64)
        } else {
            None
        };

        let (mean, std) = if self.count > self.null_count {
            let n = (self.count - self.null_count) as f64;
            let mean = self.sum / n;
            let variance = (self.sum_sq / n) - (mean * mean);
            (Some(mean), Some(variance.sqrt()))
        } else {
            (None, None)
        };

        ColumnStatistics {
            null_count: self.null_count,
            distinct_count,
            min_value: self.values.iter().min_by(|a, b| {
                a.to_string().cmp(&b.to_string())
            }).cloned(),
            max_value: self.values.iter().max_by(|a, b| {
                a.to_string().cmp(&b.to_string())
            }).cloned(),
            mean,
            std,
            histogram: None,
        }
    }
}

/// Dataset error types
#[derive(Debug, thiserror::Error)]
pub enum DatasetError {
    #[error("Dataset not found: {id:?}")]
    NotFound { id: DatasetId },

    #[error("Version not found: {id:?}")]
    VersionNotFound { id: DatasetVersionId },

    #[error("Validation error: {message}")]
    ValidationError { message: String },

    #[error("Missing parameter: {parameter}")]
    MissingParameter { parameter: String },

    #[error("Unsupported transform: {transform}")]
    UnsupportedTransform { transform: String },

    #[error("Access denied")]
    AccessDenied,

    #[error("Storage error: {0}")]
    Storage(#[from] std::io::Error),

    #[error("Compression error: {message}")]
    CompressionError { message: String },
}

/// Storage traits
#[async_trait]
pub trait DatasetMetadataStore: Send + Sync {
    async fn save_dataset(&self, dataset: &Dataset) -> Result<(), DatasetError>;
    async fn get_dataset(&self, id: DatasetId) -> Result<Dataset, DatasetError>;
    async fn save_version(&self, version: &DatasetVersion) -> Result<(), DatasetError>;
    async fn get_version(&self, id: DatasetVersionId) -> Result<DatasetVersion, DatasetError>;
    async fn get_latest_version(&self, dataset_id: DatasetId) -> Result<DatasetVersion, DatasetError>;
    async fn get_version_by_tag(&self, dataset_id: DatasetId, tag: &str) -> Result<DatasetVersion, DatasetError>;
    async fn get_version_by_hash(&self, dataset_id: DatasetId, hash: &ContentHash) -> Result<DatasetVersion, DatasetError>;
    async fn get_next_version_number(&self, dataset_id: DatasetId) -> Result<u64, DatasetError>;
}

#[async_trait]
pub trait DataVaultClient: Send + Sync {
    async fn verify_access(
        &self,
        dataset_id: DatasetId,
        user_id: &UserId,
        access_type: AccessType,
    ) -> Result<(), DatasetError>;
}

#[derive(Debug, Clone, Copy)]
pub enum AccessType {
    Read,
    Write,
    Admin,
}

/// Content store for chunk storage
pub struct ContentStore {
    storage: Arc<dyn ObjectStorage>,
    deduplication: bool,
}

#[async_trait]
pub trait ObjectStorage: Send + Sync {
    async fn put(&self, key: &str, data: &[u8]) -> Result<String, DatasetError>;
    async fn get(&self, key: &str) -> Result<Vec<u8>, DatasetError>;
    async fn exists(&self, key: &str) -> Result<bool, DatasetError>;
    async fn delete(&self, key: &str) -> Result<(), DatasetError>;
}

impl ContentStore {
    pub fn new(storage: Arc<dyn ObjectStorage>, deduplication: bool) -> Self {
        Self {
            storage,
            deduplication,
        }
    }

    pub async fn store_chunk(
        &self,
        hash: &ContentHash,
        data: &[u8],
    ) -> Result<String, DatasetError> {
        let key = format!("chunks/{}/{}", &hash.0[..2], hash.0);

        // Check for existing chunk if deduplication enabled
        if self.deduplication {
            if self.storage.exists(&key).await? {
                return Ok(key);
            }
        }

        self.storage.put(&key, data).await
    }

    pub async fn read_chunk(&self, uri: &str) -> Result<Vec<u8>, DatasetError> {
        self.storage.get(uri).await
    }

    pub async fn stream_chunks(
        &self,
        chunks: Vec<ChunkInfo>,
        _options: StreamOptions,
    ) -> Result<DataStream, DatasetError> {
        let (tx, rx) = tokio::sync::mpsc::channel(10);
        let storage = self.storage.clone();

        tokio::spawn(async move {
            for chunk in chunks {
                match storage.get(&chunk.storage_uri).await {
                    Ok(data) => {
                        // Parse and send records
                        // Simplified - would actually parse the data
                        let _ = tx.send(Ok(Vec::new())).await;
                    }
                    Err(e) => {
                        let _ = tx.send(Err(e)).await;
                        break;
                    }
                }
            }
        });

        Ok(rx)
    }
}

/// Dataset lineage graph
pub struct DatasetLineageGraph {
    nodes: HashMap<String, LineageNode>,
    edges: Vec<LineageEdge>,
}

impl DatasetLineageGraph {
    pub fn new() -> Self {
        Self {
            nodes: HashMap::new(),
            edges: Vec::new(),
        }
    }

    pub fn add_dataset_node(&mut self, dataset_id: DatasetId) {
        let node = LineageNode {
            id: format!("dataset:{}", dataset_id.0),
            node_type: LineageNodeType::Dataset,
            name: String::new(),
            version: None,
            metadata: HashMap::new(),
            created_at: Utc::now(),
        };
        self.nodes.insert(node.id.clone(), node);
    }

    pub fn add_version_node(
        &mut self,
        version_id: DatasetVersionId,
        dataset_id: DatasetId,
        parent_version_id: Option<DatasetVersionId>,
    ) {
        let node = LineageNode {
            id: format!("version:{}", version_id.0),
            node_type: LineageNodeType::DatasetVersion,
            name: String::new(),
            version: None,
            metadata: HashMap::new(),
            created_at: Utc::now(),
        };
        self.nodes.insert(node.id.clone(), node);

        // Edge from dataset to version
        self.edges.push(LineageEdge {
            source_id: format!("dataset:{}", dataset_id.0),
            target_id: format!("version:{}", version_id.0),
            edge_type: LineageEdgeType::Contains,
            metadata: HashMap::new(),
            created_at: Utc::now(),
        });

        // Edge from parent version if exists
        if let Some(parent_id) = parent_version_id {
            self.edges.push(LineageEdge {
                source_id: format!("version:{}", parent_id.0),
                target_id: format!("version:{}", version_id.0),
                edge_type: LineageEdgeType::DerivedFrom,
                metadata: HashMap::new(),
                created_at: Utc::now(),
            });
        }
    }

    pub fn get_dataset_lineage(&self, dataset_id: DatasetId) -> Result<DatasetLineage, DatasetError> {
        let prefix = format!("dataset:{}", dataset_id.0);

        let nodes: Vec<_> = self
            .nodes
            .values()
            .filter(|n| n.id.starts_with(&prefix) || self.is_connected_to(&n.id, &prefix))
            .cloned()
            .collect();

        let node_ids: std::collections::HashSet<_> = nodes.iter().map(|n| &n.id).collect();

        let edges: Vec<_> = self
            .edges
            .iter()
            .filter(|e| node_ids.contains(&e.source_id) || node_ids.contains(&e.target_id))
            .cloned()
            .collect();

        Ok(DatasetLineage {
            dataset_id,
            versions: nodes,
            edges,
        })
    }

    fn is_connected_to(&self, node_id: &str, target_prefix: &str) -> bool {
        // BFS to find connection
        let mut visited = std::collections::HashSet::new();
        let mut queue = std::collections::VecDeque::new();
        queue.push_back(node_id.to_string());

        while let Some(current) = queue.pop_front() {
            if current.starts_with(target_prefix) {
                return true;
            }

            if visited.contains(&current) {
                continue;
            }
            visited.insert(current.clone());

            for edge in &self.edges {
                if edge.source_id == current && !visited.contains(&edge.target_id) {
                    queue.push_back(edge.target_id.clone());
                }
                if edge.target_id == current && !visited.contains(&edge.source_id) {
                    queue.push_back(edge.source_id.clone());
                }
            }
        }

        false
    }
}
```

---

## Document Metadata

| Field | Value |
|-------|-------|
| **Version** | 1.0.0 |
| **Status** | Draft |
| **SPARC Phase** | Pseudocode (Part 2 of 3) |
| **Created** | 2025-11-28 |
| **Ecosystem** | LLM DevOps |
| **Previous Part** | Pseudocode Part 1: Core Data Models & Experiment Tracking |
| **Next Part** | Pseudocode Part 3: Integration APIs & Workflow Orchestration |

---

*This pseudocode document is part of the SPARC methodology. Part 3 covers Integration APIs, Reproducibility Engine, Workflow Orchestration, and Error Handling.*
---

## 5. Integration APIs

### 5.1 LLM-Test-Bench Client

```rust
//! Integration client for LLM-Test-Bench
//! Provides standardized benchmarking infrastructure integration

use async_trait::async_trait;
use std::sync::Arc;

/// Test-Bench integration client
pub struct TestBenchClient {
    inner: Arc<ResilientClient<TestBenchInner>>,
    config: TestBenchConfig,
}

#[derive(Debug, Clone)]
pub struct TestBenchConfig {
    pub base_url: String,
    pub api_key: String,
    pub timeout: std::time::Duration,
    pub retry_policy: RetryPolicy,
    pub circuit_breaker: CircuitBreakerConfig,
}

#[derive(Debug, Clone)]
pub struct CircuitBreakerConfig {
    pub failure_threshold: u32,
    pub success_threshold: u32,
    pub timeout: std::time::Duration,
    pub half_open_max_calls: u32,
}

impl Default for CircuitBreakerConfig {
    fn default() -> Self {
        Self {
            failure_threshold: 5,
            success_threshold: 3,
            timeout: std::time::Duration::from_secs(30),
            half_open_max_calls: 3,
        }
    }
}

/// Resilient client wrapper with circuit breaker and retry
pub struct ResilientClient<T> {
    inner: Arc<T>,
    circuit_breaker: Arc<CircuitBreaker>,
    retry_policy: RetryPolicy,
}

/// Circuit breaker implementation
pub struct CircuitBreaker {
    state: tokio::sync::RwLock<CircuitState>,
    config: CircuitBreakerConfig,
    failure_count: std::sync::atomic::AtomicU32,
    success_count: std::sync::atomic::AtomicU32,
    last_failure: tokio::sync::RwLock<Option<std::time::Instant>>,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum CircuitState {
    Closed,
    Open,
    HalfOpen,
}

impl CircuitBreaker {
    pub fn new(config: CircuitBreakerConfig) -> Self {
        Self {
            state: tokio::sync::RwLock::new(CircuitState::Closed),
            config,
            failure_count: std::sync::atomic::AtomicU32::new(0),
            success_count: std::sync::atomic::AtomicU32::new(0),
            last_failure: tokio::sync::RwLock::new(None),
        }
    }

    pub async fn can_execute(&self) -> bool {
        let state = *self.state.read().await;
        match state {
            CircuitState::Closed => true,
            CircuitState::Open => {
                // Check if timeout has elapsed
                let last_failure = self.last_failure.read().await;
                if let Some(time) = *last_failure {
                    if time.elapsed() >= self.config.timeout {
                        // Transition to half-open
                        *self.state.write().await = CircuitState::HalfOpen;
                        self.success_count.store(0, std::sync::atomic::Ordering::SeqCst);
                        true
                    } else {
                        false
                    }
                } else {
                    true
                }
            }
            CircuitState::HalfOpen => {
                // Allow limited calls
                let current = self.success_count.load(std::sync::atomic::Ordering::SeqCst);
                current < self.config.half_open_max_calls
            }
        }
    }

    pub async fn record_success(&self) {
        let state = *self.state.read().await;
        match state {
            CircuitState::Closed => {
                self.failure_count.store(0, std::sync::atomic::Ordering::SeqCst);
            }
            CircuitState::HalfOpen => {
                let count = self.success_count.fetch_add(1, std::sync::atomic::Ordering::SeqCst) + 1;
                if count >= self.config.success_threshold {
                    *self.state.write().await = CircuitState::Closed;
                    self.failure_count.store(0, std::sync::atomic::Ordering::SeqCst);
                }
            }
            CircuitState::Open => {}
        }
    }

    pub async fn record_failure(&self) {
        let state = *self.state.read().await;
        match state {
            CircuitState::Closed => {
                let count = self.failure_count.fetch_add(1, std::sync::atomic::Ordering::SeqCst) + 1;
                if count >= self.config.failure_threshold {
                    *self.state.write().await = CircuitState::Open;
                    *self.last_failure.write().await = Some(std::time::Instant::now());
                }
            }
            CircuitState::HalfOpen => {
                *self.state.write().await = CircuitState::Open;
                *self.last_failure.write().await = Some(std::time::Instant::now());
            }
            CircuitState::Open => {}
        }
    }
}

impl<T> ResilientClient<T> {
    pub fn new(inner: Arc<T>, retry_policy: RetryPolicy, circuit_config: CircuitBreakerConfig) -> Self {
        Self {
            inner,
            circuit_breaker: Arc::new(CircuitBreaker::new(circuit_config)),
            retry_policy,
        }
    }

    pub async fn execute<F, Fut, R, E>(&self, operation: F) -> Result<R, IntegrationError>
    where
        F: Fn(Arc<T>) -> Fut + Clone,
        Fut: std::future::Future<Output = Result<R, E>>,
        E: std::error::Error + Send + Sync + 'static,
    {
        let mut attempts = 0;
        let mut last_error = None;

        while attempts <= self.retry_policy.max_retries {
            // Check circuit breaker
            if !self.circuit_breaker.can_execute().await {
                return Err(IntegrationError::CircuitOpen);
            }

            // Execute operation
            match operation(self.inner.clone()).await {
                Ok(result) => {
                    self.circuit_breaker.record_success().await;
                    return Ok(result);
                }
                Err(e) => {
                    self.circuit_breaker.record_failure().await;
                    last_error = Some(IntegrationError::Request(Box::new(e)));
                    attempts += 1;

                    if attempts <= self.retry_policy.max_retries {
                        let delay = self.calculate_delay(attempts);
                        tokio::time::sleep(delay).await;
                    }
                }
            }
        }

        Err(last_error.unwrap_or(IntegrationError::Unknown))
    }

    fn calculate_delay(&self, attempt: u32) -> std::time::Duration {
        let base_delay = self.retry_policy.initial_delay.as_millis() as f64;
        let delay = base_delay * self.retry_policy.exponential_base.powi(attempt as i32 - 1);
        let delay = std::cmp::min(
            delay as u64,
            self.retry_policy.max_delay.as_millis() as u64,
        );
        std::time::Duration::from_millis(delay)
    }
}

/// Test-Bench API trait
#[async_trait]
pub trait TestBenchApi: Send + Sync {
    async fn submit_benchmark(&self, request: BenchmarkSubmitRequest) -> Result<BenchmarkJob, IntegrationError>;
    async fn get_job_status(&self, job_id: &str) -> Result<JobStatus, IntegrationError>;
    async fn get_results(&self, job_id: &str) -> Result<BenchmarkJobResults, IntegrationError>;
    async fn cancel_job(&self, job_id: &str) -> Result<(), IntegrationError>;
    async fn list_benchmark_suites(&self) -> Result<Vec<BenchmarkSuiteInfo>, IntegrationError>;
    async fn get_baseline(&self, model_id: &str, suite_id: &str) -> Result<BaselineResults, IntegrationError>;
}

struct TestBenchInner {
    http_client: reqwest::Client,
    base_url: String,
    api_key: String,
}

#[async_trait]
impl TestBenchApi for TestBenchInner {
    async fn submit_benchmark(&self, request: BenchmarkSubmitRequest) -> Result<BenchmarkJob, IntegrationError> {
        let response = self.http_client
            .post(&format!("{}/api/v1/benchmarks", self.base_url))
            .header("Authorization", format!("Bearer {}", self.api_key))
            .json(&request)
            .send()
            .await?;

        if response.status().is_success() {
            Ok(response.json().await?)
        } else {
            Err(IntegrationError::ApiError {
                status: response.status().as_u16(),
                message: response.text().await.unwrap_or_default(),
            })
        }
    }

    async fn get_job_status(&self, job_id: &str) -> Result<JobStatus, IntegrationError> {
        let response = self.http_client
            .get(&format!("{}/api/v1/benchmarks/{}/status", self.base_url, job_id))
            .header("Authorization", format!("Bearer {}", self.api_key))
            .send()
            .await?;

        if response.status().is_success() {
            Ok(response.json().await?)
        } else {
            Err(IntegrationError::ApiError {
                status: response.status().as_u16(),
                message: response.text().await.unwrap_or_default(),
            })
        }
    }

    async fn get_results(&self, job_id: &str) -> Result<BenchmarkJobResults, IntegrationError> {
        let response = self.http_client
            .get(&format!("{}/api/v1/benchmarks/{}/results", self.base_url, job_id))
            .header("Authorization", format!("Bearer {}", self.api_key))
            .send()
            .await?;

        if response.status().is_success() {
            Ok(response.json().await?)
        } else {
            Err(IntegrationError::ApiError {
                status: response.status().as_u16(),
                message: response.text().await.unwrap_or_default(),
            })
        }
    }

    async fn cancel_job(&self, job_id: &str) -> Result<(), IntegrationError> {
        let response = self.http_client
            .delete(&format!("{}/api/v1/benchmarks/{}", self.base_url, job_id))
            .header("Authorization", format!("Bearer {}", self.api_key))
            .send()
            .await?;

        if response.status().is_success() {
            Ok(())
        } else {
            Err(IntegrationError::ApiError {
                status: response.status().as_u16(),
                message: response.text().await.unwrap_or_default(),
            })
        }
    }

    async fn list_benchmark_suites(&self) -> Result<Vec<BenchmarkSuiteInfo>, IntegrationError> {
        let response = self.http_client
            .get(&format!("{}/api/v1/suites", self.base_url))
            .header("Authorization", format!("Bearer {}", self.api_key))
            .send()
            .await?;

        if response.status().is_success() {
            Ok(response.json().await?)
        } else {
            Err(IntegrationError::ApiError {
                status: response.status().as_u16(),
                message: response.text().await.unwrap_or_default(),
            })
        }
    }

    async fn get_baseline(&self, model_id: &str, suite_id: &str) -> Result<BaselineResults, IntegrationError> {
        let response = self.http_client
            .get(&format!(
                "{}/api/v1/baselines/{}/{}",
                self.base_url, model_id, suite_id
            ))
            .header("Authorization", format!("Bearer {}", self.api_key))
            .send()
            .await?;

        if response.status().is_success() {
            Ok(response.json().await?)
        } else {
            Err(IntegrationError::ApiError {
                status: response.status().as_u16(),
                message: response.text().await.unwrap_or_default(),
            })
        }
    }
}

impl TestBenchClient {
    pub fn new(config: TestBenchConfig) -> Self {
        let inner = Arc::new(TestBenchInner {
            http_client: reqwest::Client::builder()
                .timeout(config.timeout)
                .build()
                .expect("Failed to build HTTP client"),
            base_url: config.base_url.clone(),
            api_key: config.api_key.clone(),
        });

        let resilient = Arc::new(ResilientClient::new(
            inner,
            config.retry_policy.clone(),
            config.circuit_breaker.clone(),
        ));

        Self {
            inner: resilient,
            config,
        }
    }

    pub async fn submit_benchmark(&self, request: BenchmarkSubmitRequest) -> Result<BenchmarkJob, IntegrationError> {
        self.inner.execute(|client| async move {
            client.submit_benchmark(request.clone()).await
        }).await
    }

    pub async fn wait_for_completion(
        &self,
        job_id: &str,
        poll_interval: std::time::Duration,
        timeout: Option<std::time::Duration>,
    ) -> Result<BenchmarkJobResults, IntegrationError> {
        let start = std::time::Instant::now();

        loop {
            let status = self.inner.execute(|client| {
                let job_id = job_id.to_string();
                async move { client.get_job_status(&job_id).await }
            }).await?;

            match status.state {
                JobState::Completed => {
                    return self.inner.execute(|client| {
                        let job_id = job_id.to_string();
                        async move { client.get_results(&job_id).await }
                    }).await;
                }
                JobState::Failed => {
                    return Err(IntegrationError::JobFailed {
                        job_id: job_id.to_string(),
                        error: status.error.unwrap_or_default(),
                    });
                }
                JobState::Cancelled => {
                    return Err(IntegrationError::JobCancelled {
                        job_id: job_id.to_string(),
                    });
                }
                _ => {
                    if let Some(timeout) = timeout {
                        if start.elapsed() >= timeout {
                            return Err(IntegrationError::Timeout);
                        }
                    }
                    tokio::time::sleep(poll_interval).await;
                }
            }
        }
    }
}

/// Request/Response types for Test-Bench
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BenchmarkSubmitRequest {
    pub model_id: String,
    pub model_endpoint: Option<String>,
    pub suite_id: String,
    pub parameters: HashMap<String, serde_json::Value>,
    pub compare_to_baseline: bool,
    pub callback_url: Option<String>,
    pub metadata: HashMap<String, String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BenchmarkJob {
    pub job_id: String,
    pub status: JobStatus,
    pub submitted_at: DateTime<Utc>,
    pub estimated_completion: Option<DateTime<Utc>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct JobStatus {
    pub state: JobState,
    pub progress: f64,
    pub current_test: Option<String>,
    pub tests_completed: u32,
    pub tests_total: u32,
    pub error: Option<String>,
    pub updated_at: DateTime<Utc>,
}

#[derive(Debug, Clone, Copy, Serialize, Deserialize, PartialEq, Eq)]
#[serde(rename_all = "snake_case")]
pub enum JobState {
    Pending,
    Running,
    Completed,
    Failed,
    Cancelled,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BenchmarkJobResults {
    pub job_id: String,
    pub model_id: String,
    pub suite_id: String,
    pub results: Vec<TestResult>,
    pub aggregated: AggregatedBenchmarkResults,
    pub baseline_comparison: Option<BaselineComparison>,
    pub completed_at: DateTime<Utc>,
    pub duration_seconds: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TestResult {
    pub test_id: String,
    pub test_name: String,
    pub passed: bool,
    pub score: f64,
    pub latency_ms: u64,
    pub tokens_used: u32,
    pub details: HashMap<String, serde_json::Value>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AggregatedBenchmarkResults {
    pub overall_score: f64,
    pub pass_rate: f64,
    pub average_latency_ms: f64,
    pub total_tokens: u64,
    pub category_scores: HashMap<String, f64>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BaselineComparison {
    pub baseline_model: String,
    pub score_diff: f64,
    pub latency_diff_percent: f64,
    pub improvements: Vec<String>,
    pub regressions: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BenchmarkSuiteInfo {
    pub id: String,
    pub name: String,
    pub description: String,
    pub test_count: u32,
    pub categories: Vec<String>,
    pub estimated_duration_minutes: u32,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BaselineResults {
    pub model_id: String,
    pub suite_id: String,
    pub results: AggregatedBenchmarkResults,
    pub recorded_at: DateTime<Utc>,
}
```

### 5.2 LLM-Analytics-Hub Client

```rust
//! Integration client for LLM-Analytics-Hub
//! Provides visualization and analytical reporting integration

/// Analytics Hub client
pub struct AnalyticsHubClient {
    inner: Arc<ResilientClient<AnalyticsHubInner>>,
    metrics_buffer: Arc<MetricsBuffer>,
    config: AnalyticsHubConfig,
}

#[derive(Debug, Clone)]
pub struct AnalyticsHubConfig {
    pub base_url: String,
    pub api_key: String,
    pub buffer_size: usize,
    pub flush_interval: std::time::Duration,
    pub timeout: std::time::Duration,
    pub retry_policy: RetryPolicy,
}

/// Metrics buffer for batching
struct MetricsBuffer {
    buffer: tokio::sync::Mutex<Vec<MetricDataPoint>>,
    max_size: usize,
}

impl MetricsBuffer {
    fn new(max_size: usize) -> Self {
        Self {
            buffer: tokio::sync::Mutex::new(Vec::with_capacity(max_size)),
            max_size,
        }
    }

    async fn add(&self, point: MetricDataPoint) -> Option<Vec<MetricDataPoint>> {
        let mut buffer = self.buffer.lock().await;
        buffer.push(point);

        if buffer.len() >= self.max_size {
            let batch = std::mem::replace(&mut *buffer, Vec::with_capacity(self.max_size));
            Some(batch)
        } else {
            None
        }
    }

    async fn flush(&self) -> Vec<MetricDataPoint> {
        let mut buffer = self.buffer.lock().await;
        std::mem::replace(&mut *buffer, Vec::with_capacity(self.max_size))
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MetricDataPoint {
    pub metric_name: String,
    pub value: f64,
    pub timestamp: DateTime<Utc>,
    pub tags: HashMap<String, String>,
    pub dimensions: HashMap<String, String>,
}

/// Analytics Hub API trait
#[async_trait]
pub trait AnalyticsHubApi: Send + Sync {
    async fn push_metrics(&self, metrics: Vec<MetricDataPoint>) -> Result<(), IntegrationError>;
    async fn query(&self, query: AnalyticsQuery) -> Result<QueryResults, IntegrationError>;
    async fn create_dashboard(&self, config: DashboardConfig) -> Result<Dashboard, IntegrationError>;
    async fn get_dashboard(&self, id: &str) -> Result<Dashboard, IntegrationError>;
    async fn create_alert(&self, config: AlertConfig) -> Result<Alert, IntegrationError>;
    async fn get_anomalies(&self, query: AnomalyQuery) -> Result<Vec<Anomaly>, IntegrationError>;
}

struct AnalyticsHubInner {
    http_client: reqwest::Client,
    base_url: String,
    api_key: String,
}

#[async_trait]
impl AnalyticsHubApi for AnalyticsHubInner {
    async fn push_metrics(&self, metrics: Vec<MetricDataPoint>) -> Result<(), IntegrationError> {
        let response = self.http_client
            .post(&format!("{}/api/v1/metrics", self.base_url))
            .header("Authorization", format!("Bearer {}", self.api_key))
            .json(&metrics)
            .send()
            .await?;

        if response.status().is_success() {
            Ok(())
        } else {
            Err(IntegrationError::ApiError {
                status: response.status().as_u16(),
                message: response.text().await.unwrap_or_default(),
            })
        }
    }

    async fn query(&self, query: AnalyticsQuery) -> Result<QueryResults, IntegrationError> {
        let response = self.http_client
            .post(&format!("{}/api/v1/query", self.base_url))
            .header("Authorization", format!("Bearer {}", self.api_key))
            .json(&query)
            .send()
            .await?;

        if response.status().is_success() {
            Ok(response.json().await?)
        } else {
            Err(IntegrationError::ApiError {
                status: response.status().as_u16(),
                message: response.text().await.unwrap_or_default(),
            })
        }
    }

    async fn create_dashboard(&self, config: DashboardConfig) -> Result<Dashboard, IntegrationError> {
        let response = self.http_client
            .post(&format!("{}/api/v1/dashboards", self.base_url))
            .header("Authorization", format!("Bearer {}", self.api_key))
            .json(&config)
            .send()
            .await?;

        if response.status().is_success() {
            Ok(response.json().await?)
        } else {
            Err(IntegrationError::ApiError {
                status: response.status().as_u16(),
                message: response.text().await.unwrap_or_default(),
            })
        }
    }

    async fn get_dashboard(&self, id: &str) -> Result<Dashboard, IntegrationError> {
        let response = self.http_client
            .get(&format!("{}/api/v1/dashboards/{}", self.base_url, id))
            .header("Authorization", format!("Bearer {}", self.api_key))
            .send()
            .await?;

        if response.status().is_success() {
            Ok(response.json().await?)
        } else {
            Err(IntegrationError::ApiError {
                status: response.status().as_u16(),
                message: response.text().await.unwrap_or_default(),
            })
        }
    }

    async fn create_alert(&self, config: AlertConfig) -> Result<Alert, IntegrationError> {
        let response = self.http_client
            .post(&format!("{}/api/v1/alerts", self.base_url))
            .header("Authorization", format!("Bearer {}", self.api_key))
            .json(&config)
            .send()
            .await?;

        if response.status().is_success() {
            Ok(response.json().await?)
        } else {
            Err(IntegrationError::ApiError {
                status: response.status().as_u16(),
                message: response.text().await.unwrap_or_default(),
            })
        }
    }

    async fn get_anomalies(&self, query: AnomalyQuery) -> Result<Vec<Anomaly>, IntegrationError> {
        let response = self.http_client
            .post(&format!("{}/api/v1/anomalies", self.base_url))
            .header("Authorization", format!("Bearer {}", self.api_key))
            .json(&query)
            .send()
            .await?;

        if response.status().is_success() {
            Ok(response.json().await?)
        } else {
            Err(IntegrationError::ApiError {
                status: response.status().as_u16(),
                message: response.text().await.unwrap_or_default(),
            })
        }
    }
}

impl AnalyticsHubClient {
    pub fn new(config: AnalyticsHubConfig) -> Self {
        let inner = Arc::new(AnalyticsHubInner {
            http_client: reqwest::Client::builder()
                .timeout(config.timeout)
                .build()
                .expect("Failed to build HTTP client"),
            base_url: config.base_url.clone(),
            api_key: config.api_key.clone(),
        });

        let resilient = Arc::new(ResilientClient::new(
            inner,
            config.retry_policy.clone(),
            CircuitBreakerConfig::default(),
        ));

        let client = Self {
            inner: resilient,
            metrics_buffer: Arc::new(MetricsBuffer::new(config.buffer_size)),
            config: config.clone(),
        };

        // Start background flush task
        let buffer = client.metrics_buffer.clone();
        let resilient_clone = client.inner.clone();
        let interval = config.flush_interval;

        tokio::spawn(async move {
            let mut ticker = tokio::time::interval(interval);
            loop {
                ticker.tick().await;
                let batch = buffer.flush().await;
                if !batch.is_empty() {
                    if let Err(e) = resilient_clone.execute(|client| {
                        let batch = batch.clone();
                        async move { client.push_metrics(batch).await }
                    }).await {
                        tracing::error!("Failed to flush metrics: {}", e);
                    }
                }
            }
        });

        client
    }

    /// Push a single metric (buffered)
    pub async fn push_metric(&self, point: MetricDataPoint) -> Result<(), IntegrationError> {
        if let Some(batch) = self.metrics_buffer.add(point).await {
            self.inner.execute(|client| {
                let batch = batch.clone();
                async move { client.push_metrics(batch).await }
            }).await?;
        }
        Ok(())
    }

    /// Push metrics immediately (unbuffered)
    pub async fn push_metrics_immediate(&self, metrics: Vec<MetricDataPoint>) -> Result<(), IntegrationError> {
        self.inner.execute(|client| {
            let metrics = metrics.clone();
            async move { client.push_metrics(metrics).await }
        }).await
    }

    /// Query historical metrics
    pub async fn query(&self, query: AnalyticsQuery) -> Result<QueryResults, IntegrationError> {
        self.inner.execute(|client| {
            let query = query.clone();
            async move { client.query(query).await }
        }).await
    }

    /// Create experiment dashboard
    pub async fn create_experiment_dashboard(
        &self,
        experiment_id: ExperimentId,
        metrics: Vec<String>,
    ) -> Result<Dashboard, IntegrationError> {
        let config = DashboardConfig {
            name: format!("Experiment {} Dashboard", experiment_id.0),
            description: Some("Auto-generated experiment dashboard".to_string()),
            panels: metrics
                .iter()
                .enumerate()
                .map(|(i, metric)| PanelConfig {
                    id: format!("panel_{}", i),
                    title: metric.clone(),
                    panel_type: PanelType::LineChart,
                    query: AnalyticsQuery {
                        metric_names: vec![metric.clone()],
                        time_range: TimeRange::Relative {
                            duration: std::time::Duration::from_secs(3600),
                        },
                        filters: vec![Filter {
                            field: "experiment_id".to_string(),
                            operator: FilterOperator::Equals,
                            value: experiment_id.0.to_string(),
                        }],
                        aggregation: Some(AggregationType::Mean),
                        group_by: vec!["run_id".to_string()],
                        order_by: None,
                        limit: None,
                    },
                    position: PanelPosition {
                        x: (i % 2) as u32 * 6,
                        y: (i / 2) as u32 * 4,
                        width: 6,
                        height: 4,
                    },
                })
                .collect(),
            refresh_interval: Some(std::time::Duration::from_secs(30)),
            tags: vec!["experiment".to_string(), experiment_id.0.to_string()],
        };

        self.inner.execute(|client| {
            let config = config.clone();
            async move { client.create_dashboard(config).await }
        }).await
    }
}

/// Analytics query types
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AnalyticsQuery {
    pub metric_names: Vec<String>,
    pub time_range: TimeRange,
    pub filters: Vec<Filter>,
    pub aggregation: Option<AggregationType>,
    pub group_by: Vec<String>,
    pub order_by: Option<OrderBy>,
    pub limit: Option<u32>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum TimeRange {
    Absolute {
        start: DateTime<Utc>,
        end: DateTime<Utc>,
    },
    Relative {
        duration: std::time::Duration,
    },
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Filter {
    pub field: String,
    pub operator: FilterOperator,
    pub value: String,
}

#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum FilterOperator {
    Equals,
    NotEquals,
    Contains,
    GreaterThan,
    LessThan,
    In,
    NotIn,
}

#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum AggregationType {
    Sum,
    Mean,
    Median,
    Min,
    Max,
    Count,
    Percentile(u8),
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OrderBy {
    pub field: String,
    pub direction: SortDirection,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct QueryResults {
    pub series: Vec<TimeSeries>,
    pub metadata: QueryMetadata,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TimeSeries {
    pub metric_name: String,
    pub tags: HashMap<String, String>,
    pub points: Vec<DataPoint>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DataPoint {
    pub timestamp: DateTime<Utc>,
    pub value: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct QueryMetadata {
    pub execution_time_ms: u64,
    pub rows_scanned: u64,
    pub result_count: u64,
}

/// Dashboard types
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DashboardConfig {
    pub name: String,
    pub description: Option<String>,
    pub panels: Vec<PanelConfig>,
    pub refresh_interval: Option<std::time::Duration>,
    pub tags: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PanelConfig {
    pub id: String,
    pub title: String,
    pub panel_type: PanelType,
    pub query: AnalyticsQuery,
    pub position: PanelPosition,
}

#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum PanelType {
    LineChart,
    BarChart,
    Gauge,
    Table,
    Heatmap,
    Scatter,
    Pie,
}

#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
pub struct PanelPosition {
    pub x: u32,
    pub y: u32,
    pub width: u32,
    pub height: u32,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Dashboard {
    pub id: String,
    pub name: String,
    pub url: String,
    pub panels: Vec<PanelConfig>,
    pub created_at: DateTime<Utc>,
}

/// Alert types
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AlertConfig {
    pub name: String,
    pub description: Option<String>,
    pub condition: AlertCondition,
    pub channels: Vec<NotificationChannel>,
    pub cooldown: std::time::Duration,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AlertCondition {
    pub metric_name: String,
    pub operator: ComparisonOperator,
    pub threshold: f64,
    pub duration: std::time::Duration,
    pub filters: Vec<Filter>,
}

#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ComparisonOperator {
    GreaterThan,
    LessThan,
    GreaterThanOrEqual,
    LessThanOrEqual,
    Equal,
    NotEqual,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum NotificationChannel {
    Email { addresses: Vec<String> },
    Slack { webhook_url: String },
    PagerDuty { routing_key: String },
    Webhook { url: String },
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Alert {
    pub id: String,
    pub name: String,
    pub status: AlertStatus,
    pub created_at: DateTime<Utc>,
}

#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum AlertStatus {
    Active,
    Silenced,
    Disabled,
}

/// Anomaly detection
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AnomalyQuery {
    pub metric_name: String,
    pub time_range: TimeRange,
    pub sensitivity: f64,
    pub filters: Vec<Filter>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Anomaly {
    pub timestamp: DateTime<Utc>,
    pub metric_name: String,
    pub expected_value: f64,
    pub actual_value: f64,
    pub deviation: f64,
    pub severity: AnomalySeverity,
}

#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum AnomalySeverity {
    Low,
    Medium,
    High,
    Critical,
}
```

### 5.3 LLM-Registry Client

```rust
//! Integration client for LLM-Registry
//! Provides artifact and model versioning integration

/// Registry client
pub struct RegistryClient {
    inner: Arc<ResilientClient<RegistryInner>>,
    config: RegistryConfig,
}

#[derive(Debug, Clone)]
pub struct RegistryConfig {
    pub base_url: String,
    pub api_key: String,
    pub timeout: std::time::Duration,
    pub retry_policy: RetryPolicy,
    pub chunk_size: usize,
}

#[async_trait]
pub trait RegistryApi: Send + Sync {
    async fn register_model(&self, request: RegisterModelRequest) -> Result<ModelEntry, IntegrationError>;
    async fn get_model(&self, model_id: &str, version: Option<&str>) -> Result<ModelEntry, IntegrationError>;
    async fn list_models(&self, filter: ModelFilter) -> Result<Vec<ModelEntry>, IntegrationError>;
    async fn upload_artifact(&self, artifact: ArtifactUpload) -> Result<ArtifactEntry, IntegrationError>;
    async fn download_artifact(&self, artifact_id: &str) -> Result<Vec<u8>, IntegrationError>;
    async fn register_metric_definition(&self, metric: MetricDefinition) -> Result<(), IntegrationError>;
    async fn get_metric_definition(&self, name: &str, version: &str) -> Result<MetricDefinition, IntegrationError>;
}

struct RegistryInner {
    http_client: reqwest::Client,
    base_url: String,
    api_key: String,
    chunk_size: usize,
}

impl RegistryClient {
    pub fn new(config: RegistryConfig) -> Self {
        let inner = Arc::new(RegistryInner {
            http_client: reqwest::Client::builder()
                .timeout(config.timeout)
                .build()
                .expect("Failed to build HTTP client"),
            base_url: config.base_url.clone(),
            api_key: config.api_key.clone(),
            chunk_size: config.chunk_size,
        });

        let resilient = Arc::new(ResilientClient::new(
            inner,
            config.retry_policy.clone(),
            CircuitBreakerConfig::default(),
        ));

        Self {
            inner: resilient,
            config,
        }
    }

    pub async fn register_model(&self, request: RegisterModelRequest) -> Result<ModelEntry, IntegrationError> {
        self.inner.execute(|client| {
            let request = request.clone();
            async move { client.register_model(request).await }
        }).await
    }

    pub async fn get_model(&self, model_id: &str, version: Option<&str>) -> Result<ModelEntry, IntegrationError> {
        self.inner.execute(|client| {
            let model_id = model_id.to_string();
            let version = version.map(|s| s.to_string());
            async move { client.get_model(&model_id, version.as_deref()).await }
        }).await
    }

    pub async fn upload_experiment_artifact(
        &self,
        experiment_id: ExperimentId,
        run_id: RunId,
        artifact: ArtifactUpload,
    ) -> Result<ArtifactEntry, IntegrationError> {
        let mut upload = artifact;
        upload.metadata.insert("experiment_id".to_string(), experiment_id.0.to_string());
        upload.metadata.insert("run_id".to_string(), run_id.0.to_string());

        self.inner.execute(|client| {
            let upload = upload.clone();
            async move { client.upload_artifact(upload).await }
        }).await
    }
}

#[async_trait]
impl RegistryApi for RegistryInner {
    async fn register_model(&self, request: RegisterModelRequest) -> Result<ModelEntry, IntegrationError> {
        let response = self.http_client
            .post(&format!("{}/api/v1/models", self.base_url))
            .header("Authorization", format!("Bearer {}", self.api_key))
            .json(&request)
            .send()
            .await?;

        if response.status().is_success() {
            Ok(response.json().await?)
        } else {
            Err(IntegrationError::ApiError {
                status: response.status().as_u16(),
                message: response.text().await.unwrap_or_default(),
            })
        }
    }

    async fn get_model(&self, model_id: &str, version: Option<&str>) -> Result<ModelEntry, IntegrationError> {
        let url = match version {
            Some(v) => format!("{}/api/v1/models/{}/versions/{}", self.base_url, model_id, v),
            None => format!("{}/api/v1/models/{}/latest", self.base_url, model_id),
        };

        let response = self.http_client
            .get(&url)
            .header("Authorization", format!("Bearer {}", self.api_key))
            .send()
            .await?;

        if response.status().is_success() {
            Ok(response.json().await?)
        } else {
            Err(IntegrationError::ApiError {
                status: response.status().as_u16(),
                message: response.text().await.unwrap_or_default(),
            })
        }
    }

    async fn list_models(&self, filter: ModelFilter) -> Result<Vec<ModelEntry>, IntegrationError> {
        let response = self.http_client
            .get(&format!("{}/api/v1/models", self.base_url))
            .header("Authorization", format!("Bearer {}", self.api_key))
            .query(&filter)
            .send()
            .await?;

        if response.status().is_success() {
            Ok(response.json().await?)
        } else {
            Err(IntegrationError::ApiError {
                status: response.status().as_u16(),
                message: response.text().await.unwrap_or_default(),
            })
        }
    }

    async fn upload_artifact(&self, artifact: ArtifactUpload) -> Result<ArtifactEntry, IntegrationError> {
        // Multipart upload for large artifacts
        let form = reqwest::multipart::Form::new()
            .text("name", artifact.name.clone())
            .text("artifact_type", format!("{:?}", artifact.artifact_type))
            .text("metadata", serde_json::to_string(&artifact.metadata).unwrap_or_default())
            .part("file", reqwest::multipart::Part::bytes(artifact.data.clone())
                .file_name(artifact.name.clone()));

        let response = self.http_client
            .post(&format!("{}/api/v1/artifacts", self.base_url))
            .header("Authorization", format!("Bearer {}", self.api_key))
            .multipart(form)
            .send()
            .await?;

        if response.status().is_success() {
            Ok(response.json().await?)
        } else {
            Err(IntegrationError::ApiError {
                status: response.status().as_u16(),
                message: response.text().await.unwrap_or_default(),
            })
        }
    }

    async fn download_artifact(&self, artifact_id: &str) -> Result<Vec<u8>, IntegrationError> {
        let response = self.http_client
            .get(&format!("{}/api/v1/artifacts/{}/download", self.base_url, artifact_id))
            .header("Authorization", format!("Bearer {}", self.api_key))
            .send()
            .await?;

        if response.status().is_success() {
            Ok(response.bytes().await?.to_vec())
        } else {
            Err(IntegrationError::ApiError {
                status: response.status().as_u16(),
                message: response.text().await.unwrap_or_default(),
            })
        }
    }

    async fn register_metric_definition(&self, metric: MetricDefinition) -> Result<(), IntegrationError> {
        let response = self.http_client
            .post(&format!("{}/api/v1/metrics", self.base_url))
            .header("Authorization", format!("Bearer {}", self.api_key))
            .json(&metric)
            .send()
            .await?;

        if response.status().is_success() {
            Ok(())
        } else {
            Err(IntegrationError::ApiError {
                status: response.status().as_u16(),
                message: response.text().await.unwrap_or_default(),
            })
        }
    }

    async fn get_metric_definition(&self, name: &str, version: &str) -> Result<MetricDefinition, IntegrationError> {
        let response = self.http_client
            .get(&format!("{}/api/v1/metrics/{}/versions/{}", self.base_url, name, version))
            .header("Authorization", format!("Bearer {}", self.api_key))
            .send()
            .await?;

        if response.status().is_success() {
            Ok(response.json().await?)
        } else {
            Err(IntegrationError::ApiError {
                status: response.status().as_u16(),
                message: response.text().await.unwrap_or_default(),
            })
        }
    }
}

/// Registry types
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RegisterModelRequest {
    pub name: String,
    pub version: String,
    pub description: Option<String>,
    pub model_type: ModelType,
    pub artifact_uri: Option<String>,
    pub metrics: HashMap<String, f64>,
    pub parameters: HashMap<String, serde_json::Value>,
    pub tags: Vec<String>,
    pub metadata: HashMap<String, String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelEntry {
    pub id: String,
    pub name: String,
    pub version: String,
    pub description: Option<String>,
    pub model_type: ModelType,
    pub artifact_uri: Option<String>,
    pub metrics: HashMap<String, f64>,
    pub parameters: HashMap<String, serde_json::Value>,
    pub tags: Vec<String>,
    pub metadata: HashMap<String, String>,
    pub created_at: DateTime<Utc>,
    pub created_by: String,
    pub stage: ModelStage,
}

#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ModelType {
    LLM,
    Embedding,
    Classification,
    Regression,
    Custom,
}

#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ModelStage {
    Development,
    Staging,
    Production,
    Archived,
}

#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct ModelFilter {
    pub name: Option<String>,
    pub model_type: Option<ModelType>,
    pub stage: Option<ModelStage>,
    pub tags: Option<Vec<String>>,
}

#[derive(Debug, Clone)]
pub struct ArtifactUpload {
    pub name: String,
    pub artifact_type: ArtifactType,
    pub data: Vec<u8>,
    pub metadata: HashMap<String, String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ArtifactEntry {
    pub id: String,
    pub name: String,
    pub artifact_type: ArtifactType,
    pub size_bytes: u64,
    pub content_hash: String,
    pub storage_uri: String,
    pub metadata: HashMap<String, String>,
    pub created_at: DateTime<Utc>,
}
```

### 5.4 Integration Error Types

```rust
/// Integration error types
#[derive(Debug, thiserror::Error)]
pub enum IntegrationError {
    #[error("HTTP request error: {0}")]
    Request(#[from] Box<dyn std::error::Error + Send + Sync>),

    #[error("API error (status {status}): {message}")]
    ApiError { status: u16, message: String },

    #[error("Circuit breaker open")]
    CircuitOpen,

    #[error("Request timeout")]
    Timeout,

    #[error("Job failed: {job_id} - {error}")]
    JobFailed { job_id: String, error: String },

    #[error("Job cancelled: {job_id}")]
    JobCancelled { job_id: String },

    #[error("Serialization error: {0}")]
    Serialization(#[from] serde_json::Error),

    #[error("Unknown error")]
    Unknown,
}

impl From<reqwest::Error> for IntegrationError {
    fn from(err: reqwest::Error) -> Self {
        IntegrationError::Request(Box::new(err))
    }
}
```

---

## 6. Reproducibility Engine

### 6.1 Reproducibility Core

```rust
//! Reproducibility engine for experiment state capture and replay
//! Ensures experiments can be reliably reproduced

/// Reproducibility engine
pub struct ReproducibilityEngine {
    state_store: Arc<dyn StateStore>,
    environment_capture: Arc<EnvironmentCapture>,
    code_snapshot: Arc<CodeSnapshot>,
    config: ReproducibilityConfig,
}

#[derive(Debug, Clone)]
pub struct ReproducibilityConfig {
    pub capture_code: bool,
    pub capture_data_hashes: bool,
    pub capture_system_state: bool,
    pub strict_mode: bool,
    pub hash_algorithm: HashAlgorithm,
}

#[derive(Debug, Clone, Copy)]
pub enum HashAlgorithm {
    Sha256,
    Blake3,
}

impl Default for ReproducibilityConfig {
    fn default() -> Self {
        Self {
            capture_code: true,
            capture_data_hashes: true,
            capture_system_state: true,
            strict_mode: true,
            hash_algorithm: HashAlgorithm::Sha256,
        }
    }
}

/// Complete experiment state for reproduction
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ExperimentState {
    pub id: Uuid,
    pub experiment_id: ExperimentId,
    pub run_id: RunId,
    pub captured_at: DateTime<Utc>,
    pub environment: EnvironmentSnapshot,
    pub code_state: CodeState,
    pub data_state: DataState,
    pub configuration: ConfigurationState,
    pub random_state: RandomState,
    pub checksum: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CodeState {
    pub git_commit: Option<String>,
    pub git_branch: Option<String>,
    pub git_dirty: bool,
    pub code_hash: String,
    pub patches: Vec<CodePatch>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CodePatch {
    pub file_path: String,
    pub diff: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DataState {
    pub datasets: Vec<DatasetReference>,
    pub data_hashes: HashMap<String, String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DatasetReference {
    pub dataset_id: DatasetId,
    pub version_id: DatasetVersionId,
    pub content_hash: String,
    pub row_count: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ConfigurationState {
    pub experiment_config: serde_json::Value,
    pub run_parameters: HashMap<String, ParameterValue>,
    pub environment_variables: HashMap<String, String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RandomState {
    pub global_seed: Option<u64>,
    pub numpy_state: Option<String>,
    pub torch_state: Option<String>,
    pub random_state: Option<String>,
}

impl ReproducibilityEngine {
    pub fn new(
        state_store: Arc<dyn StateStore>,
        environment_capture: Arc<EnvironmentCapture>,
        code_snapshot: Arc<CodeSnapshot>,
        config: ReproducibilityConfig,
    ) -> Self {
        Self {
            state_store,
            environment_capture,
            code_snapshot,
            config,
        }
    }

    /// Capture complete experiment state
    pub async fn capture_state(
        &self,
        experiment_id: ExperimentId,
        run_id: RunId,
        run_parameters: HashMap<String, ParameterValue>,
        datasets: Vec<DatasetReference>,
    ) -> Result<ExperimentState, ReproducibilityError> {
        // Capture environment
        let environment = self.environment_capture.capture().await?;

        // Capture code state
        let code_state = if self.config.capture_code {
            self.code_snapshot.capture().await?
        } else {
            CodeState {
                git_commit: None,
                git_branch: None,
                git_dirty: false,
                code_hash: String::new(),
                patches: Vec::new(),
            }
        };

        // Capture data state
        let data_state = DataState {
            datasets,
            data_hashes: if self.config.capture_data_hashes {
                self.compute_data_hashes().await?
            } else {
                HashMap::new()
            },
        };

        // Capture configuration
        let configuration = ConfigurationState {
            experiment_config: serde_json::json!({}),
            run_parameters,
            environment_variables: environment.environment_variables.clone(),
        };

        // Capture random state
        let random_state = self.capture_random_state()?;

        let state = ExperimentState {
            id: Uuid::new_v4(),
            experiment_id,
            run_id,
            captured_at: Utc::now(),
            environment,
            code_state,
            data_state,
            configuration,
            random_state,
            checksum: String::new(),
        };

        // Compute checksum
        let checksum = self.compute_state_checksum(&state)?;
        let state = ExperimentState { checksum, ..state };

        // Store state
        self.state_store.save(&state).await?;

        Ok(state)
    }

    /// Validate reproducibility of a configuration
    pub async fn validate_reproducibility(
        &self,
        state: &ExperimentState,
    ) -> Result<ValidationReport, ReproducibilityError> {
        let mut issues = Vec::new();
        let mut warnings = Vec::new();

        // Check code state
        if state.code_state.git_dirty {
            if self.config.strict_mode {
                issues.push(ValidationIssue {
                    severity: IssueSeverity::Error,
                    category: IssueCategory::Code,
                    message: "Repository has uncommitted changes".to_string(),
                    suggestion: Some("Commit all changes before running experiment".to_string()),
                });
            } else {
                warnings.push(ValidationIssue {
                    severity: IssueSeverity::Warning,
                    category: IssueCategory::Code,
                    message: "Repository has uncommitted changes".to_string(),
                    suggestion: Some("Consider committing changes for full reproducibility".to_string()),
                });
            }
        }

        // Check environment
        if state.environment.dependencies.packages.is_empty() {
            warnings.push(ValidationIssue {
                severity: IssueSeverity::Warning,
                category: IssueCategory::Environment,
                message: "No dependency manifest captured".to_string(),
                suggestion: Some("Ensure Cargo.lock or requirements.txt exists".to_string()),
            });
        }

        // Check random state
        if state.random_state.global_seed.is_none() {
            warnings.push(ValidationIssue {
                severity: IssueSeverity::Warning,
                category: IssueCategory::Randomness,
                message: "No global random seed set".to_string(),
                suggestion: Some("Set a global seed for deterministic results".to_string()),
            });
        }

        // Check data integrity
        for dataset in &state.data_state.datasets {
            if dataset.content_hash.is_empty() {
                issues.push(ValidationIssue {
                    severity: IssueSeverity::Error,
                    category: IssueCategory::Data,
                    message: format!("Dataset {} has no content hash", dataset.dataset_id.0),
                    suggestion: Some("Re-register dataset with content hashing enabled".to_string()),
                });
            }
        }

        let is_reproducible = issues.is_empty();

        Ok(ValidationReport {
            state_id: state.id,
            is_reproducible,
            issues,
            warnings,
            validated_at: Utc::now(),
        })
    }

    /// Replay an experiment from captured state
    pub async fn replay_experiment(
        &self,
        state: &ExperimentState,
    ) -> Result<ReplayResult, ReproducibilityError> {
        // Validate state checksum
        let computed_checksum = self.compute_state_checksum(state)?;
        if computed_checksum != state.checksum {
            return Err(ReproducibilityError::ChecksumMismatch {
                expected: state.checksum.clone(),
                actual: computed_checksum,
            });
        }

        // Restore code state
        if self.config.capture_code {
            self.restore_code_state(&state.code_state).await?;
        }

        // Restore random state
        self.restore_random_state(&state.random_state)?;

        // Verify data availability
        for dataset in &state.data_state.datasets {
            self.verify_dataset_available(dataset).await?;
        }

        // Verify environment compatibility
        let current_env = self.environment_capture.capture().await?;
        let env_diff = self.compare_environments(&state.environment, &current_env);

        Ok(ReplayResult {
            state_id: state.id,
            environment_differences: env_diff,
            ready_to_execute: true,
            setup_commands: self.generate_setup_commands(state),
            replayed_at: Utc::now(),
        })
    }

    /// Compare two experiment states
    pub async fn compare_states(
        &self,
        state_a: &ExperimentState,
        state_b: &ExperimentState,
    ) -> Result<StateComparison, ReproducibilityError> {
        let mut differences = Vec::new();

        // Compare code
        if state_a.code_state.code_hash != state_b.code_state.code_hash {
            differences.push(StateDifference {
                category: DifferenceCategory::Code,
                field: "code_hash".to_string(),
                value_a: state_a.code_state.code_hash.clone(),
                value_b: state_b.code_state.code_hash.clone(),
                impact: DifferenceImpact::High,
            });
        }

        // Compare parameters
        for (key, value_a) in &state_a.configuration.run_parameters {
            if let Some(value_b) = state_b.configuration.run_parameters.get(key) {
                if value_a != value_b {
                    differences.push(StateDifference {
                        category: DifferenceCategory::Configuration,
                        field: key.clone(),
                        value_a: format!("{:?}", value_a),
                        value_b: format!("{:?}", value_b),
                        impact: DifferenceImpact::High,
                    });
                }
            } else {
                differences.push(StateDifference {
                    category: DifferenceCategory::Configuration,
                    field: key.clone(),
                    value_a: format!("{:?}", value_a),
                    value_b: "missing".to_string(),
                    impact: DifferenceImpact::High,
                });
            }
        }

        // Compare data
        for dataset_a in &state_a.data_state.datasets {
            let matching = state_b
                .data_state
                .datasets
                .iter()
                .find(|d| d.dataset_id == dataset_a.dataset_id);

            if let Some(dataset_b) = matching {
                if dataset_a.content_hash != dataset_b.content_hash {
                    differences.push(StateDifference {
                        category: DifferenceCategory::Data,
                        field: format!("dataset:{}", dataset_a.dataset_id.0),
                        value_a: dataset_a.content_hash.clone(),
                        value_b: dataset_b.content_hash.clone(),
                        impact: DifferenceImpact::High,
                    });
                }
            }
        }

        // Compare environment
        let env_diffs = self.compare_environments(&state_a.environment, &state_b.environment);
        for diff in env_diffs {
            differences.push(StateDifference {
                category: DifferenceCategory::Environment,
                field: diff.clone(),
                value_a: "state_a".to_string(),
                value_b: "state_b".to_string(),
                impact: DifferenceImpact::Medium,
            });
        }

        let are_equivalent = differences.is_empty();

        Ok(StateComparison {
            state_a_id: state_a.id,
            state_b_id: state_b.id,
            are_equivalent,
            differences,
            compared_at: Utc::now(),
        })
    }

    /// Generate reproducibility certificate
    pub async fn generate_certificate(
        &self,
        state: &ExperimentState,
        run_results: &ExperimentRun,
    ) -> Result<ReproducibilityCertificate, ReproducibilityError> {
        let validation = self.validate_reproducibility(state).await?;

        if !validation.is_reproducible && self.config.strict_mode {
            return Err(ReproducibilityError::ValidationFailed {
                issues: validation.issues,
            });
        }

        let certificate = ReproducibilityCertificate {
            id: Uuid::new_v4(),
            state_id: state.id,
            experiment_id: state.experiment_id,
            run_id: state.run_id,
            validation_report: validation,
            environment_hash: self.hash_environment(&state.environment),
            code_hash: state.code_state.code_hash.clone(),
            data_hash: self.hash_data_state(&state.data_state),
            configuration_hash: self.hash_configuration(&state.configuration),
            results_hash: self.hash_results(run_results),
            issued_at: Utc::now(),
            signature: self.sign_certificate(state, run_results)?,
        };

        Ok(certificate)
    }

    // Private helper methods

    fn compute_state_checksum(&self, state: &ExperimentState) -> Result<String, ReproducibilityError> {
        let mut hasher = match self.config.hash_algorithm {
            HashAlgorithm::Sha256 => {
                use sha2::{Sha256, Digest};
                let state_bytes = serde_json::to_vec(&ExperimentState {
                    checksum: String::new(),
                    ..state.clone()
                })?;
                let mut hasher = Sha256::new();
                hasher.update(&state_bytes);
                hex::encode(hasher.finalize())
            }
            HashAlgorithm::Blake3 => {
                let state_bytes = serde_json::to_vec(&ExperimentState {
                    checksum: String::new(),
                    ..state.clone()
                })?;
                let hash = blake3::hash(&state_bytes);
                hash.to_hex().to_string()
            }
        };

        Ok(hasher)
    }

    fn capture_random_state(&self) -> Result<RandomState, ReproducibilityError> {
        // This would integrate with the actual random state capture
        Ok(RandomState {
            global_seed: None,
            numpy_state: None,
            torch_state: None,
            random_state: None,
        })
    }

    fn restore_random_state(&self, _state: &RandomState) -> Result<(), ReproducibilityError> {
        // Restore random state from captured state
        Ok(())
    }

    async fn restore_code_state(&self, state: &CodeState) -> Result<(), ReproducibilityError> {
        if let Some(ref commit) = state.git_commit {
            // Git checkout
            tokio::process::Command::new("git")
                .args(["checkout", commit])
                .output()
                .await
                .map_err(|e| ReproducibilityError::CodeRestoreFailed {
                    message: e.to_string(),
                })?;

            // Apply patches if any
            for patch in &state.patches {
                tokio::process::Command::new("git")
                    .args(["apply", "-"])
                    .stdin(std::process::Stdio::piped())
                    .spawn()
                    .map_err(|e| ReproducibilityError::CodeRestoreFailed {
                        message: e.to_string(),
                    })?;
            }
        }

        Ok(())
    }

    async fn verify_dataset_available(
        &self,
        dataset: &DatasetReference,
    ) -> Result<(), ReproducibilityError> {
        // Verify dataset is accessible
        // This would check with the data vault
        Ok(())
    }

    async fn compute_data_hashes(&self) -> Result<HashMap<String, String>, ReproducibilityError> {
        // Compute hashes for all referenced data
        Ok(HashMap::new())
    }

    fn compare_environments(
        &self,
        a: &EnvironmentSnapshot,
        b: &EnvironmentSnapshot,
    ) -> Vec<String> {
        let mut differences = Vec::new();

        if a.os.name != b.os.name {
            differences.push(format!("OS: {} vs {}", a.os.name, b.os.name));
        }

        if a.hardware.cpu_model != b.hardware.cpu_model {
            differences.push(format!(
                "CPU: {} vs {}",
                a.hardware.cpu_model, b.hardware.cpu_model
            ));
        }

        // Compare dependencies
        let a_packages: HashMap<_, _> = a
            .dependencies
            .packages
            .iter()
            .map(|p| (&p.name, &p.version))
            .collect();

        for package in &b.dependencies.packages {
            if let Some(version_a) = a_packages.get(&package.name) {
                if *version_a != &package.version {
                    differences.push(format!(
                        "Package {}: {} vs {}",
                        package.name, version_a, package.version
                    ));
                }
            } else {
                differences.push(format!("Package {} added in b", package.name));
            }
        }

        differences
    }

    fn generate_setup_commands(&self, state: &ExperimentState) -> Vec<String> {
        let mut commands = Vec::new();

        // Git checkout
        if let Some(ref commit) = state.code_state.git_commit {
            commands.push(format!("git checkout {}", commit));
        }

        // Install dependencies
        match state.environment.dependencies.format {
            DependencyFormat::CargoLock => {
                commands.push("cargo build --release".to_string());
            }
            DependencyFormat::PipFreeze => {
                commands.push("pip install -r requirements.txt".to_string());
            }
            DependencyFormat::CondaEnvironment => {
                commands.push("conda env create -f environment.yml".to_string());
            }
            _ => {}
        }

        commands
    }

    fn hash_environment(&self, env: &EnvironmentSnapshot) -> String {
        use sha2::{Sha256, Digest};
        let bytes = serde_json::to_vec(env).unwrap_or_default();
        hex::encode(Sha256::digest(&bytes))
    }

    fn hash_data_state(&self, data: &DataState) -> String {
        use sha2::{Sha256, Digest};
        let bytes = serde_json::to_vec(data).unwrap_or_default();
        hex::encode(Sha256::digest(&bytes))
    }

    fn hash_configuration(&self, config: &ConfigurationState) -> String {
        use sha2::{Sha256, Digest};
        let bytes = serde_json::to_vec(config).unwrap_or_default();
        hex::encode(Sha256::digest(&bytes))
    }

    fn hash_results(&self, run: &ExperimentRun) -> String {
        use sha2::{Sha256, Digest};
        let bytes = serde_json::to_vec(&run.metrics).unwrap_or_default();
        hex::encode(Sha256::digest(&bytes))
    }

    fn sign_certificate(
        &self,
        state: &ExperimentState,
        run: &ExperimentRun,
    ) -> Result<String, ReproducibilityError> {
        // Sign with system key
        use sha2::{Sha256, Digest};
        let data = format!(
            "{}:{}:{}:{}",
            state.id, state.checksum, run.id.0, Utc::now().timestamp()
        );
        Ok(hex::encode(Sha256::digest(data.as_bytes())))
    }
}

/// Code snapshot service
pub struct CodeSnapshot {
    repo_path: std::path::PathBuf,
}

impl CodeSnapshot {
    pub fn new(repo_path: std::path::PathBuf) -> Self {
        Self { repo_path }
    }

    pub async fn capture(&self) -> Result<CodeState, ReproducibilityError> {
        // Get git commit
        let commit_output = tokio::process::Command::new("git")
            .args(["rev-parse", "HEAD"])
            .current_dir(&self.repo_path)
            .output()
            .await?;

        let git_commit = if commit_output.status.success() {
            Some(String::from_utf8_lossy(&commit_output.stdout).trim().to_string())
        } else {
            None
        };

        // Get branch
        let branch_output = tokio::process::Command::new("git")
            .args(["rev-parse", "--abbrev-ref", "HEAD"])
            .current_dir(&self.repo_path)
            .output()
            .await?;

        let git_branch = if branch_output.status.success() {
            Some(String::from_utf8_lossy(&branch_output.stdout).trim().to_string())
        } else {
            None
        };

        // Check if dirty
        let status_output = tokio::process::Command::new("git")
            .args(["status", "--porcelain"])
            .current_dir(&self.repo_path)
            .output()
            .await?;

        let git_dirty = !status_output.stdout.is_empty();

        // Get patches for uncommitted changes
        let patches = if git_dirty {
            let diff_output = tokio::process::Command::new("git")
                .args(["diff"])
                .current_dir(&self.repo_path)
                .output()
                .await?;

            vec![CodePatch {
                file_path: "working_tree".to_string(),
                diff: String::from_utf8_lossy(&diff_output.stdout).to_string(),
            }]
        } else {
            Vec::new()
        };

        // Compute code hash
        let code_hash = self.compute_code_hash().await?;

        Ok(CodeState {
            git_commit,
            git_branch,
            git_dirty,
            code_hash,
            patches,
        })
    }

    async fn compute_code_hash(&self) -> Result<String, ReproducibilityError> {
        // Hash all source files
        use sha2::{Sha256, Digest};
        let output = tokio::process::Command::new("git")
            .args(["ls-files", "-s"])
            .current_dir(&self.repo_path)
            .output()
            .await?;

        Ok(hex::encode(Sha256::digest(&output.stdout)))
    }
}

/// State store trait
#[async_trait]
pub trait StateStore: Send + Sync {
    async fn save(&self, state: &ExperimentState) -> Result<(), ReproducibilityError>;
    async fn get(&self, id: Uuid) -> Result<ExperimentState, ReproducibilityError>;
    async fn get_for_run(&self, run_id: RunId) -> Result<ExperimentState, ReproducibilityError>;
    async fn list_for_experiment(&self, experiment_id: ExperimentId) -> Result<Vec<ExperimentState>, ReproducibilityError>;
}

/// Reproducibility types
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ValidationReport {
    pub state_id: Uuid,
    pub is_reproducible: bool,
    pub issues: Vec<ValidationIssue>,
    pub warnings: Vec<ValidationIssue>,
    pub validated_at: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ValidationIssue {
    pub severity: IssueSeverity,
    pub category: IssueCategory,
    pub message: String,
    pub suggestion: Option<String>,
}

#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum IssueSeverity {
    Error,
    Warning,
    Info,
}

#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum IssueCategory {
    Code,
    Environment,
    Data,
    Configuration,
    Randomness,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ReplayResult {
    pub state_id: Uuid,
    pub environment_differences: Vec<String>,
    pub ready_to_execute: bool,
    pub setup_commands: Vec<String>,
    pub replayed_at: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StateComparison {
    pub state_a_id: Uuid,
    pub state_b_id: Uuid,
    pub are_equivalent: bool,
    pub differences: Vec<StateDifference>,
    pub compared_at: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StateDifference {
    pub category: DifferenceCategory,
    pub field: String,
    pub value_a: String,
    pub value_b: String,
    pub impact: DifferenceImpact,
}

#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum DifferenceCategory {
    Code,
    Environment,
    Data,
    Configuration,
    Randomness,
}

#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum DifferenceImpact {
    Low,
    Medium,
    High,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ReproducibilityCertificate {
    pub id: Uuid,
    pub state_id: Uuid,
    pub experiment_id: ExperimentId,
    pub run_id: RunId,
    pub validation_report: ValidationReport,
    pub environment_hash: String,
    pub code_hash: String,
    pub data_hash: String,
    pub configuration_hash: String,
    pub results_hash: String,
    pub issued_at: DateTime<Utc>,
    pub signature: String,
}

/// Reproducibility errors
#[derive(Debug, thiserror::Error)]
pub enum ReproducibilityError {
    #[error("Checksum mismatch: expected {expected}, got {actual}")]
    ChecksumMismatch { expected: String, actual: String },

    #[error("Validation failed: {issues:?}")]
    ValidationFailed { issues: Vec<ValidationIssue> },

    #[error("Code restore failed: {message}")]
    CodeRestoreFailed { message: String },

    #[error("State not found: {id}")]
    StateNotFound { id: Uuid },

    #[error("IO error: {0}")]
    Io(#[from] std::io::Error),

    #[error("Serialization error: {0}")]
    Serialization(#[from] serde_json::Error),

    #[error("Environment capture error: {0}")]
    EnvironmentCapture(#[from] TrackerError),
}
```

---

## 7. Workflow Orchestration

### 7.1 Workflow Engine

```rust
//! Workflow orchestration for research pipelines
//! Provides DAG-based workflow execution with checkpointing

/// Workflow orchestrator
pub struct WorkflowOrchestrator {
    workflow_store: Arc<dyn WorkflowStore>,
    run_store: Arc<dyn WorkflowRunStore>,
    executor: Arc<WorkflowExecutor>,
    checkpoint_manager: Arc<CheckpointManager>,
    scheduler: Arc<WorkflowScheduler>,
    event_publisher: Arc<dyn EventPublisher>,
    config: OrchestratorConfig,
}

#[derive(Debug, Clone)]
pub struct OrchestratorConfig {
    pub max_concurrent_workflows: usize,
    pub max_concurrent_steps: usize,
    pub default_timeout: std::time::Duration,
    pub checkpoint_interval: std::time::Duration,
    pub retry_policy: RetryPolicy,
}

/// Workflow definition
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Workflow {
    pub id: WorkflowId,
    pub name: String,
    pub description: Option<String>,
    pub version: SemanticVersion,
    pub steps: Vec<WorkflowStep>,
    pub parameters: Vec<WorkflowParameter>,
    pub triggers: Vec<WorkflowTrigger>,
    pub default_timeout: Option<std::time::Duration>,
    pub retry_policy: Option<RetryPolicy>,
    pub created_at: DateTime<Utc>,
    pub created_by: UserId,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct WorkflowStep {
    pub id: String,
    pub name: String,
    pub step_type: StepType,
    pub config: StepConfig,
    pub dependencies: Vec<String>,
    pub condition: Option<StepCondition>,
    pub retry: Option<StepRetry>,
    pub timeout: Option<std::time::Duration>,
    pub outputs: Vec<StepOutput>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum StepType {
    Experiment,
    Benchmark,
    Transform,
    Conditional,
    Parallel,
    Loop,
    SubWorkflow,
    Custom(String),
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StepConfig {
    pub parameters: HashMap<String, serde_json::Value>,
    pub inputs: HashMap<String, InputRef>,
    pub resources: Option<ResourceRequirements>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum InputRef {
    Parameter(String),
    StepOutput { step_id: String, output_name: String },
    Literal(serde_json::Value),
    Expression(String),
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StepCondition {
    pub expression: String,
    pub on_false: ConditionAction,
}

#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ConditionAction {
    Skip,
    Fail,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StepRetry {
    pub max_attempts: u32,
    pub delay: std::time::Duration,
    pub backoff_multiplier: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StepOutput {
    pub name: String,
    pub output_type: OutputType,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum OutputType {
    Scalar,
    Artifact,
    Dataset,
    Metrics,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct WorkflowParameter {
    pub name: String,
    pub parameter_type: ParameterType,
    pub required: bool,
    pub default: Option<serde_json::Value>,
    pub description: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ParameterType {
    String,
    Integer,
    Float,
    Boolean,
    List,
    Object,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum WorkflowTrigger {
    Manual,
    Schedule { cron: String },
    Event { event_type: String, filter: Option<String> },
    Webhook { path: String },
}

/// Workflow run
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct WorkflowRun {
    pub id: WorkflowRunId,
    pub workflow_id: WorkflowId,
    pub workflow_version: SemanticVersion,
    pub status: WorkflowRunStatus,
    pub parameters: HashMap<String, serde_json::Value>,
    pub step_states: HashMap<String, StepState>,
    pub outputs: HashMap<String, serde_json::Value>,
    pub started_at: Option<DateTime<Utc>>,
    pub ended_at: Option<DateTime<Utc>>,
    pub created_at: DateTime<Utc>,
    pub created_by: UserId,
    pub error: Option<WorkflowError>,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum WorkflowRunStatus {
    Pending,
    Running,
    Paused,
    Completed,
    Failed,
    Cancelled,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StepState {
    pub step_id: String,
    pub status: StepStatus,
    pub attempt: u32,
    pub inputs: HashMap<String, serde_json::Value>,
    pub outputs: HashMap<String, serde_json::Value>,
    pub started_at: Option<DateTime<Utc>>,
    pub ended_at: Option<DateTime<Utc>>,
    pub error: Option<String>,
    pub logs_uri: Option<String>,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum StepStatus {
    Pending,
    Running,
    Completed,
    Failed,
    Skipped,
    Cancelled,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct WorkflowError {
    pub step_id: Option<String>,
    pub error_type: String,
    pub message: String,
    pub recoverable: bool,
}

impl WorkflowOrchestrator {
    pub fn new(
        workflow_store: Arc<dyn WorkflowStore>,
        run_store: Arc<dyn WorkflowRunStore>,
        executor: Arc<WorkflowExecutor>,
        checkpoint_manager: Arc<CheckpointManager>,
        scheduler: Arc<WorkflowScheduler>,
        event_publisher: Arc<dyn EventPublisher>,
        config: OrchestratorConfig,
    ) -> Self {
        Self {
            workflow_store,
            run_store,
            executor,
            checkpoint_manager,
            scheduler,
            event_publisher,
            config,
        }
    }

    /// Submit a workflow for execution
    pub async fn submit(
        &self,
        workflow_id: WorkflowId,
        parameters: HashMap<String, serde_json::Value>,
        user_id: UserId,
    ) -> Result<WorkflowRun, WorkflowError> {
        // Get workflow definition
        let workflow = self.workflow_store.get(workflow_id).await?;

        // Validate parameters
        self.validate_parameters(&workflow, &parameters)?;

        // Create run
        let run_id = WorkflowRunId(Uuid::new_v4());
        let now = Utc::now();

        let step_states: HashMap<String, StepState> = workflow
            .steps
            .iter()
            .map(|s| {
                (
                    s.id.clone(),
                    StepState {
                        step_id: s.id.clone(),
                        status: StepStatus::Pending,
                        attempt: 0,
                        inputs: HashMap::new(),
                        outputs: HashMap::new(),
                        started_at: None,
                        ended_at: None,
                        error: None,
                        logs_uri: None,
                    },
                )
            })
            .collect();

        let run = WorkflowRun {
            id: run_id,
            workflow_id,
            workflow_version: workflow.version.clone(),
            status: WorkflowRunStatus::Pending,
            parameters,
            step_states,
            outputs: HashMap::new(),
            started_at: None,
            ended_at: None,
            created_at: now,
            created_by: user_id,
            error: None,
        };

        // Save run
        self.run_store.save(&run).await?;

        // Queue for execution
        self.scheduler.enqueue(run_id).await?;

        Ok(run)
    }

    /// Execute a workflow run
    pub async fn execute(&self, run_id: WorkflowRunId) -> Result<WorkflowRun, WorkflowError> {
        let mut run = self.run_store.get(run_id).await?;
        let workflow = self.workflow_store.get(run.workflow_id).await?;

        // Update status
        run.status = WorkflowRunStatus::Running;
        run.started_at = Some(Utc::now());
        self.run_store.save(&run).await?;

        // Build execution DAG
        let dag = self.build_dag(&workflow)?;

        // Execute steps in topological order
        let mut completed_steps = HashSet::new();

        while completed_steps.len() < workflow.steps.len() {
            // Find ready steps
            let ready_steps: Vec<_> = dag
                .iter()
                .filter(|(step_id, deps)| {
                    !completed_steps.contains(*step_id)
                        && deps.iter().all(|d| completed_steps.contains(d))
                })
                .map(|(step_id, _)| step_id.clone())
                .collect();

            if ready_steps.is_empty() && completed_steps.len() < workflow.steps.len() {
                // Deadlock or all remaining steps failed
                break;
            }

            // Execute ready steps in parallel
            let mut handles = Vec::new();

            for step_id in ready_steps.iter().take(self.config.max_concurrent_steps) {
                let step = workflow
                    .steps
                    .iter()
                    .find(|s| &s.id == step_id)
                    .unwrap()
                    .clone();

                let executor = self.executor.clone();
                let run_clone = run.clone();
                let checkpoint_manager = self.checkpoint_manager.clone();
                let step_id = step_id.clone();

                let handle = tokio::spawn(async move {
                    let result = executor
                        .execute_step(&step, &run_clone, &checkpoint_manager)
                        .await;
                    (step_id, result)
                });

                handles.push(handle);
            }

            // Wait for step results
            for handle in handles {
                let (step_id, result) = handle.await.map_err(|e| WorkflowError {
                    step_id: None,
                    error_type: "task_join".to_string(),
                    message: e.to_string(),
                    recoverable: false,
                })?;

                match result {
                    Ok(state) => {
                        run.step_states.insert(step_id.clone(), state);
                        if run.step_states[&step_id].status == StepStatus::Completed
                            || run.step_states[&step_id].status == StepStatus::Skipped
                        {
                            completed_steps.insert(step_id);
                        }
                    }
                    Err(e) => {
                        run.step_states.get_mut(&step_id).unwrap().status = StepStatus::Failed;
                        run.step_states.get_mut(&step_id).unwrap().error = Some(e.message.clone());

                        if !e.recoverable {
                            run.status = WorkflowRunStatus::Failed;
                            run.error = Some(e);
                            run.ended_at = Some(Utc::now());
                            self.run_store.save(&run).await?;
                            return Err(run.error.clone().unwrap());
                        }
                    }
                }

                // Checkpoint
                self.checkpoint_manager.save_checkpoint(&run).await?;
                self.run_store.save(&run).await?;
            }
        }

        // Finalize
        run.status = if completed_steps.len() == workflow.steps.len() {
            WorkflowRunStatus::Completed
        } else {
            WorkflowRunStatus::Failed
        };
        run.ended_at = Some(Utc::now());

        // Collect outputs
        for step in &workflow.steps {
            for output in &step.outputs {
                if let Some(state) = run.step_states.get(&step.id) {
                    if let Some(value) = state.outputs.get(&output.name) {
                        run.outputs
                            .insert(format!("{}.{}", step.id, output.name), value.clone());
                    }
                }
            }
        }

        self.run_store.save(&run).await?;

        Ok(run)
    }

    /// Pause a running workflow
    pub async fn pause(&self, run_id: WorkflowRunId) -> Result<WorkflowRun, WorkflowError> {
        let mut run = self.run_store.get(run_id).await?;

        if run.status != WorkflowRunStatus::Running {
            return Err(WorkflowError {
                step_id: None,
                error_type: "invalid_state".to_string(),
                message: "Workflow is not running".to_string(),
                recoverable: false,
            });
        }

        run.status = WorkflowRunStatus::Paused;
        self.run_store.save(&run).await?;

        Ok(run)
    }

    /// Resume a paused workflow
    pub async fn resume(&self, run_id: WorkflowRunId) -> Result<WorkflowRun, WorkflowError> {
        let run = self.run_store.get(run_id).await?;

        if run.status != WorkflowRunStatus::Paused {
            return Err(WorkflowError {
                step_id: None,
                error_type: "invalid_state".to_string(),
                message: "Workflow is not paused".to_string(),
                recoverable: false,
            });
        }

        // Re-queue for execution
        self.scheduler.enqueue(run_id).await?;

        self.execute(run_id).await
    }

    /// Cancel a workflow
    pub async fn cancel(&self, run_id: WorkflowRunId) -> Result<WorkflowRun, WorkflowError> {
        let mut run = self.run_store.get(run_id).await?;

        if run.status == WorkflowRunStatus::Completed
            || run.status == WorkflowRunStatus::Cancelled
            || run.status == WorkflowRunStatus::Failed
        {
            return Err(WorkflowError {
                step_id: None,
                error_type: "invalid_state".to_string(),
                message: "Workflow already terminated".to_string(),
                recoverable: false,
            });
        }

        run.status = WorkflowRunStatus::Cancelled;
        run.ended_at = Some(Utc::now());

        // Cancel running steps
        for state in run.step_states.values_mut() {
            if state.status == StepStatus::Running {
                state.status = StepStatus::Cancelled;
                state.ended_at = Some(Utc::now());
            }
        }

        self.run_store.save(&run).await?;

        Ok(run)
    }

    /// Get workflow run status
    pub async fn get_status(&self, run_id: WorkflowRunId) -> Result<WorkflowRun, WorkflowError> {
        self.run_store.get(run_id).await
    }

    // Private helpers

    fn validate_parameters(
        &self,
        workflow: &Workflow,
        parameters: &HashMap<String, serde_json::Value>,
    ) -> Result<(), WorkflowError> {
        for param in &workflow.parameters {
            if param.required && !parameters.contains_key(&param.name) && param.default.is_none() {
                return Err(WorkflowError {
                    step_id: None,
                    error_type: "validation".to_string(),
                    message: format!("Missing required parameter: {}", param.name),
                    recoverable: false,
                });
            }
        }
        Ok(())
    }

    fn build_dag(&self, workflow: &Workflow) -> Result<HashMap<String, Vec<String>>, WorkflowError> {
        let mut dag = HashMap::new();

        for step in &workflow.steps {
            dag.insert(step.id.clone(), step.dependencies.clone());
        }

        // Validate no cycles
        self.validate_dag(&dag)?;

        Ok(dag)
    }

    fn validate_dag(&self, dag: &HashMap<String, Vec<String>>) -> Result<(), WorkflowError> {
        // Topological sort to detect cycles
        let mut visited = HashSet::new();
        let mut in_stack = HashSet::new();

        fn visit(
            node: &str,
            dag: &HashMap<String, Vec<String>>,
            visited: &mut HashSet<String>,
            in_stack: &mut HashSet<String>,
        ) -> Result<(), WorkflowError> {
            if in_stack.contains(node) {
                return Err(WorkflowError {
                    step_id: Some(node.to_string()),
                    error_type: "cycle".to_string(),
                    message: "Cycle detected in workflow DAG".to_string(),
                    recoverable: false,
                });
            }

            if visited.contains(node) {
                return Ok(());
            }

            in_stack.insert(node.to_string());

            if let Some(deps) = dag.get(node) {
                for dep in deps {
                    visit(dep, dag, visited, in_stack)?;
                }
            }

            in_stack.remove(node);
            visited.insert(node.to_string());

            Ok(())
        }

        for node in dag.keys() {
            visit(node, dag, &mut visited, &mut in_stack)?;
        }

        Ok(())
    }
}

/// Workflow executor
pub struct WorkflowExecutor {
    experiment_tracker: Arc<ExperimentTracker>,
    benchmark_runner: Arc<BenchmarkRunner>,
    dataset_manager: Arc<DatasetManager>,
}

impl WorkflowExecutor {
    pub async fn execute_step(
        &self,
        step: &WorkflowStep,
        run: &WorkflowRun,
        checkpoint_manager: &CheckpointManager,
    ) -> Result<StepState, WorkflowError> {
        let mut state = StepState {
            step_id: step.id.clone(),
            status: StepStatus::Running,
            attempt: 0,
            inputs: HashMap::new(),
            outputs: HashMap::new(),
            started_at: Some(Utc::now()),
            ended_at: None,
            error: None,
            logs_uri: None,
        };

        // Resolve inputs
        state.inputs = self.resolve_inputs(&step.config.inputs, run)?;

        // Check condition
        if let Some(ref condition) = step.condition {
            let should_run = self.evaluate_condition(&condition.expression, &state.inputs)?;
            if !should_run {
                state.status = StepStatus::Skipped;
                state.ended_at = Some(Utc::now());
                return Ok(state);
            }
        }

        // Execute with retries
        let max_attempts = step.retry.as_ref().map(|r| r.max_attempts).unwrap_or(1);
        let mut last_error = None;

        while state.attempt < max_attempts {
            state.attempt += 1;

            match self.execute_step_type(step, &state.inputs).await {
                Ok(outputs) => {
                    state.outputs = outputs;
                    state.status = StepStatus::Completed;
                    state.ended_at = Some(Utc::now());
                    return Ok(state);
                }
                Err(e) => {
                    last_error = Some(e);
                    if state.attempt < max_attempts {
                        if let Some(ref retry) = step.retry {
                            let delay = retry.delay.mul_f64(
                                retry.backoff_multiplier.powi(state.attempt as i32 - 1),
                            );
                            tokio::time::sleep(delay).await;
                        }
                    }
                }
            }
        }

        state.status = StepStatus::Failed;
        state.error = last_error.map(|e| e.message);
        state.ended_at = Some(Utc::now());

        Err(WorkflowError {
            step_id: Some(step.id.clone()),
            error_type: "execution".to_string(),
            message: state.error.clone().unwrap_or_default(),
            recoverable: false,
        })
    }

    async fn execute_step_type(
        &self,
        step: &WorkflowStep,
        inputs: &HashMap<String, serde_json::Value>,
    ) -> Result<HashMap<String, serde_json::Value>, WorkflowError> {
        match &step.step_type {
            StepType::Experiment => self.execute_experiment_step(step, inputs).await,
            StepType::Benchmark => self.execute_benchmark_step(step, inputs).await,
            StepType::Transform => self.execute_transform_step(step, inputs).await,
            StepType::Conditional => self.execute_conditional_step(step, inputs).await,
            StepType::Parallel => self.execute_parallel_step(step, inputs).await,
            StepType::Loop => self.execute_loop_step(step, inputs).await,
            StepType::SubWorkflow => self.execute_subworkflow_step(step, inputs).await,
            StepType::Custom(name) => self.execute_custom_step(name, step, inputs).await,
        }
    }

    async fn execute_experiment_step(
        &self,
        _step: &WorkflowStep,
        _inputs: &HashMap<String, serde_json::Value>,
    ) -> Result<HashMap<String, serde_json::Value>, WorkflowError> {
        // Execute experiment using tracker
        Ok(HashMap::new())
    }

    async fn execute_benchmark_step(
        &self,
        _step: &WorkflowStep,
        _inputs: &HashMap<String, serde_json::Value>,
    ) -> Result<HashMap<String, serde_json::Value>, WorkflowError> {
        // Execute benchmark using runner
        Ok(HashMap::new())
    }

    async fn execute_transform_step(
        &self,
        _step: &WorkflowStep,
        _inputs: &HashMap<String, serde_json::Value>,
    ) -> Result<HashMap<String, serde_json::Value>, WorkflowError> {
        // Execute data transformation
        Ok(HashMap::new())
    }

    async fn execute_conditional_step(
        &self,
        _step: &WorkflowStep,
        _inputs: &HashMap<String, serde_json::Value>,
    ) -> Result<HashMap<String, serde_json::Value>, WorkflowError> {
        // Execute conditional logic
        Ok(HashMap::new())
    }

    async fn execute_parallel_step(
        &self,
        _step: &WorkflowStep,
        _inputs: &HashMap<String, serde_json::Value>,
    ) -> Result<HashMap<String, serde_json::Value>, WorkflowError> {
        // Execute parallel branches
        Ok(HashMap::new())
    }

    async fn execute_loop_step(
        &self,
        _step: &WorkflowStep,
        _inputs: &HashMap<String, serde_json::Value>,
    ) -> Result<HashMap<String, serde_json::Value>, WorkflowError> {
        // Execute loop
        Ok(HashMap::new())
    }

    async fn execute_subworkflow_step(
        &self,
        _step: &WorkflowStep,
        _inputs: &HashMap<String, serde_json::Value>,
    ) -> Result<HashMap<String, serde_json::Value>, WorkflowError> {
        // Execute sub-workflow
        Ok(HashMap::new())
    }

    async fn execute_custom_step(
        &self,
        _name: &str,
        _step: &WorkflowStep,
        _inputs: &HashMap<String, serde_json::Value>,
    ) -> Result<HashMap<String, serde_json::Value>, WorkflowError> {
        // Execute custom step type
        Ok(HashMap::new())
    }

    fn resolve_inputs(
        &self,
        input_refs: &HashMap<String, InputRef>,
        run: &WorkflowRun,
    ) -> Result<HashMap<String, serde_json::Value>, WorkflowError> {
        let mut resolved = HashMap::new();

        for (name, input_ref) in input_refs {
            let value = match input_ref {
                InputRef::Parameter(param_name) => run
                    .parameters
                    .get(param_name)
                    .cloned()
                    .ok_or_else(|| WorkflowError {
                        step_id: None,
                        error_type: "input_resolution".to_string(),
                        message: format!("Parameter not found: {}", param_name),
                        recoverable: false,
                    })?,
                InputRef::StepOutput { step_id, output_name } => run
                    .step_states
                    .get(step_id)
                    .and_then(|s| s.outputs.get(output_name))
                    .cloned()
                    .ok_or_else(|| WorkflowError {
                        step_id: Some(step_id.clone()),
                        error_type: "input_resolution".to_string(),
                        message: format!("Step output not found: {}.{}", step_id, output_name),
                        recoverable: false,
                    })?,
                InputRef::Literal(value) => value.clone(),
                InputRef::Expression(expr) => self.evaluate_expression(expr, run)?,
            };

            resolved.insert(name.clone(), value);
        }

        Ok(resolved)
    }

    fn evaluate_condition(
        &self,
        _expression: &str,
        _inputs: &HashMap<String, serde_json::Value>,
    ) -> Result<bool, WorkflowError> {
        // Evaluate condition expression
        Ok(true)
    }

    fn evaluate_expression(
        &self,
        _expression: &str,
        _run: &WorkflowRun,
    ) -> Result<serde_json::Value, WorkflowError> {
        // Evaluate expression
        Ok(serde_json::Value::Null)
    }
}

/// Checkpoint manager
pub struct CheckpointManager {
    store: Arc<dyn CheckpointStore>,
}

impl CheckpointManager {
    pub async fn save_checkpoint(&self, run: &WorkflowRun) -> Result<(), WorkflowError> {
        let checkpoint = Checkpoint {
            run_id: run.id,
            state: serde_json::to_value(run).map_err(|e| WorkflowError {
                step_id: None,
                error_type: "serialization".to_string(),
                message: e.to_string(),
                recoverable: false,
            })?,
            created_at: Utc::now(),
        };

        self.store.save(&checkpoint).await
    }

    pub async fn load_checkpoint(
        &self,
        run_id: WorkflowRunId,
    ) -> Result<Option<Checkpoint>, WorkflowError> {
        self.store.get(run_id).await
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Checkpoint {
    pub run_id: WorkflowRunId,
    pub state: serde_json::Value,
    pub created_at: DateTime<Utc>,
}

/// Workflow scheduler
pub struct WorkflowScheduler {
    queue: Arc<tokio::sync::Mutex<std::collections::VecDeque<WorkflowRunId>>>,
}

impl WorkflowScheduler {
    pub fn new() -> Self {
        Self {
            queue: Arc::new(tokio::sync::Mutex::new(std::collections::VecDeque::new())),
        }
    }

    pub async fn enqueue(&self, run_id: WorkflowRunId) -> Result<(), WorkflowError> {
        let mut queue = self.queue.lock().await;
        queue.push_back(run_id);
        Ok(())
    }

    pub async fn dequeue(&self) -> Option<WorkflowRunId> {
        let mut queue = self.queue.lock().await;
        queue.pop_front()
    }
}

/// Storage traits
#[async_trait]
pub trait WorkflowStore: Send + Sync {
    async fn save(&self, workflow: &Workflow) -> Result<(), WorkflowError>;
    async fn get(&self, id: WorkflowId) -> Result<Workflow, WorkflowError>;
    async fn list(&self, filter: WorkflowFilter) -> Result<Vec<Workflow>, WorkflowError>;
}

#[async_trait]
pub trait WorkflowRunStore: Send + Sync {
    async fn save(&self, run: &WorkflowRun) -> Result<(), WorkflowError>;
    async fn get(&self, id: WorkflowRunId) -> Result<WorkflowRun, WorkflowError>;
    async fn list_for_workflow(&self, workflow_id: WorkflowId) -> Result<Vec<WorkflowRun>, WorkflowError>;
}

#[async_trait]
pub trait CheckpointStore: Send + Sync {
    async fn save(&self, checkpoint: &Checkpoint) -> Result<(), WorkflowError>;
    async fn get(&self, run_id: WorkflowRunId) -> Result<Option<Checkpoint>, WorkflowError>;
}

#[derive(Debug, Clone, Default)]
pub struct WorkflowFilter {
    pub name: Option<String>,
    pub created_by: Option<UserId>,
    pub tags: Option<Vec<String>>,
}

use std::collections::HashSet;
```

---

## Document Metadata

| Field | Value |
|-------|-------|
| **Version** | 1.0.0 |
| **Status** | Draft |
| **SPARC Phase** | Pseudocode (Part 3 of 3) |
| **Created** | 2025-11-28 |
| **Ecosystem** | LLM DevOps |
| **Previous Parts** | Part 1: Core Data Models & Experiment Tracking |
|  | Part 2: Metric Benchmarking & Dataset Versioning |
| **Next Phase** | Architecture |

---

*This pseudocode document completes the SPARC Pseudocode phase for LLM-Research-Lab. The next phase will cover system architecture design including deployment topology, data flow diagrams, and infrastructure specifications.*

---

# PART III: ARCHITECTURE

> **SPARC Phase 3**
> Defines the system design and infrastructure

---

## 1. Architecture Overview

### 1.1 System Context

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                           LLM DevOps Ecosystem                                   │
│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────┐ │
│  │  LLM-Test-Bench │  │LLM-Analytics-Hub│  │   LLM-Registry  │  │LLM-Data-Vault│ │
│  │   (Benchmarks)  │  │  (Visualization)│  │   (Artifacts)   │  │(Data Gov.)  │ │
│  └────────┬────────┘  └────────┬────────┘  └────────┬────────┘  └──────┬──────┘ │
│           │                    │                    │                   │        │
│           └────────────────────┼────────────────────┼───────────────────┘        │
│                                │                    │                            │
│                    ┌───────────▼────────────────────▼───────────┐                │
│                    │         LLM-Research-Lab                    │                │
│                    │  ┌─────────────────────────────────────┐   │                │
│                    │  │           API Gateway               │   │                │
│                    │  └─────────────────────────────────────┘   │                │
│                    │  ┌───────────┐ ┌───────────┐ ┌──────────┐  │                │
│                    │  │Experiment │ │  Metric   │ │ Dataset  │  │                │
│                    │  │ Tracking  │ │Benchmarking│ │Versioning│  │                │
│                    │  └───────────┘ └───────────┘ └──────────┘  │                │
│                    │  ┌───────────┐ ┌───────────┐ ┌──────────┐  │                │
│                    │  │Repro-     │ │ Workflow  │ │Statistical│  │                │
│                    │  │ducibility │ │Orchestrate│ │ Analysis │  │                │
│                    │  └───────────┘ └───────────┘ └──────────┘  │                │
│                    └────────────────────────────────────────────┘                │
└─────────────────────────────────────────────────────────────────────────────────┘
                                        │
                    ┌───────────────────┼───────────────────┐
                    │                   │                   │
              ┌─────▼─────┐      ┌──────▼──────┐     ┌──────▼──────┐
              │    AI     │      │    Data     │      │   MLOps    │
              │Researchers│      │ Scientists  │      │  Engineers │
              └───────────┘      └─────────────┘      └────────────┘
```

### 1.2 Architecture Principles

| Principle | Description | Implementation |
|-----------|-------------|----------------|
| **Microservices** | Loosely coupled, independently deployable services | Each domain (Experiment, Metric, Dataset) is a separate service |
| **Event-Driven** | Asynchronous communication via events | Apache Kafka for event streaming, webhooks for integrations |
| **API-First** | Contract-driven development | OpenAPI 3.1 specs, gRPC for internal services |
| **Cloud-Native** | Designed for Kubernetes deployment | Helm charts, operators, horizontal scaling |
| **Zero-Trust** | Never trust, always verify | mTLS, JWT tokens, RBAC at every layer |
| **Observability** | Full visibility into system behavior | OpenTelemetry, structured logging, distributed tracing |
| **Immutability** | Data and artifacts are immutable | Content-addressable storage, append-only logs |
| **Reproducibility** | Every experiment can be replayed | Complete state capture, deterministic execution |

### 1.3 Technology Stack

```yaml
# Core Technology Decisions
runtime:
  language: Rust 1.75+
  async_runtime: Tokio
  web_framework: Axum
  grpc: Tonic

data:
  primary_database: PostgreSQL 16
  time_series: ClickHouse
  cache: Redis 7
  search: Meilisearch
  object_storage: S3-compatible (MinIO/AWS S3)

messaging:
  event_streaming: Apache Kafka 3.6
  task_queue: Redis Streams
  pubsub: NATS

orchestration:
  container: Kubernetes 1.28+
  service_mesh: Linkerd (optional)
  secrets: HashiCorp Vault
  config: Kubernetes ConfigMaps + Sealed Secrets

observability:
  metrics: Prometheus + Grafana
  logging: Vector + Loki
  tracing: Jaeger (OpenTelemetry)
  profiling: Pyroscope

ci_cd:
  pipelines: GitHub Actions
  artifacts: Harbor Registry
  iac: Terraform + Pulumi
  gitops: ArgoCD
```

---

## 2. Service Architecture

### 2.1 Service Decomposition

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                              API Layer                                       │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │                      API Gateway (Kong/Envoy)                        │    │
│  │  • Rate Limiting  • Auth  • Routing  • TLS Termination  • CORS      │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
└─────────────────────────────────────────────────────────────────────────────┘
                                      │
        ┌─────────────────────────────┼─────────────────────────────┐
        │                             │                             │
        ▼                             ▼                             ▼
┌───────────────┐           ┌───────────────┐           ┌───────────────┐
│  Experiment   │           │    Metric     │           │    Dataset    │
│   Service     │           │   Service     │           │   Service     │
│               │           │               │           │               │
│ • CRUD Exp    │           │ • Metric Reg  │           │ • Versioning  │
│ • Run Mgmt    │           │ • Benchmarks  │           │ • Transforms  │
│ • Artifacts   │           │ • Aggregation │           │ • Lineage     │
│ • Comparison  │           │ • Statistics  │           │ • Streaming   │
└───────┬───────┘           └───────┬───────┘           └───────┬───────┘
        │                           │                           │
        └───────────────────────────┼───────────────────────────┘
                                    │
        ┌─────────────────────────────┼─────────────────────────────┐
        │                             │                             │
        ▼                             ▼                             ▼
┌───────────────┐           ┌───────────────┐           ┌───────────────┐
│Reproducibility│           │   Workflow    │           │  Integration  │
│   Service     │           │   Service     │           │   Service     │
│               │           │               │           │               │
│ • State Cap   │           │ • DAG Exec    │           │ • Test-Bench  │
│ • Validation  │           │ • Scheduling  │           │ • Analytics   │
│ • Replay      │           │ • Checkpoint  │           │ • Registry    │
│ • Certs       │           │ • Recovery    │           │ • Data-Vault  │
└───────────────┘           └───────────────┘           └───────────────┘
                                    │
                                    ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                           Shared Services                                    │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────┐ │
│  │   Auth/     │  │   Event     │  │  Storage    │  │    Notification     │ │
│  │   AuthZ     │  │   Bus       │  │  Service    │  │      Service        │ │
│  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────────────┘ │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 2.2 Service Specifications

#### 2.2.1 Experiment Service

```yaml
service:
  name: experiment-service
  version: 1.0.0
  port: 8001
  grpc_port: 9001

responsibilities:
  - Experiment CRUD operations
  - Run lifecycle management
  - Artifact storage coordination
  - Environment capture
  - Run comparison and analysis
  - Lineage tracking

dependencies:
  internal:
    - storage-service
    - event-bus
    - auth-service
  external:
    - postgresql (primary store)
    - redis (caching)
    - s3 (artifacts)
    - kafka (events)

api_endpoints:
  rest:
    - POST   /api/v1/experiments
    - GET    /api/v1/experiments/{id}
    - PUT    /api/v1/experiments/{id}
    - DELETE /api/v1/experiments/{id}
    - GET    /api/v1/experiments
    - POST   /api/v1/experiments/{id}/runs
    - GET    /api/v1/experiments/{id}/runs
    - GET    /api/v1/runs/{id}
    - PUT    /api/v1/runs/{id}/status
    - POST   /api/v1/runs/{id}/metrics
    - POST   /api/v1/runs/{id}/artifacts
    - GET    /api/v1/runs/{id}/artifacts
    - POST   /api/v1/runs/compare
    - GET    /api/v1/experiments/{id}/lineage

  grpc:
    - ExperimentService.CreateExperiment
    - ExperimentService.GetExperiment
    - ExperimentService.StartRun
    - ExperimentService.StreamMetrics
    - ExperimentService.UploadArtifact

scaling:
  min_replicas: 2
  max_replicas: 10
  cpu_threshold: 70%
  memory_threshold: 80%

resources:
  requests:
    cpu: 500m
    memory: 512Mi
  limits:
    cpu: 2000m
    memory: 2Gi

health:
  liveness: /health/live
  readiness: /health/ready
  startup: /health/startup
```

#### 2.2.2 Metric Service

```yaml
service:
  name: metric-service
  version: 1.0.0
  port: 8002
  grpc_port: 9002

responsibilities:
  - Metric definition registry
  - Benchmark execution
  - Statistical analysis
  - Metric aggregation
  - Cross-model comparison

dependencies:
  internal:
    - experiment-service
    - storage-service
    - event-bus
  external:
    - postgresql (definitions)
    - clickhouse (time-series)
    - redis (caching)
    - kafka (events)

api_endpoints:
  rest:
    - POST   /api/v1/metrics
    - GET    /api/v1/metrics/{id}
    - GET    /api/v1/metrics
    - POST   /api/v1/benchmarks
    - GET    /api/v1/benchmarks/{id}
    - GET    /api/v1/benchmarks/{id}/status
    - GET    /api/v1/benchmarks/{id}/results
    - POST   /api/v1/benchmarks/compare
    - POST   /api/v1/statistics/test
    - GET    /api/v1/statistics/aggregations

  grpc:
    - MetricService.RegisterMetric
    - MetricService.ComputeMetric
    - MetricService.RunBenchmark
    - MetricService.StreamResults

scaling:
  min_replicas: 2
  max_replicas: 20
  cpu_threshold: 60%

resources:
  requests:
    cpu: 1000m
    memory: 1Gi
  limits:
    cpu: 4000m
    memory: 8Gi
```

#### 2.2.3 Dataset Service

```yaml
service:
  name: dataset-service
  version: 1.0.0
  port: 8003
  grpc_port: 9003

responsibilities:
  - Dataset registration and versioning
  - Content-addressable storage
  - Data transformation pipelines
  - Lineage graph management
  - Data streaming

dependencies:
  internal:
    - storage-service
    - integration-service (data-vault)
    - event-bus
  external:
    - postgresql (metadata)
    - s3 (content)
    - redis (caching)

api_endpoints:
  rest:
    - POST   /api/v1/datasets
    - GET    /api/v1/datasets/{id}
    - GET    /api/v1/datasets
    - POST   /api/v1/datasets/{id}/versions
    - GET    /api/v1/datasets/{id}/versions
    - GET    /api/v1/datasets/{id}/versions/{version}
    - POST   /api/v1/datasets/{id}/splits
    - GET    /api/v1/datasets/{id}/lineage
    - GET    /api/v1/datasets/{id}/versions/{version}/stream
    - POST   /api/v1/datasets/{id}/versions/compare

  grpc:
    - DatasetService.RegisterDataset
    - DatasetService.CreateVersion
    - DatasetService.StreamData
    - DatasetService.GetLineage

scaling:
  min_replicas: 2
  max_replicas: 15

resources:
  requests:
    cpu: 500m
    memory: 1Gi
  limits:
    cpu: 2000m
    memory: 4Gi
```

#### 2.2.4 Workflow Service

```yaml
service:
  name: workflow-service
  version: 1.0.0
  port: 8004
  grpc_port: 9004

responsibilities:
  - Workflow definition management
  - DAG execution engine
  - Step scheduling and coordination
  - Checkpoint management
  - Recovery and retry handling

dependencies:
  internal:
    - experiment-service
    - metric-service
    - dataset-service
    - event-bus
  external:
    - postgresql (workflows, runs)
    - redis (scheduling, locks)
    - s3 (checkpoints)

api_endpoints:
  rest:
    - POST   /api/v1/workflows
    - GET    /api/v1/workflows/{id}
    - GET    /api/v1/workflows
    - POST   /api/v1/workflows/{id}/runs
    - GET    /api/v1/workflows/{id}/runs
    - GET    /api/v1/workflow-runs/{id}
    - POST   /api/v1/workflow-runs/{id}/pause
    - POST   /api/v1/workflow-runs/{id}/resume
    - POST   /api/v1/workflow-runs/{id}/cancel
    - GET    /api/v1/workflow-runs/{id}/logs

scaling:
  min_replicas: 2
  max_replicas: 8

resources:
  requests:
    cpu: 500m
    memory: 512Mi
  limits:
    cpu: 2000m
    memory: 2Gi
```

#### 2.2.5 Reproducibility Service

```yaml
service:
  name: reproducibility-service
  version: 1.0.0
  port: 8005
  grpc_port: 9005

responsibilities:
  - Experiment state capture
  - Reproducibility validation
  - Experiment replay
  - Certificate generation
  - State comparison

dependencies:
  internal:
    - experiment-service
    - dataset-service
    - storage-service
  external:
    - postgresql (states)
    - s3 (snapshots)

api_endpoints:
  rest:
    - POST   /api/v1/states
    - GET    /api/v1/states/{id}
    - GET    /api/v1/runs/{run_id}/state
    - POST   /api/v1/states/{id}/validate
    - POST   /api/v1/states/{id}/replay
    - POST   /api/v1/states/compare
    - POST   /api/v1/certificates
    - GET    /api/v1/certificates/{id}

scaling:
  min_replicas: 2
  max_replicas: 6
```

#### 2.2.6 Integration Service

```yaml
service:
  name: integration-service
  version: 1.0.0
  port: 8006
  grpc_port: 9006

responsibilities:
  - External system integration
  - Circuit breaker management
  - Retry coordination
  - Webhook handling
  - Event translation

integrations:
  - name: test-bench
    type: rest
    circuit_breaker: true
    retry: exponential

  - name: analytics-hub
    type: rest + streaming
    buffering: true

  - name: registry
    type: grpc
    streaming: true

  - name: data-vault
    type: rest
    auth: oauth2

api_endpoints:
  rest:
    - POST   /api/v1/integrations/{name}/test
    - GET    /api/v1/integrations/{name}/status
    - POST   /api/v1/webhooks
    - GET    /api/v1/webhooks/{id}

scaling:
  min_replicas: 2
  max_replicas: 10
```

---

## 3. Deployment Architecture

### 3.1 Kubernetes Topology

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         Kubernetes Cluster                                   │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │                        Ingress Namespace                                │ │
│  │  ┌──────────────────────────────────────────────────────────────────┐  │ │
│  │  │  Ingress Controller (NGINX/Traefik)                              │  │ │
│  │  │  • TLS Termination                                               │  │ │
│  │  │  • Path-based Routing                                            │  │ │
│  │  │  • Rate Limiting                                                 │  │ │
│  │  └──────────────────────────────────────────────────────────────────┘  │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                      │                                       │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │                      research-lab Namespace                             │ │
│  │                                                                         │ │
│  │  ┌─────────────┐ ┌─────────────┐ ┌─────────────┐ ┌─────────────┐       │ │
│  │  │ experiment  │ │   metric    │ │   dataset   │ │  workflow   │       │ │
│  │  │  Deployment │ │ Deployment  │ │ Deployment  │ │ Deployment  │       │ │
│  │  │  (2-10)     │ │  (2-20)     │ │  (2-15)     │ │  (2-8)      │       │ │
│  │  └─────────────┘ └─────────────┘ └─────────────┘ └─────────────┘       │ │
│  │                                                                         │ │
│  │  ┌─────────────┐ ┌─────────────┐ ┌─────────────────────────────────┐   │ │
│  │  │reproducibil │ │ integration │ │         Shared Services         │   │ │
│  │  │ Deployment  │ │ Deployment  │ │  ┌─────┐ ┌─────┐ ┌──────────┐  │   │ │
│  │  │  (2-6)      │ │  (2-10)     │ │  │Redis│ │NATS │ │ConfigMaps│  │   │ │
│  │  └─────────────┘ └─────────────┘ │  └─────┘ └─────┘ └──────────┘  │   │ │
│  │                                   └─────────────────────────────────┘   │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │                        data Namespace                                   │ │
│  │  ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐           │ │
│  │  │   PostgreSQL    │ │   ClickHouse    │ │     Kafka       │           │ │
│  │  │  StatefulSet    │ │  StatefulSet    │ │  StatefulSet    │           │ │
│  │  │  (3 replicas)   │ │  (3 replicas)   │ │  (3 brokers)    │           │ │
│  │  └─────────────────┘ └─────────────────┘ └─────────────────┘           │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │                     observability Namespace                             │ │
│  │  ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐     │ │
│  │  │Prometheus│ │ Grafana  │ │  Jaeger  │ │   Loki   │ │ Vector   │     │ │
│  │  └──────────┘ └──────────┘ └──────────┘ └──────────┘ └──────────┘     │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 3.2 Helm Chart Structure

```
helm/
├── Chart.yaml
├── values.yaml
├── values-dev.yaml
├── values-staging.yaml
├── values-prod.yaml
├── templates/
│   ├── _helpers.tpl
│   ├── namespace.yaml
│   ├── configmap.yaml
│   ├── secrets.yaml
│   ├── serviceaccount.yaml
│   │
│   ├── experiment/
│   │   ├── deployment.yaml
│   │   ├── service.yaml
│   │   ├── hpa.yaml
│   │   ├── pdb.yaml
│   │   └── servicemonitor.yaml
│   │
│   ├── metric/
│   │   ├── deployment.yaml
│   │   ├── service.yaml
│   │   ├── hpa.yaml
│   │   └── servicemonitor.yaml
│   │
│   ├── dataset/
│   │   ├── deployment.yaml
│   │   ├── service.yaml
│   │   ├── hpa.yaml
│   │   └── servicemonitor.yaml
│   │
│   ├── workflow/
│   │   ├── deployment.yaml
│   │   ├── service.yaml
│   │   ├── hpa.yaml
│   │   └── servicemonitor.yaml
│   │
│   ├── reproducibility/
│   │   ├── deployment.yaml
│   │   ├── service.yaml
│   │   └── hpa.yaml
│   │
│   ├── integration/
│   │   ├── deployment.yaml
│   │   ├── service.yaml
│   │   └── hpa.yaml
│   │
│   ├── ingress.yaml
│   ├── networkpolicy.yaml
│   └── podsecuritypolicy.yaml
│
└── charts/
    ├── postgresql/
    ├── clickhouse/
    ├── kafka/
    └── redis/
```

### 3.3 Resource Specifications

```yaml
# values-prod.yaml
global:
  environment: production
  imageRegistry: registry.example.com
  imagePullSecrets:
    - name: regcred

experimentService:
  replicaCount: 3
  image:
    repository: llm-research-lab/experiment-service
    tag: "1.0.0"
  resources:
    requests:
      cpu: 500m
      memory: 512Mi
    limits:
      cpu: 2000m
      memory: 2Gi
  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 10
    targetCPUUtilizationPercentage: 70
    targetMemoryUtilizationPercentage: 80
  podDisruptionBudget:
    minAvailable: 1
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                app: experiment-service
            topologyKey: kubernetes.io/hostname
  topologySpreadConstraints:
    - maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
      labelSelector:
        matchLabels:
          app: experiment-service

metricService:
  replicaCount: 3
  resources:
    requests:
      cpu: 1000m
      memory: 1Gi
    limits:
      cpu: 4000m
      memory: 8Gi
  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 20
    # Custom metric for benchmark queue depth
    metrics:
      - type: External
        external:
          metric:
            name: benchmark_queue_depth
          target:
            type: AverageValue
            averageValue: 10

datasetService:
  replicaCount: 3
  resources:
    requests:
      cpu: 500m
      memory: 1Gi
    limits:
      cpu: 2000m
      memory: 4Gi
  persistence:
    enabled: true
    size: 100Gi
    storageClass: fast-ssd

workflowService:
  replicaCount: 2
  resources:
    requests:
      cpu: 500m
      memory: 512Mi
    limits:
      cpu: 2000m
      memory: 2Gi

postgresql:
  architecture: replication
  primary:
    persistence:
      size: 500Gi
      storageClass: fast-ssd
    resources:
      requests:
        cpu: 2000m
        memory: 4Gi
      limits:
        cpu: 4000m
        memory: 8Gi
  readReplicas:
    replicaCount: 2
    persistence:
      size: 500Gi

clickhouse:
  shards: 3
  replicaCount: 2
  persistence:
    size: 1Ti
    storageClass: fast-ssd
  resources:
    requests:
      cpu: 4000m
      memory: 16Gi
    limits:
      cpu: 8000m
      memory: 32Gi

kafka:
  replicaCount: 3
  persistence:
    size: 500Gi
  resources:
    requests:
      cpu: 2000m
      memory: 4Gi

redis:
  architecture: replication
  master:
    persistence:
      size: 50Gi
  replica:
    replicaCount: 2
```

### 3.4 Network Policies

```yaml
# NetworkPolicy for experiment-service
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: experiment-service-network-policy
  namespace: research-lab
spec:
  podSelector:
    matchLabels:
      app: experiment-service
  policyTypes:
    - Ingress
    - Egress
  ingress:
    # Allow from ingress controller
    - from:
        - namespaceSelector:
            matchLabels:
              name: ingress
          podSelector:
            matchLabels:
              app: ingress-nginx
      ports:
        - protocol: TCP
          port: 8001
        - protocol: TCP
          port: 9001
    # Allow from other services in namespace
    - from:
        - podSelector:
            matchLabels:
              app.kubernetes.io/part-of: research-lab
      ports:
        - protocol: TCP
          port: 9001
    # Allow Prometheus scraping
    - from:
        - namespaceSelector:
            matchLabels:
              name: observability
          podSelector:
            matchLabels:
              app: prometheus
      ports:
        - protocol: TCP
          port: 9090
  egress:
    # Allow to PostgreSQL
    - to:
        - namespaceSelector:
            matchLabels:
              name: data
          podSelector:
            matchLabels:
              app: postgresql
      ports:
        - protocol: TCP
          port: 5432
    # Allow to Redis
    - to:
        - namespaceSelector:
            matchLabels:
              name: data
          podSelector:
            matchLabels:
              app: redis
      ports:
        - protocol: TCP
          port: 6379
    # Allow to Kafka
    - to:
        - namespaceSelector:
            matchLabels:
              name: data
          podSelector:
            matchLabels:
              app: kafka
      ports:
        - protocol: TCP
          port: 9092
    # Allow to S3 (external)
    - to:
        - ipBlock:
            cidr: 0.0.0.0/0
      ports:
        - protocol: TCP
          port: 443
    # Allow DNS
    - to:
        - namespaceSelector: {}
          podSelector:
            matchLabels:
              k8s-app: kube-dns
      ports:
        - protocol: UDP
          port: 53
```

---

## 4. Data Architecture

### 4.1 Database Schema Design

#### 4.1.1 PostgreSQL Schema

```sql
-- Schema: research_lab

-- Enable required extensions
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pgcrypto";
CREATE EXTENSION IF NOT EXISTS "pg_trgm";

-- ============================================
-- EXPERIMENTS DOMAIN
-- ============================================

CREATE TABLE experiments (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    name VARCHAR(256) NOT NULL,
    description TEXT,
    hypothesis TEXT,
    owner_id UUID NOT NULL,
    status VARCHAR(32) NOT NULL DEFAULT 'draft',
    config JSONB NOT NULL DEFAULT '{}',
    metadata JSONB NOT NULL DEFAULT '{}',
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    archived_at TIMESTAMPTZ,

    CONSTRAINT experiments_name_check CHECK (char_length(name) >= 1),
    CONSTRAINT experiments_status_check CHECK (
        status IN ('draft', 'active', 'paused', 'completed', 'archived', 'failed')
    )
);

CREATE INDEX idx_experiments_owner_id ON experiments(owner_id);
CREATE INDEX idx_experiments_status ON experiments(status);
CREATE INDEX idx_experiments_created_at ON experiments(created_at DESC);
CREATE INDEX idx_experiments_name_trgm ON experiments USING gin(name gin_trgm_ops);
CREATE INDEX idx_experiments_metadata ON experiments USING gin(metadata jsonb_path_ops);

CREATE TABLE experiment_tags (
    experiment_id UUID NOT NULL REFERENCES experiments(id) ON DELETE CASCADE,
    tag VARCHAR(128) NOT NULL,
    PRIMARY KEY (experiment_id, tag)
);

CREATE INDEX idx_experiment_tags_tag ON experiment_tags(tag);

CREATE TABLE experiment_collaborators (
    experiment_id UUID NOT NULL REFERENCES experiments(id) ON DELETE CASCADE,
    user_id UUID NOT NULL,
    role VARCHAR(32) NOT NULL DEFAULT 'viewer',
    added_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    PRIMARY KEY (experiment_id, user_id),

    CONSTRAINT collaborators_role_check CHECK (role IN ('viewer', 'editor', 'admin'))
);

CREATE TABLE experiment_runs (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    experiment_id UUID NOT NULL REFERENCES experiments(id) ON DELETE CASCADE,
    run_number BIGINT NOT NULL,
    name VARCHAR(256),
    status VARCHAR(32) NOT NULL DEFAULT 'pending',
    parameters JSONB NOT NULL DEFAULT '{}',
    environment JSONB NOT NULL DEFAULT '{}',
    parent_run_id UUID REFERENCES experiment_runs(id),
    started_at TIMESTAMPTZ,
    ended_at TIMESTAMPTZ,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    created_by UUID NOT NULL,
    error JSONB,

    CONSTRAINT runs_status_check CHECK (
        status IN ('pending', 'queued', 'running', 'completed', 'failed', 'cancelled', 'timed_out')
    ),
    UNIQUE (experiment_id, run_number)
);

CREATE INDEX idx_runs_experiment_id ON experiment_runs(experiment_id);
CREATE INDEX idx_runs_status ON experiment_runs(status);
CREATE INDEX idx_runs_created_at ON experiment_runs(created_at DESC);
CREATE INDEX idx_runs_parent_run_id ON experiment_runs(parent_run_id);

CREATE TABLE run_tags (
    run_id UUID NOT NULL REFERENCES experiment_runs(id) ON DELETE CASCADE,
    tag VARCHAR(128) NOT NULL,
    PRIMARY KEY (run_id, tag)
);

CREATE TABLE run_artifacts (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    run_id UUID NOT NULL REFERENCES experiment_runs(id) ON DELETE CASCADE,
    name VARCHAR(512) NOT NULL,
    artifact_type VARCHAR(64) NOT NULL,
    content_hash VARCHAR(128) NOT NULL,
    size_bytes BIGINT NOT NULL,
    storage_uri TEXT NOT NULL,
    metadata JSONB NOT NULL DEFAULT '{}',
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_artifacts_run_id ON run_artifacts(run_id);
CREATE INDEX idx_artifacts_content_hash ON run_artifacts(content_hash);
CREATE INDEX idx_artifacts_type ON run_artifacts(artifact_type);

-- ============================================
-- METRICS DOMAIN
-- ============================================

CREATE TABLE metric_definitions (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    name VARCHAR(256) NOT NULL,
    description TEXT,
    version_major INTEGER NOT NULL,
    version_minor INTEGER NOT NULL,
    version_patch INTEGER NOT NULL,
    metric_type VARCHAR(64) NOT NULL,
    properties JSONB NOT NULL DEFAULT '{}',
    parameters_schema JSONB,
    created_by UUID NOT NULL,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    deprecated BOOLEAN NOT NULL DEFAULT FALSE,
    deprecation_message TEXT,

    UNIQUE (name, version_major, version_minor, version_patch)
);

CREATE INDEX idx_metrics_name ON metric_definitions(name);
CREATE INDEX idx_metrics_type ON metric_definitions(metric_type);
CREATE INDEX idx_metrics_deprecated ON metric_definitions(deprecated);

CREATE TABLE benchmark_suites (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    name VARCHAR(256) NOT NULL,
    description TEXT,
    metrics JSONB NOT NULL DEFAULT '[]',
    test_cases JSONB NOT NULL DEFAULT '[]',
    aggregation_config JSONB NOT NULL DEFAULT '{}',
    created_by UUID NOT NULL,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE TABLE benchmark_jobs (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    suite_id UUID NOT NULL REFERENCES benchmark_suites(id),
    run_id UUID REFERENCES experiment_runs(id),
    model_id VARCHAR(256) NOT NULL,
    model_config JSONB NOT NULL DEFAULT '{}',
    status VARCHAR(32) NOT NULL DEFAULT 'pending',
    progress DECIMAL(5, 2) NOT NULL DEFAULT 0,
    results JSONB,
    started_at TIMESTAMPTZ,
    completed_at TIMESTAMPTZ,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    error JSONB,

    CONSTRAINT benchmark_status_check CHECK (
        status IN ('pending', 'running', 'completed', 'failed', 'cancelled')
    )
);

CREATE INDEX idx_benchmark_jobs_suite ON benchmark_jobs(suite_id);
CREATE INDEX idx_benchmark_jobs_status ON benchmark_jobs(status);
CREATE INDEX idx_benchmark_jobs_model ON benchmark_jobs(model_id);

-- ============================================
-- DATASETS DOMAIN
-- ============================================

CREATE TABLE datasets (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    name VARCHAR(256) NOT NULL,
    description TEXT,
    schema JSONB NOT NULL,
    governance JSONB NOT NULL DEFAULT '{}',
    owner_id UUID NOT NULL,
    metadata JSONB NOT NULL DEFAULT '{}',
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_datasets_owner ON datasets(owner_id);
CREATE INDEX idx_datasets_name_trgm ON datasets USING gin(name gin_trgm_ops);

CREATE TABLE dataset_tags (
    dataset_id UUID NOT NULL REFERENCES datasets(id) ON DELETE CASCADE,
    tag VARCHAR(128) NOT NULL,
    PRIMARY KEY (dataset_id, tag)
);

CREATE TABLE dataset_versions (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    dataset_id UUID NOT NULL REFERENCES datasets(id) ON DELETE CASCADE,
    version_number BIGINT NOT NULL,
    content_hash VARCHAR(128) NOT NULL,
    parent_version_id UUID REFERENCES dataset_versions(id),
    transform JSONB,
    statistics JSONB NOT NULL DEFAULT '{}',
    manifest JSONB NOT NULL,
    message TEXT,
    created_by UUID NOT NULL,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),

    UNIQUE (dataset_id, version_number)
);

CREATE INDEX idx_dataset_versions_dataset ON dataset_versions(dataset_id);
CREATE INDEX idx_dataset_versions_hash ON dataset_versions(content_hash);
CREATE INDEX idx_dataset_versions_parent ON dataset_versions(parent_version_id);

CREATE TABLE dataset_version_tags (
    version_id UUID NOT NULL REFERENCES dataset_versions(id) ON DELETE CASCADE,
    tag VARCHAR(128) NOT NULL,
    PRIMARY KEY (version_id, tag)
);

-- ============================================
-- WORKFLOWS DOMAIN
-- ============================================

CREATE TABLE workflows (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    name VARCHAR(256) NOT NULL,
    description TEXT,
    version_major INTEGER NOT NULL,
    version_minor INTEGER NOT NULL,
    version_patch INTEGER NOT NULL,
    definition JSONB NOT NULL,
    parameters JSONB NOT NULL DEFAULT '[]',
    triggers JSONB NOT NULL DEFAULT '[]',
    created_by UUID NOT NULL,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),

    UNIQUE (name, version_major, version_minor, version_patch)
);

CREATE INDEX idx_workflows_name ON workflows(name);

CREATE TABLE workflow_runs (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    workflow_id UUID NOT NULL REFERENCES workflows(id),
    workflow_version VARCHAR(32) NOT NULL,
    status VARCHAR(32) NOT NULL DEFAULT 'pending',
    parameters JSONB NOT NULL DEFAULT '{}',
    step_states JSONB NOT NULL DEFAULT '{}',
    outputs JSONB NOT NULL DEFAULT '{}',
    started_at TIMESTAMPTZ,
    ended_at TIMESTAMPTZ,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    created_by UUID NOT NULL,
    error JSONB,

    CONSTRAINT workflow_run_status_check CHECK (
        status IN ('pending', 'running', 'paused', 'completed', 'failed', 'cancelled')
    )
);

CREATE INDEX idx_workflow_runs_workflow ON workflow_runs(workflow_id);
CREATE INDEX idx_workflow_runs_status ON workflow_runs(status);

CREATE TABLE workflow_checkpoints (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    run_id UUID NOT NULL REFERENCES workflow_runs(id) ON DELETE CASCADE,
    state JSONB NOT NULL,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_checkpoints_run ON workflow_checkpoints(run_id);
CREATE INDEX idx_checkpoints_created ON workflow_checkpoints(created_at DESC);

-- ============================================
-- REPRODUCIBILITY DOMAIN
-- ============================================

CREATE TABLE experiment_states (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    experiment_id UUID NOT NULL REFERENCES experiments(id),
    run_id UUID NOT NULL REFERENCES experiment_runs(id),
    environment JSONB NOT NULL,
    code_state JSONB NOT NULL,
    data_state JSONB NOT NULL,
    configuration JSONB NOT NULL,
    random_state JSONB NOT NULL DEFAULT '{}',
    checksum VARCHAR(128) NOT NULL,
    captured_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_states_experiment ON experiment_states(experiment_id);
CREATE INDEX idx_states_run ON experiment_states(run_id);

CREATE TABLE reproducibility_certificates (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    state_id UUID NOT NULL REFERENCES experiment_states(id),
    experiment_id UUID NOT NULL REFERENCES experiments(id),
    run_id UUID NOT NULL REFERENCES experiment_runs(id),
    validation_report JSONB NOT NULL,
    environment_hash VARCHAR(128) NOT NULL,
    code_hash VARCHAR(128) NOT NULL,
    data_hash VARCHAR(128) NOT NULL,
    configuration_hash VARCHAR(128) NOT NULL,
    results_hash VARCHAR(128) NOT NULL,
    signature TEXT NOT NULL,
    issued_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_certificates_experiment ON reproducibility_certificates(experiment_id);
CREATE INDEX idx_certificates_run ON reproducibility_certificates(run_id);

-- ============================================
-- LINEAGE DOMAIN
-- ============================================

CREATE TABLE lineage_nodes (
    id VARCHAR(256) PRIMARY KEY,
    node_type VARCHAR(64) NOT NULL,
    name VARCHAR(512),
    version VARCHAR(64),
    metadata JSONB NOT NULL DEFAULT '{}',
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_lineage_nodes_type ON lineage_nodes(node_type);

CREATE TABLE lineage_edges (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    source_id VARCHAR(256) NOT NULL REFERENCES lineage_nodes(id) ON DELETE CASCADE,
    target_id VARCHAR(256) NOT NULL REFERENCES lineage_nodes(id) ON DELETE CASCADE,
    edge_type VARCHAR(64) NOT NULL,
    metadata JSONB NOT NULL DEFAULT '{}',
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),

    UNIQUE (source_id, target_id, edge_type)
);

CREATE INDEX idx_lineage_edges_source ON lineage_edges(source_id);
CREATE INDEX idx_lineage_edges_target ON lineage_edges(target_id);
CREATE INDEX idx_lineage_edges_type ON lineage_edges(edge_type);

-- ============================================
-- AUDIT LOG
-- ============================================

CREATE TABLE audit_log (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    user_id UUID,
    action VARCHAR(64) NOT NULL,
    resource_type VARCHAR(64) NOT NULL,
    resource_id UUID,
    old_value JSONB,
    new_value JSONB,
    ip_address INET,
    user_agent TEXT,
    request_id UUID
);

CREATE INDEX idx_audit_timestamp ON audit_log(timestamp DESC);
CREATE INDEX idx_audit_user ON audit_log(user_id);
CREATE INDEX idx_audit_resource ON audit_log(resource_type, resource_id);

-- Partitioning for audit log (by month)
-- In production, set up automated partition management

-- ============================================
-- FUNCTIONS & TRIGGERS
-- ============================================

-- Auto-update updated_at timestamp
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ language 'plpgsql';

CREATE TRIGGER update_experiments_updated_at
    BEFORE UPDATE ON experiments
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_datasets_updated_at
    BEFORE UPDATE ON datasets
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_workflows_updated_at
    BEFORE UPDATE ON workflows
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_benchmark_suites_updated_at
    BEFORE UPDATE ON benchmark_suites
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

-- Auto-increment run number
CREATE OR REPLACE FUNCTION get_next_run_number(exp_id UUID)
RETURNS BIGINT AS $$
DECLARE
    next_num BIGINT;
BEGIN
    SELECT COALESCE(MAX(run_number), 0) + 1 INTO next_num
    FROM experiment_runs
    WHERE experiment_id = exp_id;
    RETURN next_num;
END;
$$ LANGUAGE plpgsql;

-- Auto-increment dataset version number
CREATE OR REPLACE FUNCTION get_next_version_number(ds_id UUID)
RETURNS BIGINT AS $$
DECLARE
    next_num BIGINT;
BEGIN
    SELECT COALESCE(MAX(version_number), 0) + 1 INTO next_num
    FROM dataset_versions
    WHERE dataset_id = ds_id;
    RETURN next_num;
END;
$$ LANGUAGE plpgsql;
```

#### 4.1.2 ClickHouse Schema (Time-Series Metrics)

```sql
-- Database for time-series metrics
CREATE DATABASE IF NOT EXISTS research_lab;

-- Run metrics table (main time-series data)
CREATE TABLE research_lab.run_metrics
(
    run_id UUID,
    metric_name LowCardinality(String),
    value Float64,
    step UInt64,
    timestamp DateTime64(3),
    context Map(String, String),

    INDEX idx_metric_name metric_name TYPE bloom_filter GRANULARITY 4,
    INDEX idx_step step TYPE minmax GRANULARITY 4
)
ENGINE = MergeTree()
PARTITION BY toYYYYMM(timestamp)
ORDER BY (run_id, metric_name, timestamp)
TTL timestamp + INTERVAL 2 YEAR
SETTINGS index_granularity = 8192;

-- Materialized view for metric aggregations per run
CREATE MATERIALIZED VIEW research_lab.run_metrics_agg
ENGINE = SummingMergeTree()
PARTITION BY toYYYYMM(timestamp)
ORDER BY (run_id, metric_name)
AS SELECT
    run_id,
    metric_name,
    min(timestamp) as first_timestamp,
    max(timestamp) as last_timestamp,
    count() as sample_count,
    min(value) as min_value,
    max(value) as max_value,
    sum(value) as sum_value,
    avg(value) as avg_value
FROM research_lab.run_metrics
GROUP BY run_id, metric_name;

-- Benchmark results table
CREATE TABLE research_lab.benchmark_results
(
    job_id UUID,
    test_case_id UUID,
    metric_name LowCardinality(String),
    value Float64,
    model_id LowCardinality(String),
    timestamp DateTime64(3),
    latency_ms UInt64,
    tokens_used UInt32,
    details String, -- JSON

    INDEX idx_model model_id TYPE bloom_filter GRANULARITY 4
)
ENGINE = MergeTree()
PARTITION BY toYYYYMM(timestamp)
ORDER BY (job_id, metric_name, timestamp)
TTL timestamp + INTERVAL 1 YEAR;

-- System metrics table
CREATE TABLE research_lab.system_metrics
(
    service LowCardinality(String),
    instance String,
    metric_name LowCardinality(String),
    value Float64,
    timestamp DateTime64(3),
    labels Map(String, String)
)
ENGINE = MergeTree()
PARTITION BY toYYYYMM(timestamp)
ORDER BY (service, metric_name, timestamp)
TTL timestamp + INTERVAL 90 DAY;

-- Query patterns table for analytics
CREATE TABLE research_lab.query_patterns
(
    user_id UUID,
    query_type LowCardinality(String),
    resource_type LowCardinality(String),
    resource_id UUID,
    duration_ms UInt32,
    timestamp DateTime64(3)
)
ENGINE = MergeTree()
PARTITION BY toYYYYMM(timestamp)
ORDER BY (query_type, timestamp)
TTL timestamp + INTERVAL 180 DAY;
```

### 4.2 Data Flow Architecture

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                              Data Flow Architecture                              │
└─────────────────────────────────────────────────────────────────────────────────┘

                              ┌───────────────────┐
                              │   API Gateway     │
                              └─────────┬─────────┘
                                        │
              ┌─────────────────────────┼─────────────────────────┐
              │                         │                         │
              ▼                         ▼                         ▼
     ┌────────────────┐       ┌────────────────┐       ┌────────────────┐
     │  Write Path    │       │  Read Path     │       │ Stream Path    │
     └───────┬────────┘       └───────┬────────┘       └───────┬────────┘
             │                        │                        │
             ▼                        ▼                        ▼
     ┌────────────────┐       ┌────────────────┐       ┌────────────────┐
     │   Services     │       │     Redis      │       │     Kafka      │
     │ (Validation)   │       │    (Cache)     │       │   (Events)     │
     └───────┬────────┘       └───────┬────────┘       └───────┬────────┘
             │                        │                        │
             ▼                        │                        ▼
     ┌────────────────┐               │                ┌────────────────┐
     │   PostgreSQL   │◄──────────────┘                │  Consumers     │
     │   (Primary)    │                                │  (Workers)     │
     └───────┬────────┘                                └───────┬────────┘
             │                                                 │
             ├──────────────────────┐                         │
             │                      │                         │
             ▼                      ▼                         ▼
     ┌────────────────┐    ┌────────────────┐         ┌────────────────┐
     │   PostgreSQL   │    │    S3/MinIO    │         │   ClickHouse   │
     │   (Replicas)   │    │  (Artifacts)   │         │ (Time-Series)  │
     └────────────────┘    └────────────────┘         └────────────────┘


Write Path:
1. Request → API Gateway → Service
2. Service validates and transforms
3. Write to PostgreSQL (primary)
4. Publish event to Kafka
5. Invalidate Redis cache
6. Store artifacts to S3

Read Path:
1. Request → API Gateway → Service
2. Check Redis cache
3. Cache miss → Query PostgreSQL replica
4. Populate cache
5. Return response

Stream Path:
1. Event published to Kafka
2. Consumers process asynchronously
3. Update ClickHouse for analytics
4. Trigger notifications/webhooks
5. Update search indices
```

### 4.3 Caching Strategy

```yaml
caching:
  layers:
    # L1: In-process cache (per pod)
    l1:
      type: in-memory
      implementation: moka
      max_size: 100MB
      ttl: 60s

    # L2: Distributed cache
    l2:
      type: redis
      cluster: true
      max_memory: 8GB
      eviction_policy: allkeys-lru

  patterns:
    # Experiment metadata
    experiments:
      key_pattern: "exp:{id}"
      ttl: 300s
      invalidation: write-through

    # Run data
    runs:
      key_pattern: "run:{id}"
      ttl: 180s
      invalidation: write-through

    # Metric definitions
    metrics:
      key_pattern: "metric:{name}:{version}"
      ttl: 3600s
      invalidation: on-update

    # Dataset metadata
    datasets:
      key_pattern: "ds:{id}"
      ttl: 600s
      invalidation: write-through

    # User sessions
    sessions:
      key_pattern: "session:{token}"
      ttl: 3600s
      sliding: true

    # Rate limiting
    rate_limits:
      key_pattern: "rl:{user_id}:{endpoint}"
      ttl: 60s
      type: sliding_window

  invalidation:
    strategies:
      - type: event-driven
        source: kafka
        topic: cache-invalidation

      - type: ttl
        check_interval: 10s

      - type: write-through
        sync: true
```

---

## 5. Storage Architecture

### 5.1 Object Storage Layout

```
s3://research-lab-artifacts/
├── experiments/
│   └── {experiment_id}/
│       └── runs/
│           └── {run_id}/
│               ├── artifacts/
│               │   ├── {artifact_id}/
│               │   │   ├── data              # Actual artifact data
│               │   │   └── metadata.json     # Artifact metadata
│               │   └── manifest.json         # Artifacts manifest
│               ├── logs/
│               │   ├── stdout.log
│               │   ├── stderr.log
│               │   └── metrics.log
│               ├── checkpoints/
│               │   └── {checkpoint_id}.json
│               └── environment.json          # Environment snapshot
│
├── datasets/
│   └── {dataset_id}/
│       └── versions/
│           └── {version_id}/
│               ├── chunks/
│               │   ├── chunk_000000/
│               │   │   └── data.parquet
│               │   ├── chunk_000001/
│               │   │   └── data.parquet
│               │   └── ...
│               ├── manifest.json
│               └── statistics.json
│
├── workflows/
│   └── {workflow_id}/
│       └── runs/
│           └── {run_id}/
│               ├── checkpoints/
│               │   └── {step_id}/
│               │       └── state.json
│               └── logs/
│                   └── {step_id}.log
│
├── reproducibility/
│   └── states/
│       └── {state_id}/
│           ├── environment.json
│           ├── code.tar.gz              # Code snapshot
│           ├── patches/
│           │   └── *.patch
│           └── certificates/
│               └── {certificate_id}.json
│
└── temp/
    └── uploads/
        └── {upload_id}/                  # Temporary upload staging
```

### 5.2 Storage Policies

```yaml
storage_policies:
  artifacts:
    storage_class: STANDARD
    encryption: AES-256
    versioning: enabled
    lifecycle:
      - rule: transition_to_ia
        days: 90
        storage_class: STANDARD_IA
      - rule: transition_to_glacier
        days: 365
        storage_class: GLACIER
      - rule: expire
        days: 2555  # 7 years

  datasets:
    storage_class: STANDARD
    encryption: AES-256
    versioning: enabled
    replication:
      enabled: true
      destination: s3://research-lab-artifacts-replica/
    lifecycle:
      - rule: transition_to_ia
        days: 180
        storage_class: STANDARD_IA

  logs:
    storage_class: STANDARD_IA
    encryption: AES-256
    versioning: disabled
    lifecycle:
      - rule: expire
        days: 90

  temp:
    storage_class: STANDARD
    encryption: AES-256
    versioning: disabled
    lifecycle:
      - rule: expire
        days: 1

  checkpoints:
    storage_class: STANDARD
    encryption: AES-256
    versioning: enabled
    lifecycle:
      - rule: transition_to_ia
        days: 30
      - rule: expire
        days: 365
```

### 5.3 Content-Addressable Storage

```rust
/// Content-addressable storage implementation
pub struct ContentAddressableStore {
    storage: Arc<dyn ObjectStorage>,
    index: Arc<ContentIndex>,
    deduplication: bool,
    compression: CompressionType,
}

impl ContentAddressableStore {
    /// Store content and return content hash
    pub async fn store(&self, data: &[u8]) -> Result<ContentReference, StorageError> {
        // 1. Compute content hash
        let hash = ContentHash::from_bytes(data);

        // 2. Check if content already exists (deduplication)
        if self.deduplication {
            if let Some(reference) = self.index.get(&hash).await? {
                // Increment reference count
                self.index.increment_refs(&hash).await?;
                return Ok(reference);
            }
        }

        // 3. Compress content
        let compressed = self.compress(data)?;

        // 4. Generate storage path
        let path = self.generate_path(&hash);

        // 5. Store to object storage
        self.storage.put(&path, &compressed).await?;

        // 6. Update index
        let reference = ContentReference {
            hash: hash.clone(),
            path,
            size_bytes: data.len() as u64,
            compressed_size: compressed.len() as u64,
            compression: self.compression,
            stored_at: Utc::now(),
        };

        self.index.put(&hash, &reference).await?;

        Ok(reference)
    }

    /// Retrieve content by hash
    pub async fn retrieve(&self, hash: &ContentHash) -> Result<Vec<u8>, StorageError> {
        // 1. Look up in index
        let reference = self.index.get(hash).await?
            .ok_or(StorageError::NotFound)?;

        // 2. Retrieve from object storage
        let compressed = self.storage.get(&reference.path).await?;

        // 3. Decompress
        let data = self.decompress(&compressed, reference.compression)?;

        // 4. Verify hash
        let computed_hash = ContentHash::from_bytes(&data);
        if computed_hash != *hash {
            return Err(StorageError::IntegrityError);
        }

        Ok(data)
    }

    /// Generate hierarchical path from hash
    fn generate_path(&self, hash: &ContentHash) -> String {
        let hex = hash.as_str();
        format!("cas/{}/{}/{}", &hex[0..2], &hex[2..4], hex)
    }
}
```

---

## Document Metadata

| Field | Value |
|-------|-------|
| **Version** | 1.0.0 |
| **Status** | Draft |
| **SPARC Phase** | Architecture (Part 1 of 2) |
| **Created** | 2025-11-28 |
| **Ecosystem** | LLM DevOps |
| **Next Part** | Architecture Part 2: APIs, Security & Observability |

---

*This architecture document is part of the SPARC methodology. Part 2 covers API Design, Security Architecture, Observability, and Operational Procedures.*
---

## 6. API Architecture

### 6.1 API Gateway Configuration

```yaml
# Kong Gateway Configuration
apiVersion: configuration.konghq.com/v1
kind: KongPlugin
metadata:
  name: rate-limiting
config:
  minute: 1000
  hour: 10000
  policy: redis
  redis_host: redis.data.svc.cluster.local
  redis_port: 6379
---
apiVersion: configuration.konghq.com/v1
kind: KongPlugin
metadata:
  name: jwt-auth
config:
  key_claim_name: kid
  claims_to_verify:
    - exp
    - nbf
  run_on_preflight: false
---
apiVersion: configuration.konghq.com/v1
kind: KongPlugin
metadata:
  name: cors
config:
  origins:
    - "*"
  methods:
    - GET
    - POST
    - PUT
    - PATCH
    - DELETE
    - OPTIONS
  headers:
    - Authorization
    - Content-Type
    - X-Request-ID
    - X-Correlation-ID
  exposed_headers:
    - X-Request-ID
    - X-Correlation-ID
    - X-RateLimit-Remaining
  credentials: true
  max_age: 3600
---
apiVersion: configuration.konghq.com/v1
kind: KongPlugin
metadata:
  name: request-transformer
config:
  add:
    headers:
      - "X-Gateway-Version:1.0"
      - "X-Forwarded-Service:research-lab"
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: research-lab-api
  annotations:
    konghq.com/plugins: rate-limiting,jwt-auth,cors,request-transformer
    konghq.com/protocols: https
    konghq.com/https-redirect-status-code: "308"
spec:
  ingressClassName: kong
  tls:
    - hosts:
        - api.research-lab.example.com
      secretName: research-lab-tls
  rules:
    - host: api.research-lab.example.com
      http:
        paths:
          - path: /api/v1/experiments
            pathType: Prefix
            backend:
              service:
                name: experiment-service
                port:
                  number: 8001
          - path: /api/v1/runs
            pathType: Prefix
            backend:
              service:
                name: experiment-service
                port:
                  number: 8001
          - path: /api/v1/metrics
            pathType: Prefix
            backend:
              service:
                name: metric-service
                port:
                  number: 8002
          - path: /api/v1/benchmarks
            pathType: Prefix
            backend:
              service:
                name: metric-service
                port:
                  number: 8002
          - path: /api/v1/datasets
            pathType: Prefix
            backend:
              service:
                name: dataset-service
                port:
                  number: 8003
          - path: /api/v1/workflows
            pathType: Prefix
            backend:
              service:
                name: workflow-service
                port:
                  number: 8004
          - path: /api/v1/states
            pathType: Prefix
            backend:
              service:
                name: reproducibility-service
                port:
                  number: 8005
          - path: /api/v1/certificates
            pathType: Prefix
            backend:
              service:
                name: reproducibility-service
                port:
                  number: 8005
```

### 6.2 OpenAPI Specification

```yaml
openapi: 3.1.0
info:
  title: LLM Research Lab API
  description: |
    API for the LLM Research Lab - experimental evaluation and
    reproducible research workflows for Large Language Models.
  version: 1.0.0
  contact:
    name: LLM DevOps Team
    email: support@llm-devops.io
  license:
    name: Apache 2.0
    url: https://www.apache.org/licenses/LICENSE-2.0

servers:
  - url: https://api.research-lab.example.com/api/v1
    description: Production
  - url: https://api.staging.research-lab.example.com/api/v1
    description: Staging

security:
  - bearerAuth: []

tags:
  - name: experiments
    description: Experiment management
  - name: runs
    description: Experiment run management
  - name: metrics
    description: Metric definitions and benchmarking
  - name: datasets
    description: Dataset versioning and management
  - name: workflows
    description: Workflow orchestration
  - name: reproducibility
    description: Reproducibility and state management

paths:
  /experiments:
    post:
      summary: Create experiment
      operationId: createExperiment
      tags: [experiments]
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CreateExperimentRequest'
      responses:
        '201':
          description: Experiment created
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Experiment'
          headers:
            Location:
              description: URL of created experiment
              schema:
                type: string
        '400':
          $ref: '#/components/responses/BadRequest'
        '401':
          $ref: '#/components/responses/Unauthorized'
        '422':
          $ref: '#/components/responses/UnprocessableEntity'

    get:
      summary: List experiments
      operationId: listExperiments
      tags: [experiments]
      parameters:
        - $ref: '#/components/parameters/PageOffset'
        - $ref: '#/components/parameters/PageLimit'
        - name: status
          in: query
          schema:
            type: array
            items:
              $ref: '#/components/schemas/ExperimentStatus'
        - name: tags
          in: query
          schema:
            type: array
            items:
              type: string
        - name: owner_id
          in: query
          schema:
            type: string
            format: uuid
        - name: sort
          in: query
          schema:
            type: string
            enum: [created_at, updated_at, name]
            default: created_at
        - name: order
          in: query
          schema:
            type: string
            enum: [asc, desc]
            default: desc
      responses:
        '200':
          description: List of experiments
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ExperimentList'

  /experiments/{experimentId}:
    parameters:
      - $ref: '#/components/parameters/ExperimentId'

    get:
      summary: Get experiment
      operationId: getExperiment
      tags: [experiments]
      responses:
        '200':
          description: Experiment details
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Experiment'
        '404':
          $ref: '#/components/responses/NotFound'

    put:
      summary: Update experiment
      operationId: updateExperiment
      tags: [experiments]
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/UpdateExperimentRequest'
      responses:
        '200':
          description: Experiment updated
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Experiment'
        '404':
          $ref: '#/components/responses/NotFound'
        '409':
          $ref: '#/components/responses/Conflict'

    delete:
      summary: Delete experiment
      operationId: deleteExperiment
      tags: [experiments]
      responses:
        '204':
          description: Experiment deleted
        '404':
          $ref: '#/components/responses/NotFound'
        '409':
          description: Cannot delete - has active runs
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'

  /experiments/{experimentId}/runs:
    parameters:
      - $ref: '#/components/parameters/ExperimentId'

    post:
      summary: Start experiment run
      operationId: startRun
      tags: [runs]
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/StartRunRequest'
      responses:
        '201':
          description: Run started
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ExperimentRun'
        '400':
          $ref: '#/components/responses/BadRequest'
        '404':
          $ref: '#/components/responses/NotFound'
        '409':
          description: Experiment not in active state
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'

    get:
      summary: List runs for experiment
      operationId: listRuns
      tags: [runs]
      parameters:
        - $ref: '#/components/parameters/PageOffset'
        - $ref: '#/components/parameters/PageLimit'
        - name: status
          in: query
          schema:
            type: array
            items:
              $ref: '#/components/schemas/RunStatus'
      responses:
        '200':
          description: List of runs
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/RunList'

  /runs/{runId}:
    parameters:
      - $ref: '#/components/parameters/RunId'

    get:
      summary: Get run details
      operationId: getRun
      tags: [runs]
      responses:
        '200':
          description: Run details
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ExperimentRun'
        '404':
          $ref: '#/components/responses/NotFound'

  /runs/{runId}/metrics:
    parameters:
      - $ref: '#/components/parameters/RunId'

    post:
      summary: Log metrics
      operationId: logMetrics
      tags: [runs]
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/LogMetricsRequest'
      responses:
        '202':
          description: Metrics accepted
        '400':
          $ref: '#/components/responses/BadRequest'
        '404':
          $ref: '#/components/responses/NotFound'
        '409':
          description: Run not in running state
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'

    get:
      summary: Get run metrics
      operationId: getRunMetrics
      tags: [runs]
      parameters:
        - name: metric_names
          in: query
          schema:
            type: array
            items:
              type: string
        - name: start_time
          in: query
          schema:
            type: string
            format: date-time
        - name: end_time
          in: query
          schema:
            type: string
            format: date-time
      responses:
        '200':
          description: Run metrics
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/RunMetrics'

  /runs/{runId}/artifacts:
    parameters:
      - $ref: '#/components/parameters/RunId'

    post:
      summary: Upload artifact
      operationId: uploadArtifact
      tags: [runs]
      requestBody:
        required: true
        content:
          multipart/form-data:
            schema:
              type: object
              required: [name, artifact_type, file]
              properties:
                name:
                  type: string
                artifact_type:
                  $ref: '#/components/schemas/ArtifactType'
                file:
                  type: string
                  format: binary
                metadata:
                  type: string
                  description: JSON-encoded metadata
      responses:
        '201':
          description: Artifact uploaded
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ArtifactRef'
        '413':
          description: Artifact too large
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'

    get:
      summary: List artifacts
      operationId: listArtifacts
      tags: [runs]
      responses:
        '200':
          description: List of artifacts
          content:
            application/json:
              schema:
                type: array
                items:
                  $ref: '#/components/schemas/ArtifactRef'

  /runs/{runId}/status:
    parameters:
      - $ref: '#/components/parameters/RunId'

    put:
      summary: Update run status
      operationId: updateRunStatus
      tags: [runs]
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/UpdateRunStatusRequest'
      responses:
        '200':
          description: Status updated
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ExperimentRun'
        '409':
          description: Invalid state transition
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'

  /runs/compare:
    post:
      summary: Compare runs
      operationId: compareRuns
      tags: [runs]
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CompareRunsRequest'
      responses:
        '200':
          description: Comparison results
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/RunComparison'

  /benchmarks:
    post:
      summary: Submit benchmark
      operationId: submitBenchmark
      tags: [metrics]
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/BenchmarkSubmitRequest'
      responses:
        '202':
          description: Benchmark accepted
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/BenchmarkJob'

  /benchmarks/{jobId}:
    parameters:
      - name: jobId
        in: path
        required: true
        schema:
          type: string
          format: uuid

    get:
      summary: Get benchmark status
      operationId: getBenchmarkStatus
      tags: [metrics]
      responses:
        '200':
          description: Benchmark status
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/BenchmarkJob'

  /benchmarks/{jobId}/results:
    parameters:
      - name: jobId
        in: path
        required: true
        schema:
          type: string
          format: uuid

    get:
      summary: Get benchmark results
      operationId: getBenchmarkResults
      tags: [metrics]
      responses:
        '200':
          description: Benchmark results
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/BenchmarkResults'
        '202':
          description: Benchmark still running
        '404':
          $ref: '#/components/responses/NotFound'

  /datasets:
    post:
      summary: Register dataset
      operationId: registerDataset
      tags: [datasets]
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/RegisterDatasetRequest'
      responses:
        '201':
          description: Dataset registered
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Dataset'

    get:
      summary: List datasets
      operationId: listDatasets
      tags: [datasets]
      parameters:
        - $ref: '#/components/parameters/PageOffset'
        - $ref: '#/components/parameters/PageLimit'
      responses:
        '200':
          description: List of datasets
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/DatasetList'

  /datasets/{datasetId}/versions:
    parameters:
      - name: datasetId
        in: path
        required: true
        schema:
          type: string
          format: uuid

    post:
      summary: Create dataset version
      operationId: createDatasetVersion
      tags: [datasets]
      requestBody:
        required: true
        content:
          multipart/form-data:
            schema:
              type: object
              required: [file]
              properties:
                file:
                  type: string
                  format: binary
                parent_version_id:
                  type: string
                  format: uuid
                message:
                  type: string
                tags:
                  type: array
                  items:
                    type: string
      responses:
        '201':
          description: Version created
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/DatasetVersion'

  /datasets/{datasetId}/versions/{versionId}/stream:
    parameters:
      - name: datasetId
        in: path
        required: true
        schema:
          type: string
          format: uuid
      - name: versionId
        in: path
        required: true
        schema:
          type: string
          format: uuid

    get:
      summary: Stream dataset
      operationId: streamDataset
      tags: [datasets]
      parameters:
        - name: columns
          in: query
          schema:
            type: array
            items:
              type: string
        - name: limit
          in: query
          schema:
            type: integer
        - name: offset
          in: query
          schema:
            type: integer
      responses:
        '200':
          description: Dataset stream
          content:
            application/x-ndjson:
              schema:
                type: object

  /workflows:
    post:
      summary: Create workflow
      operationId: createWorkflow
      tags: [workflows]
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CreateWorkflowRequest'
      responses:
        '201':
          description: Workflow created
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Workflow'

  /workflows/{workflowId}/runs:
    parameters:
      - name: workflowId
        in: path
        required: true
        schema:
          type: string
          format: uuid

    post:
      summary: Submit workflow run
      operationId: submitWorkflowRun
      tags: [workflows]
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/SubmitWorkflowRunRequest'
      responses:
        '202':
          description: Workflow run submitted
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/WorkflowRun'

  /workflow-runs/{runId}:
    parameters:
      - name: runId
        in: path
        required: true
        schema:
          type: string
          format: uuid

    get:
      summary: Get workflow run
      operationId: getWorkflowRun
      tags: [workflows]
      responses:
        '200':
          description: Workflow run details
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/WorkflowRun'

  /workflow-runs/{runId}/pause:
    parameters:
      - name: runId
        in: path
        required: true
        schema:
          type: string
          format: uuid

    post:
      summary: Pause workflow run
      operationId: pauseWorkflowRun
      tags: [workflows]
      responses:
        '200':
          description: Workflow paused
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/WorkflowRun'

  /workflow-runs/{runId}/resume:
    parameters:
      - name: runId
        in: path
        required: true
        schema:
          type: string
          format: uuid

    post:
      summary: Resume workflow run
      operationId: resumeWorkflowRun
      tags: [workflows]
      responses:
        '202':
          description: Workflow resumed
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/WorkflowRun'

  /workflow-runs/{runId}/cancel:
    parameters:
      - name: runId
        in: path
        required: true
        schema:
          type: string
          format: uuid

    post:
      summary: Cancel workflow run
      operationId: cancelWorkflowRun
      tags: [workflows]
      responses:
        '200':
          description: Workflow cancelled
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/WorkflowRun'

  /states:
    post:
      summary: Capture experiment state
      operationId: captureState
      tags: [reproducibility]
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CaptureStateRequest'
      responses:
        '201':
          description: State captured
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ExperimentState'

  /states/{stateId}/validate:
    parameters:
      - name: stateId
        in: path
        required: true
        schema:
          type: string
          format: uuid

    post:
      summary: Validate reproducibility
      operationId: validateReproducibility
      tags: [reproducibility]
      responses:
        '200':
          description: Validation report
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ValidationReport'

  /states/{stateId}/replay:
    parameters:
      - name: stateId
        in: path
        required: true
        schema:
          type: string
          format: uuid

    post:
      summary: Replay experiment
      operationId: replayExperiment
      tags: [reproducibility]
      responses:
        '200':
          description: Replay result
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ReplayResult'

  /certificates:
    post:
      summary: Generate reproducibility certificate
      operationId: generateCertificate
      tags: [reproducibility]
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/GenerateCertificateRequest'
      responses:
        '201':
          description: Certificate generated
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ReproducibilityCertificate'

components:
  securitySchemes:
    bearerAuth:
      type: http
      scheme: bearer
      bearerFormat: JWT

  parameters:
    ExperimentId:
      name: experimentId
      in: path
      required: true
      schema:
        type: string
        format: uuid

    RunId:
      name: runId
      in: path
      required: true
      schema:
        type: string
        format: uuid

    PageOffset:
      name: offset
      in: query
      schema:
        type: integer
        minimum: 0
        default: 0

    PageLimit:
      name: limit
      in: query
      schema:
        type: integer
        minimum: 1
        maximum: 100
        default: 20

  responses:
    BadRequest:
      description: Invalid request
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/Error'

    Unauthorized:
      description: Authentication required
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/Error'

    NotFound:
      description: Resource not found
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/Error'

    Conflict:
      description: Resource conflict
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/Error'

    UnprocessableEntity:
      description: Validation failed
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/ValidationError'

  schemas:
    Error:
      type: object
      required: [error]
      properties:
        error:
          type: object
          required: [code, message]
          properties:
            code:
              type: string
            message:
              type: string
            details:
              type: object
            request_id:
              type: string

    ValidationError:
      type: object
      required: [error]
      properties:
        error:
          type: object
          required: [code, message, fields]
          properties:
            code:
              type: string
              example: VALIDATION_ERROR
            message:
              type: string
            fields:
              type: array
              items:
                type: object
                properties:
                  field:
                    type: string
                  message:
                    type: string
                  code:
                    type: string

    ExperimentStatus:
      type: string
      enum: [draft, active, paused, completed, archived, failed]

    RunStatus:
      type: string
      enum: [pending, queued, running, completed, failed, cancelled, timed_out]

    ArtifactType:
      type: string
      enum: [model, checkpoint, dataset, visualization, log, config, report, custom]

    Experiment:
      type: object
      required: [id, name, owner_id, status, config, created_at, updated_at]
      properties:
        id:
          type: string
          format: uuid
        name:
          type: string
        description:
          type: string
        hypothesis:
          type: string
        owner_id:
          type: string
          format: uuid
        collaborators:
          type: array
          items:
            type: string
            format: uuid
        tags:
          type: array
          items:
            type: string
        status:
          $ref: '#/components/schemas/ExperimentStatus'
        config:
          type: object
        metadata:
          type: object
        created_at:
          type: string
          format: date-time
        updated_at:
          type: string
          format: date-time
        archived_at:
          type: string
          format: date-time

    ExperimentList:
      type: object
      required: [items, total_count, has_more]
      properties:
        items:
          type: array
          items:
            $ref: '#/components/schemas/Experiment'
        total_count:
          type: integer
        has_more:
          type: boolean
        next_offset:
          type: integer

    CreateExperimentRequest:
      type: object
      required: [name, config]
      properties:
        name:
          type: string
          minLength: 1
          maxLength: 256
        description:
          type: string
        hypothesis:
          type: string
        collaborators:
          type: array
          items:
            type: string
            format: uuid
        tags:
          type: array
          items:
            type: string
        config:
          type: object
        metadata:
          type: object

    UpdateExperimentRequest:
      type: object
      properties:
        name:
          type: string
        description:
          type: string
        hypothesis:
          type: string
        status:
          $ref: '#/components/schemas/ExperimentStatus'
        tags:
          type: array
          items:
            type: string
        config:
          type: object
        metadata:
          type: object

    ExperimentRun:
      type: object
      required: [id, experiment_id, run_number, status, parameters, created_at]
      properties:
        id:
          type: string
          format: uuid
        experiment_id:
          type: string
          format: uuid
        run_number:
          type: integer
        name:
          type: string
        status:
          $ref: '#/components/schemas/RunStatus'
        parameters:
          type: object
        environment:
          type: object
        metrics:
          type: object
        artifacts:
          type: array
          items:
            $ref: '#/components/schemas/ArtifactRef'
        parent_run_id:
          type: string
          format: uuid
        tags:
          type: array
          items:
            type: string
        started_at:
          type: string
          format: date-time
        ended_at:
          type: string
          format: date-time
        created_at:
          type: string
          format: date-time
        created_by:
          type: string
          format: uuid
        error:
          type: object

    RunList:
      type: object
      required: [items, total_count, has_more]
      properties:
        items:
          type: array
          items:
            $ref: '#/components/schemas/ExperimentRun'
        total_count:
          type: integer
        has_more:
          type: boolean

    StartRunRequest:
      type: object
      properties:
        name:
          type: string
        parameters:
          type: object
        tags:
          type: array
          items:
            type: string
        parent_run_id:
          type: string
          format: uuid

    LogMetricsRequest:
      type: object
      required: [metrics]
      properties:
        metrics:
          type: array
          items:
            type: object
            required: [name, value]
            properties:
              name:
                type: string
              value:
                type: number
              step:
                type: integer
              context:
                type: object

    UpdateRunStatusRequest:
      type: object
      required: [status]
      properties:
        status:
          $ref: '#/components/schemas/RunStatus'
        final_metrics:
          type: object
        error:
          type: object
          properties:
            error_type:
              type: string
            message:
              type: string
            stack_trace:
              type: string

    ArtifactRef:
      type: object
      required: [id, name, artifact_type, content_hash, size_bytes, storage_uri, created_at]
      properties:
        id:
          type: string
          format: uuid
        name:
          type: string
        artifact_type:
          $ref: '#/components/schemas/ArtifactType'
        content_hash:
          type: string
        size_bytes:
          type: integer
        storage_uri:
          type: string
        metadata:
          type: object
        created_at:
          type: string
          format: date-time

    RunMetrics:
      type: object
      properties:
        scalars:
          type: object
          additionalProperties:
            type: object
            properties:
              values:
                type: array
                items:
                  type: object
                  properties:
                    value:
                      type: number
                    step:
                      type: integer
                    timestamp:
                      type: string
                      format: date-time
              aggregations:
                type: object

    CompareRunsRequest:
      type: object
      required: [run_ids]
      properties:
        run_ids:
          type: array
          items:
            type: string
            format: uuid
          minItems: 2
        config:
          type: object
          properties:
            same_experiment_only:
              type: boolean
              default: true
            higher_is_better:
              type: boolean
              default: true
            compare_environments:
              type: boolean
              default: false
            run_statistical_tests:
              type: boolean
              default: true

    RunComparison:
      type: object
      properties:
        run_ids:
          type: array
          items:
            type: string
            format: uuid
        metric_comparisons:
          type: object
        parameter_diff:
          type: object
        environment_diff:
          type: object
        statistical_tests:
          type: object
        generated_at:
          type: string
          format: date-time

    BenchmarkSubmitRequest:
      type: object
      required: [model_id, suite_id]
      properties:
        model_id:
          type: string
        model_endpoint:
          type: string
        suite_id:
          type: string
          format: uuid
        parameters:
          type: object
        compare_to_baseline:
          type: boolean
          default: false
        callback_url:
          type: string
          format: uri

    BenchmarkJob:
      type: object
      properties:
        job_id:
          type: string
          format: uuid
        status:
          type: object
          properties:
            state:
              type: string
              enum: [pending, running, completed, failed, cancelled]
            progress:
              type: number
            current_test:
              type: string
            tests_completed:
              type: integer
            tests_total:
              type: integer
        submitted_at:
          type: string
          format: date-time

    BenchmarkResults:
      type: object
      properties:
        job_id:
          type: string
          format: uuid
        model_id:
          type: string
        suite_id:
          type: string
          format: uuid
        results:
          type: array
          items:
            type: object
        aggregated:
          type: object
        baseline_comparison:
          type: object
        completed_at:
          type: string
          format: date-time

    Dataset:
      type: object
      properties:
        id:
          type: string
          format: uuid
        name:
          type: string
        description:
          type: string
        schema:
          type: object
        governance:
          type: object
        owner_id:
          type: string
          format: uuid
        tags:
          type: array
          items:
            type: string
        created_at:
          type: string
          format: date-time
        updated_at:
          type: string
          format: date-time

    DatasetList:
      type: object
      properties:
        items:
          type: array
          items:
            $ref: '#/components/schemas/Dataset'
        total_count:
          type: integer
        has_more:
          type: boolean

    RegisterDatasetRequest:
      type: object
      required: [name, schema]
      properties:
        name:
          type: string
        description:
          type: string
        schema:
          type: object
        governance:
          type: object
        tags:
          type: array
          items:
            type: string

    DatasetVersion:
      type: object
      properties:
        id:
          type: string
          format: uuid
        dataset_id:
          type: string
          format: uuid
        version_number:
          type: integer
        content_hash:
          type: string
        parent_version_id:
          type: string
          format: uuid
        statistics:
          type: object
        created_at:
          type: string
          format: date-time

    Workflow:
      type: object
      properties:
        id:
          type: string
          format: uuid
        name:
          type: string
        description:
          type: string
        version:
          type: string
        steps:
          type: array
          items:
            type: object
        parameters:
          type: array
          items:
            type: object
        created_at:
          type: string
          format: date-time

    CreateWorkflowRequest:
      type: object
      required: [name, steps]
      properties:
        name:
          type: string
        description:
          type: string
        steps:
          type: array
          items:
            type: object
        parameters:
          type: array
          items:
            type: object
        triggers:
          type: array
          items:
            type: object

    WorkflowRun:
      type: object
      properties:
        id:
          type: string
          format: uuid
        workflow_id:
          type: string
          format: uuid
        status:
          type: string
          enum: [pending, running, paused, completed, failed, cancelled]
        parameters:
          type: object
        step_states:
          type: object
        outputs:
          type: object
        started_at:
          type: string
          format: date-time
        ended_at:
          type: string
          format: date-time

    SubmitWorkflowRunRequest:
      type: object
      properties:
        parameters:
          type: object

    ExperimentState:
      type: object
      properties:
        id:
          type: string
          format: uuid
        experiment_id:
          type: string
          format: uuid
        run_id:
          type: string
          format: uuid
        environment:
          type: object
        code_state:
          type: object
        data_state:
          type: object
        configuration:
          type: object
        random_state:
          type: object
        checksum:
          type: string
        captured_at:
          type: string
          format: date-time

    CaptureStateRequest:
      type: object
      required: [experiment_id, run_id]
      properties:
        experiment_id:
          type: string
          format: uuid
        run_id:
          type: string
          format: uuid
        parameters:
          type: object
        datasets:
          type: array
          items:
            type: object

    ValidationReport:
      type: object
      properties:
        state_id:
          type: string
          format: uuid
        is_reproducible:
          type: boolean
        issues:
          type: array
          items:
            type: object
        warnings:
          type: array
          items:
            type: object
        validated_at:
          type: string
          format: date-time

    ReplayResult:
      type: object
      properties:
        state_id:
          type: string
          format: uuid
        environment_differences:
          type: array
          items:
            type: string
        ready_to_execute:
          type: boolean
        setup_commands:
          type: array
          items:
            type: string

    GenerateCertificateRequest:
      type: object
      required: [state_id, run_id]
      properties:
        state_id:
          type: string
          format: uuid
        run_id:
          type: string
          format: uuid

    ReproducibilityCertificate:
      type: object
      properties:
        id:
          type: string
          format: uuid
        state_id:
          type: string
          format: uuid
        experiment_id:
          type: string
          format: uuid
        run_id:
          type: string
          format: uuid
        validation_report:
          $ref: '#/components/schemas/ValidationReport'
        environment_hash:
          type: string
        code_hash:
          type: string
        data_hash:
          type: string
        signature:
          type: string
        issued_at:
          type: string
          format: date-time
```

### 6.3 gRPC Service Definitions

```protobuf
// proto/experiment.proto
syntax = "proto3";

package research_lab.experiment.v1;

import "google/protobuf/timestamp.proto";
import "google/protobuf/struct.proto";

option go_package = "github.com/llm-devops/research-lab/gen/go/experiment/v1";

service ExperimentService {
  // Experiment operations
  rpc CreateExperiment(CreateExperimentRequest) returns (Experiment);
  rpc GetExperiment(GetExperimentRequest) returns (Experiment);
  rpc UpdateExperiment(UpdateExperimentRequest) returns (Experiment);
  rpc ListExperiments(ListExperimentsRequest) returns (ListExperimentsResponse);

  // Run operations
  rpc StartRun(StartRunRequest) returns (ExperimentRun);
  rpc GetRun(GetRunRequest) returns (ExperimentRun);
  rpc UpdateRunStatus(UpdateRunStatusRequest) returns (ExperimentRun);
  rpc ListRuns(ListRunsRequest) returns (ListRunsResponse);

  // Streaming operations
  rpc StreamMetrics(stream StreamMetricsRequest) returns (StreamMetricsResponse);
  rpc WatchRun(WatchRunRequest) returns (stream RunEvent);

  // Artifact operations
  rpc UploadArtifact(stream UploadArtifactRequest) returns (ArtifactRef);
  rpc DownloadArtifact(DownloadArtifactRequest) returns (stream DownloadArtifactResponse);
}

message Experiment {
  string id = 1;
  string name = 2;
  string description = 3;
  string hypothesis = 4;
  string owner_id = 5;
  repeated string collaborators = 6;
  repeated string tags = 7;
  ExperimentStatus status = 8;
  google.protobuf.Struct config = 9;
  google.protobuf.Struct metadata = 10;
  google.protobuf.Timestamp created_at = 11;
  google.protobuf.Timestamp updated_at = 12;
}

enum ExperimentStatus {
  EXPERIMENT_STATUS_UNSPECIFIED = 0;
  EXPERIMENT_STATUS_DRAFT = 1;
  EXPERIMENT_STATUS_ACTIVE = 2;
  EXPERIMENT_STATUS_PAUSED = 3;
  EXPERIMENT_STATUS_COMPLETED = 4;
  EXPERIMENT_STATUS_ARCHIVED = 5;
  EXPERIMENT_STATUS_FAILED = 6;
}

message ExperimentRun {
  string id = 1;
  string experiment_id = 2;
  int64 run_number = 3;
  string name = 4;
  RunStatus status = 5;
  google.protobuf.Struct parameters = 6;
  google.protobuf.Struct environment = 7;
  google.protobuf.Struct metrics = 8;
  repeated ArtifactRef artifacts = 9;
  string parent_run_id = 10;
  repeated string tags = 11;
  google.protobuf.Timestamp started_at = 12;
  google.protobuf.Timestamp ended_at = 13;
  google.protobuf.Timestamp created_at = 14;
  string created_by = 15;
  RunError error = 16;
}

enum RunStatus {
  RUN_STATUS_UNSPECIFIED = 0;
  RUN_STATUS_PENDING = 1;
  RUN_STATUS_QUEUED = 2;
  RUN_STATUS_RUNNING = 3;
  RUN_STATUS_COMPLETED = 4;
  RUN_STATUS_FAILED = 5;
  RUN_STATUS_CANCELLED = 6;
  RUN_STATUS_TIMED_OUT = 7;
}

message RunError {
  string error_type = 1;
  string message = 2;
  string stack_trace = 3;
  google.protobuf.Timestamp occurred_at = 4;
  bool recoverable = 5;
}

message ArtifactRef {
  string id = 1;
  string name = 2;
  ArtifactType artifact_type = 3;
  string content_hash = 4;
  int64 size_bytes = 5;
  string storage_uri = 6;
  map<string, string> metadata = 7;
  google.protobuf.Timestamp created_at = 8;
}

enum ArtifactType {
  ARTIFACT_TYPE_UNSPECIFIED = 0;
  ARTIFACT_TYPE_MODEL = 1;
  ARTIFACT_TYPE_CHECKPOINT = 2;
  ARTIFACT_TYPE_DATASET = 3;
  ARTIFACT_TYPE_VISUALIZATION = 4;
  ARTIFACT_TYPE_LOG = 5;
  ARTIFACT_TYPE_CONFIG = 6;
  ARTIFACT_TYPE_REPORT = 7;
  ARTIFACT_TYPE_CUSTOM = 8;
}

message CreateExperimentRequest {
  string name = 1;
  string description = 2;
  string hypothesis = 3;
  repeated string collaborators = 4;
  repeated string tags = 5;
  google.protobuf.Struct config = 6;
  google.protobuf.Struct metadata = 7;
}

message GetExperimentRequest {
  string id = 1;
}

message UpdateExperimentRequest {
  string id = 1;
  string name = 2;
  string description = 3;
  string hypothesis = 4;
  ExperimentStatus status = 5;
  repeated string tags = 6;
  google.protobuf.Struct config = 7;
  google.protobuf.Struct metadata = 8;
}

message ListExperimentsRequest {
  int32 offset = 1;
  int32 limit = 2;
  repeated ExperimentStatus statuses = 3;
  repeated string tags = 4;
  string owner_id = 5;
}

message ListExperimentsResponse {
  repeated Experiment experiments = 1;
  int64 total_count = 2;
  bool has_more = 3;
}

message StartRunRequest {
  string experiment_id = 1;
  string name = 2;
  google.protobuf.Struct parameters = 3;
  repeated string tags = 4;
  string parent_run_id = 5;
}

message GetRunRequest {
  string id = 1;
}

message UpdateRunStatusRequest {
  string id = 1;
  RunStatus status = 2;
  google.protobuf.Struct final_metrics = 3;
  RunError error = 4;
}

message ListRunsRequest {
  string experiment_id = 1;
  int32 offset = 2;
  int32 limit = 3;
  repeated RunStatus statuses = 4;
}

message ListRunsResponse {
  repeated ExperimentRun runs = 1;
  int64 total_count = 2;
  bool has_more = 3;
}

message StreamMetricsRequest {
  string run_id = 1;
  repeated MetricEntry metrics = 2;
}

message MetricEntry {
  string name = 1;
  double value = 2;
  int64 step = 3;
  map<string, string> context = 4;
}

message StreamMetricsResponse {
  int32 accepted_count = 1;
}

message WatchRunRequest {
  string run_id = 1;
}

message RunEvent {
  oneof event {
    StatusChangeEvent status_change = 1;
    MetricsUpdateEvent metrics_update = 2;
    ArtifactAddedEvent artifact_added = 3;
    ErrorEvent error = 4;
  }
}

message StatusChangeEvent {
  RunStatus old_status = 1;
  RunStatus new_status = 2;
  google.protobuf.Timestamp timestamp = 3;
}

message MetricsUpdateEvent {
  repeated MetricEntry metrics = 1;
  google.protobuf.Timestamp timestamp = 2;
}

message ArtifactAddedEvent {
  ArtifactRef artifact = 1;
  google.protobuf.Timestamp timestamp = 2;
}

message ErrorEvent {
  RunError error = 1;
  google.protobuf.Timestamp timestamp = 2;
}

message UploadArtifactRequest {
  oneof content {
    ArtifactMetadata metadata = 1;
    bytes chunk = 2;
  }
}

message ArtifactMetadata {
  string run_id = 1;
  string name = 2;
  ArtifactType artifact_type = 3;
  map<string, string> metadata = 4;
}

message DownloadArtifactRequest {
  string artifact_id = 1;
}

message DownloadArtifactResponse {
  bytes chunk = 1;
}
```

---

## 7. Security Architecture

### 7.1 Authentication & Authorization

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                           Security Architecture                                  │
└─────────────────────────────────────────────────────────────────────────────────┘

                    ┌───────────────────────────────────────┐
                    │           Identity Provider            │
                    │    (Keycloak / Auth0 / Okta)          │
                    │  • User Management                    │
                    │  • SSO / SAML / OIDC                  │
                    │  • MFA                                │
                    └───────────────────┬───────────────────┘
                                        │
                                        │ JWT Token
                                        ▼
┌─────────────────────────────────────────────────────────────────────────────────┐
│                              API Gateway                                         │
│  ┌─────────────────────────────────────────────────────────────────────────┐    │
│  │                        JWT Validation                                    │    │
│  │  • Signature verification                                               │    │
│  │  • Expiration check                                                     │    │
│  │  • Audience validation                                                  │    │
│  │  • Issuer validation                                                    │    │
│  └─────────────────────────────────────────────────────────────────────────┘    │
│                                      │                                           │
│  ┌─────────────────────────────────────────────────────────────────────────┐    │
│  │                        Rate Limiting                                     │    │
│  │  • Per-user limits                                                      │    │
│  │  • Per-endpoint limits                                                  │    │
│  │  • Burst protection                                                     │    │
│  └─────────────────────────────────────────────────────────────────────────┘    │
└─────────────────────────────────────────────────────────────────────────────────┘
                                        │
                                        ▼
┌─────────────────────────────────────────────────────────────────────────────────┐
│                              Service Layer                                       │
│  ┌─────────────────────────────────────────────────────────────────────────┐    │
│  │                     Authorization (RBAC)                                 │    │
│  │                                                                         │    │
│  │  Roles:                    Permissions:                                 │    │
│  │  ┌─────────────┐          ┌──────────────────────────────────────┐     │    │
│  │  │   Admin     │ ───────► │ experiments:*, runs:*, datasets:*,  │     │    │
│  │  └─────────────┘          │ metrics:*, workflows:*, admin:*     │     │    │
│  │  ┌─────────────┐          └──────────────────────────────────────┘     │    │
│  │  │ Researcher  │ ───────► experiments:crud, runs:crud, datasets:read, │    │
│  │  └─────────────┘          metrics:read, workflows:execute             │    │
│  │  ┌─────────────┐          ┌──────────────────────────────────────┐     │    │
│  │  │  Viewer     │ ───────► │ experiments:read, runs:read,         │     │    │
│  │  └─────────────┘          │ datasets:read, metrics:read          │     │    │
│  │  ┌─────────────┐          └──────────────────────────────────────┘     │    │
│  │  │   Service   │ ───────► integration:*, internal:*                   │    │
│  │  └─────────────┘                                                       │    │
│  └─────────────────────────────────────────────────────────────────────────┘    │
│                                                                                  │
│  ┌─────────────────────────────────────────────────────────────────────────┐    │
│  │                   Resource-Level Authorization                           │    │
│  │  • Owner-based access control                                           │    │
│  │  • Collaborator permissions                                             │    │
│  │  • Team/organization scoping                                            │    │
│  └─────────────────────────────────────────────────────────────────────────┘    │
└─────────────────────────────────────────────────────────────────────────────────┘
```

### 7.2 Secrets Management

```yaml
# Vault configuration for secrets management
vault:
  address: https://vault.example.com
  auth_method: kubernetes
  kubernetes_role: research-lab

  secret_engines:
    # Database credentials
    database:
      path: database/creds/research-lab
      ttl: 1h
      max_ttl: 24h

    # API keys for external services
    kv:
      path: secret/research-lab
      secrets:
        - name: s3-credentials
          keys: [access_key, secret_key]
        - name: kafka-credentials
          keys: [username, password, ca_cert]
        - name: external-api-keys
          keys: [test_bench_api_key, analytics_hub_api_key, registry_api_key]

    # PKI for mTLS
    pki:
      path: pki/issue/research-lab
      common_name: "*.research-lab.svc.cluster.local"
      ttl: 72h

  policies:
    - name: research-lab-read
      rules: |
        path "secret/data/research-lab/*" {
          capabilities = ["read"]
        }
        path "database/creds/research-lab" {
          capabilities = ["read"]
        }
        path "pki/issue/research-lab" {
          capabilities = ["create", "update"]
        }

# Kubernetes Sealed Secrets for GitOps
sealed_secrets:
  - name: postgresql-credentials
    namespace: research-lab
    data:
      username: ENC[...]
      password: ENC[...]

  - name: redis-credentials
    namespace: research-lab
    data:
      password: ENC[...]

  - name: s3-credentials
    namespace: research-lab
    data:
      access_key: ENC[...]
      secret_key: ENC[...]
```

### 7.3 Network Security

```yaml
# mTLS Configuration with Linkerd
apiVersion: policy.linkerd.io/v1beta1
kind: Server
metadata:
  name: experiment-service
  namespace: research-lab
spec:
  podSelector:
    matchLabels:
      app: experiment-service
  port: 8001
  proxyProtocol: HTTP/2
---
apiVersion: policy.linkerd.io/v1beta1
kind: ServerAuthorization
metadata:
  name: experiment-service-auth
  namespace: research-lab
spec:
  server:
    name: experiment-service
  client:
    meshTLS:
      serviceAccounts:
        - name: api-gateway
        - name: metric-service
        - name: workflow-service
        - name: reproducibility-service
---
# Network encryption at rest
encryption:
  database:
    type: AES-256-GCM
    key_rotation: 90d
    key_storage: vault

  object_storage:
    type: AES-256
    server_side: true
    kms: aws-kms  # or vault-transit

  kafka:
    type: TLS
    client_auth: required
    protocol: SASL_SSL
    mechanism: SCRAM-SHA-512
```

### 7.4 Data Protection

```yaml
data_protection:
  # PII handling
  pii:
    detection:
      enabled: true
      patterns:
        - email
        - phone
        - ssn
        - credit_card
    handling:
      log_sanitization: true
      export_redaction: true

  # Data classification
  classification:
    levels:
      - name: public
        encryption: optional
        retention: unlimited
        access: all_authenticated

      - name: internal
        encryption: required
        retention: 2_years
        access: researchers

      - name: confidential
        encryption: required
        retention: 7_years
        access: authorized_only
        audit: full

      - name: restricted
        encryption: required
        retention: 7_years
        access: need_to_know
        audit: full
        dlp: enabled

  # Data retention
  retention:
    policies:
      - resource: experiments
        default: 2_years
        archive_after: 1_year

      - resource: runs
        default: 2_years
        archive_after: 6_months

      - resource: metrics
        default: 1_year
        downsample_after: 90_days

      - resource: artifacts
        default: 7_years
        tier_after: 90_days

      - resource: audit_logs
        default: 7_years
        immutable: true

  # Backup and DR
  backup:
    postgresql:
      type: continuous
      provider: pg_basebackup
      retention: 30_days
      point_in_time: 7_days

    s3:
      type: cross_region_replication
      destination: us-west-2
      versioning: enabled

    clickhouse:
      type: daily
      retention: 30_days
```

### 7.5 Compliance Framework

```yaml
compliance:
  frameworks:
    - SOC2_Type_II
    - GDPR
    - HIPAA  # if handling health data

  controls:
    access_control:
      - Multi-factor authentication
      - Role-based access control
      - Principle of least privilege
      - Regular access reviews

    audit_logging:
      - All data access logged
      - Admin actions logged
      - Log integrity verified
      - Retention: 7 years

    data_protection:
      - Encryption at rest
      - Encryption in transit
      - Key management via Vault
      - Data classification

    incident_response:
      - Automated alerting
      - Runbook automation
      - Post-incident review
      - SLA: 1 hour acknowledgment

    change_management:
      - GitOps workflow
      - Peer review required
      - Automated testing
      - Rollback capability

  audit_trail:
    events:
      - user_login
      - user_logout
      - resource_created
      - resource_updated
      - resource_deleted
      - permission_changed
      - export_requested
      - admin_action

    storage:
      type: immutable
      encryption: true
      retention: 7_years
```

---

## 8. Observability Architecture

### 8.1 Metrics Collection

```yaml
# Prometheus configuration
prometheus:
  global:
    scrape_interval: 15s
    evaluation_interval: 15s

  scrape_configs:
    # Service discovery for Kubernetes pods
    - job_name: 'research-lab-services'
      kubernetes_sd_configs:
        - role: pod
          namespaces:
            names: ['research-lab']
      relabel_configs:
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
          action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          target_label: __address__
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: namespace
        - source_labels: [__meta_kubernetes_pod_name]
          action: replace
          target_label: pod
        - source_labels: [__meta_kubernetes_pod_label_app]
          action: replace
          target_label: app

  recording_rules:
    - name: research_lab_rules
      rules:
        # Request rate
        - record: research_lab:http_requests:rate5m
          expr: sum(rate(http_requests_total{namespace="research-lab"}[5m])) by (service, method, status)

        # Request latency percentiles
        - record: research_lab:http_request_duration:p50
          expr: histogram_quantile(0.50, sum(rate(http_request_duration_seconds_bucket{namespace="research-lab"}[5m])) by (service, le))

        - record: research_lab:http_request_duration:p95
          expr: histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{namespace="research-lab"}[5m])) by (service, le))

        - record: research_lab:http_request_duration:p99
          expr: histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket{namespace="research-lab"}[5m])) by (service, le))

        # Error rate
        - record: research_lab:http_errors:rate5m
          expr: sum(rate(http_requests_total{namespace="research-lab", status=~"5.."}[5m])) by (service) / sum(rate(http_requests_total{namespace="research-lab"}[5m])) by (service)

        # Active experiments
        - record: research_lab:experiments:active
          expr: count(experiments_status{status="active"})

        # Running benchmarks
        - record: research_lab:benchmarks:running
          expr: count(benchmark_jobs_status{status="running"})

  alerting_rules:
    - name: research_lab_alerts
      rules:
        - alert: HighErrorRate
          expr: research_lab:http_errors:rate5m > 0.05
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "High error rate in {{ $labels.service }}"
            description: "Error rate is {{ $value | humanizePercentage }} for service {{ $labels.service }}"

        - alert: HighLatency
          expr: research_lab:http_request_duration:p95 > 2
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "High latency in {{ $labels.service }}"
            description: "P95 latency is {{ $value | humanizeDuration }} for service {{ $labels.service }}"

        - alert: ServiceDown
          expr: up{namespace="research-lab"} == 0
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: "Service {{ $labels.service }} is down"

        - alert: DatabaseConnectionPoolExhausted
          expr: db_pool_connections_available{namespace="research-lab"} == 0
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: "Database connection pool exhausted for {{ $labels.service }}"

        - alert: HighMemoryUsage
          expr: container_memory_usage_bytes{namespace="research-lab"} / container_spec_memory_limit_bytes > 0.9
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "High memory usage in {{ $labels.pod }}"
```

### 8.2 Application Metrics

```rust
//! Application metrics implementation using prometheus-client

use prometheus_client::{
    encoding::text::encode,
    metrics::{counter::Counter, gauge::Gauge, histogram::Histogram},
    registry::Registry,
};
use std::sync::Arc;

/// Service metrics collection
pub struct ServiceMetrics {
    pub registry: Registry,

    // HTTP metrics
    pub http_requests_total: Counter,
    pub http_request_duration: Histogram,
    pub http_requests_in_flight: Gauge,

    // Business metrics
    pub experiments_created: Counter,
    pub experiments_active: Gauge,
    pub runs_started: Counter,
    pub runs_completed: Counter,
    pub runs_failed: Counter,
    pub metrics_logged: Counter,
    pub artifacts_uploaded: Counter,
    pub artifacts_size_bytes: Counter,

    // Benchmark metrics
    pub benchmarks_submitted: Counter,
    pub benchmarks_completed: Counter,
    pub benchmark_duration: Histogram,

    // Dataset metrics
    pub datasets_registered: Counter,
    pub dataset_versions_created: Counter,
    pub dataset_size_bytes: Gauge,

    // Workflow metrics
    pub workflows_submitted: Counter,
    pub workflows_completed: Counter,
    pub workflow_steps_executed: Counter,
    pub workflow_duration: Histogram,

    // Infrastructure metrics
    pub db_query_duration: Histogram,
    pub db_connections_active: Gauge,
    pub cache_hits: Counter,
    pub cache_misses: Counter,
    pub s3_operations: Counter,
    pub kafka_messages_produced: Counter,
    pub kafka_messages_consumed: Counter,
}

impl ServiceMetrics {
    pub fn new(service_name: &str) -> Self {
        let mut registry = Registry::default();

        // HTTP metrics with labels
        let http_requests_total = Counter::default();
        registry.register(
            "http_requests_total",
            "Total HTTP requests",
            http_requests_total.clone(),
        );

        let http_request_duration = Histogram::new([
            0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0,
        ].into_iter());
        registry.register(
            "http_request_duration_seconds",
            "HTTP request duration in seconds",
            http_request_duration.clone(),
        );

        // ... register all other metrics

        Self {
            registry,
            http_requests_total,
            http_request_duration,
            http_requests_in_flight: Gauge::default(),
            experiments_created: Counter::default(),
            experiments_active: Gauge::default(),
            runs_started: Counter::default(),
            runs_completed: Counter::default(),
            runs_failed: Counter::default(),
            metrics_logged: Counter::default(),
            artifacts_uploaded: Counter::default(),
            artifacts_size_bytes: Counter::default(),
            benchmarks_submitted: Counter::default(),
            benchmarks_completed: Counter::default(),
            benchmark_duration: Histogram::new([1.0, 5.0, 10.0, 30.0, 60.0, 300.0, 600.0, 1800.0].into_iter()),
            datasets_registered: Counter::default(),
            dataset_versions_created: Counter::default(),
            dataset_size_bytes: Gauge::default(),
            workflows_submitted: Counter::default(),
            workflows_completed: Counter::default(),
            workflow_steps_executed: Counter::default(),
            workflow_duration: Histogram::new([1.0, 5.0, 10.0, 30.0, 60.0, 300.0, 600.0, 1800.0, 3600.0].into_iter()),
            db_query_duration: Histogram::new([0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0].into_iter()),
            db_connections_active: Gauge::default(),
            cache_hits: Counter::default(),
            cache_misses: Counter::default(),
            s3_operations: Counter::default(),
            kafka_messages_produced: Counter::default(),
            kafka_messages_consumed: Counter::default(),
        }
    }

    pub fn encode(&self) -> String {
        let mut buffer = String::new();
        encode(&mut buffer, &self.registry).unwrap();
        buffer
    }
}

/// Axum middleware for HTTP metrics
pub async fn metrics_middleware<B>(
    State(metrics): State<Arc<ServiceMetrics>>,
    req: Request<B>,
    next: Next<B>,
) -> Response {
    let method = req.method().clone();
    let path = req.uri().path().to_string();

    metrics.http_requests_in_flight.inc();
    let start = std::time::Instant::now();

    let response = next.run(req).await;

    let duration = start.elapsed().as_secs_f64();
    let status = response.status().as_u16();

    metrics.http_requests_total.inc();
    metrics.http_request_duration.observe(duration);
    metrics.http_requests_in_flight.dec();

    response
}
```

### 8.3 Distributed Tracing

```yaml
# OpenTelemetry configuration
opentelemetry:
  service_name: research-lab

  exporters:
    otlp:
      endpoint: jaeger-collector.observability.svc:4317
      protocol: grpc
      tls:
        insecure: false
        ca_file: /etc/ssl/certs/ca-certificates.crt

  processors:
    batch:
      max_queue_size: 2048
      scheduled_delay_millis: 5000
      max_export_batch_size: 512

  sampling:
    type: parent_based
    root:
      type: trace_id_ratio
      ratio: 0.1  # Sample 10% of traces
    remote_parent_sampled: always_on
    remote_parent_not_sampled: always_off

  propagators:
    - tracecontext
    - baggage
    - b3

  resource:
    attributes:
      - key: service.namespace
        value: research-lab
      - key: deployment.environment
        value: ${ENVIRONMENT}
      - key: service.version
        value: ${VERSION}
```

```rust
//! Tracing implementation with OpenTelemetry

use opentelemetry::{
    global,
    sdk::{propagation::TraceContextPropagator, trace, Resource},
    trace::{Span, SpanKind, Tracer},
    KeyValue,
};
use opentelemetry_otlp::WithExportConfig;
use tracing_opentelemetry::OpenTelemetryLayer;
use tracing_subscriber::{layer::SubscriberExt, util::SubscriberInitExt, EnvFilter};

pub fn init_tracing(service_name: &str, otlp_endpoint: &str) -> Result<(), TraceError> {
    // Set up propagator
    global::set_text_map_propagator(TraceContextPropagator::new());

    // Create OTLP exporter
    let exporter = opentelemetry_otlp::new_exporter()
        .tonic()
        .with_endpoint(otlp_endpoint);

    // Create tracer
    let tracer = opentelemetry_otlp::new_pipeline()
        .tracing()
        .with_exporter(exporter)
        .with_trace_config(
            trace::config()
                .with_sampler(trace::Sampler::TraceIdRatioBased(0.1))
                .with_resource(Resource::new(vec![
                    KeyValue::new("service.name", service_name.to_string()),
                    KeyValue::new("service.namespace", "research-lab"),
                ])),
        )
        .install_batch(opentelemetry::runtime::Tokio)?;

    // Set up tracing subscriber
    let telemetry = OpenTelemetryLayer::new(tracer);

    tracing_subscriber::registry()
        .with(EnvFilter::from_default_env())
        .with(telemetry)
        .with(tracing_subscriber::fmt::layer())
        .init();

    Ok(())
}

/// Custom span attributes for research lab operations
pub fn create_experiment_span(experiment_id: &str, user_id: &str) -> impl Span {
    let tracer = global::tracer("research-lab");
    tracer
        .span_builder("create_experiment")
        .with_kind(SpanKind::Internal)
        .with_attributes(vec![
            KeyValue::new("experiment.id", experiment_id.to_string()),
            KeyValue::new("user.id", user_id.to_string()),
        ])
        .start(&tracer)
}

pub fn create_run_span(run_id: &str, experiment_id: &str) -> impl Span {
    let tracer = global::tracer("research-lab");
    tracer
        .span_builder("start_run")
        .with_kind(SpanKind::Internal)
        .with_attributes(vec![
            KeyValue::new("run.id", run_id.to_string()),
            KeyValue::new("experiment.id", experiment_id.to_string()),
        ])
        .start(&tracer)
}

pub fn create_benchmark_span(job_id: &str, model_id: &str, suite_id: &str) -> impl Span {
    let tracer = global::tracer("research-lab");
    tracer
        .span_builder("run_benchmark")
        .with_kind(SpanKind::Internal)
        .with_attributes(vec![
            KeyValue::new("benchmark.job_id", job_id.to_string()),
            KeyValue::new("benchmark.model_id", model_id.to_string()),
            KeyValue::new("benchmark.suite_id", suite_id.to_string()),
        ])
        .start(&tracer)
}
```

### 8.4 Logging Configuration

```yaml
# Vector configuration for log collection
vector:
  sources:
    kubernetes_logs:
      type: kubernetes_logs
      namespace: research-lab

  transforms:
    parse_json:
      type: remap
      inputs: [kubernetes_logs]
      source: |
        . = parse_json!(.message)
        .timestamp = parse_timestamp!(.timestamp, format: "%+")
        .kubernetes = del(.kubernetes)
        .file = del(.file)

    add_metadata:
      type: remap
      inputs: [parse_json]
      source: |
        .environment = get_env_var!("ENVIRONMENT")
        .cluster = get_env_var!("CLUSTER_NAME")

    filter_noise:
      type: filter
      inputs: [add_metadata]
      condition: |
        !match(.message, r'^Health check')

    route_by_level:
      type: route
      inputs: [filter_noise]
      route:
        error: .level == "error" || .level == "ERROR"
        warn: .level == "warn" || .level == "WARN"
        info: .level == "info" || .level == "INFO"
        debug: .level == "debug" || .level == "DEBUG"

  sinks:
    loki:
      type: loki
      inputs: [route_by_level.info, route_by_level.warn, route_by_level.error]
      endpoint: http://loki.observability.svc:3100
      labels:
        service: "{{ .kubernetes.pod_labels.app }}"
        namespace: "{{ .kubernetes.namespace }}"
        level: "{{ .level }}"
      encoding:
        codec: json

    error_alerts:
      type: http
      inputs: [route_by_level.error]
      uri: https://alerts.example.com/webhook
      encoding:
        codec: json
      request:
        headers:
          Authorization: "Bearer ${ALERT_WEBHOOK_TOKEN}"
```

```rust
//! Structured logging configuration

use serde::Serialize;
use tracing::{info, error, warn, instrument, Level};
use tracing_subscriber::{fmt, layer::SubscriberExt, util::SubscriberInitExt, EnvFilter};

#[derive(Serialize)]
pub struct LogContext {
    pub request_id: String,
    pub user_id: Option<String>,
    pub experiment_id: Option<String>,
    pub run_id: Option<String>,
}

pub fn init_logging() {
    let filter = EnvFilter::try_from_default_env()
        .unwrap_or_else(|_| EnvFilter::new("info"));

    let fmt_layer = fmt::layer()
        .json()
        .with_target(true)
        .with_thread_ids(true)
        .with_file(true)
        .with_line_number(true);

    tracing_subscriber::registry()
        .with(filter)
        .with(fmt_layer)
        .init();
}

/// Structured logging macros with context
#[macro_export]
macro_rules! log_event {
    ($level:expr, $event:expr, $context:expr, $($field:tt)*) => {
        match $level {
            Level::ERROR => {
                tracing::error!(
                    event = $event,
                    request_id = %$context.request_id,
                    user_id = ?$context.user_id,
                    experiment_id = ?$context.experiment_id,
                    run_id = ?$context.run_id,
                    $($field)*
                );
            }
            Level::WARN => {
                tracing::warn!(
                    event = $event,
                    request_id = %$context.request_id,
                    user_id = ?$context.user_id,
                    $($field)*
                );
            }
            Level::INFO => {
                tracing::info!(
                    event = $event,
                    request_id = %$context.request_id,
                    user_id = ?$context.user_id,
                    $($field)*
                );
            }
            _ => {
                tracing::debug!(
                    event = $event,
                    request_id = %$context.request_id,
                    $($field)*
                );
            }
        }
    };
}
```

### 8.5 Dashboards

```yaml
# Grafana dashboard configuration
dashboards:
  - name: Research Lab Overview
    uid: research-lab-overview
    panels:
      - title: Request Rate
        type: graph
        queries:
          - expr: sum(rate(http_requests_total{namespace="research-lab"}[5m])) by (service)

      - title: Error Rate
        type: graph
        queries:
          - expr: research_lab:http_errors:rate5m

      - title: P95 Latency
        type: graph
        queries:
          - expr: research_lab:http_request_duration:p95

      - title: Active Experiments
        type: stat
        queries:
          - expr: research_lab:experiments:active

      - title: Running Benchmarks
        type: stat
        queries:
          - expr: research_lab:benchmarks:running

      - title: Database Connections
        type: graph
        queries:
          - expr: db_connections_active{namespace="research-lab"}

  - name: Experiment Details
    uid: research-lab-experiments
    variables:
      - name: experiment_id
        type: query
        query: label_values(experiment_runs_total, experiment_id)
    panels:
      - title: Runs by Status
        type: piechart
        queries:
          - expr: count(run_status{experiment_id="$experiment_id"}) by (status)

      - title: Run Duration Distribution
        type: histogram
        queries:
          - expr: histogram_quantile(0.95, sum(rate(run_duration_seconds_bucket{experiment_id="$experiment_id"}[1h])) by (le))

      - title: Metrics Logged
        type: graph
        queries:
          - expr: sum(rate(metrics_logged_total{experiment_id="$experiment_id"}[5m]))

  - name: Benchmark Performance
    uid: research-lab-benchmarks
    panels:
      - title: Benchmark Queue Depth
        type: graph
        queries:
          - expr: benchmark_queue_depth

      - title: Benchmark Duration by Suite
        type: graph
        queries:
          - expr: histogram_quantile(0.95, sum(rate(benchmark_duration_seconds_bucket[1h])) by (suite_id, le))

      - title: Model Comparison
        type: table
        queries:
          - expr: avg(benchmark_score) by (model_id, suite_id)
```

---

## 9. Operational Procedures

### 9.1 Deployment Pipeline

```yaml
# GitHub Actions CI/CD Pipeline
name: CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install Rust
        uses: dtolnay/rust-action@stable

      - name: Run tests
        run: cargo test --all-features

      - name: Run clippy
        run: cargo clippy --all-features -- -D warnings

      - name: Check formatting
        run: cargo fmt --all -- --check

  build:
    needs: test
    runs-on: ubuntu-latest
    outputs:
      image_tag: ${{ steps.meta.outputs.tags }}
    steps:
      - uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=sha,prefix=
            type=ref,event=branch
            type=semver,pattern={{version}}

      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

  deploy-staging:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/develop'
    environment: staging
    steps:
      - uses: actions/checkout@v4

      - name: Deploy to staging
        uses: azure/k8s-deploy@v4
        with:
          namespace: research-lab-staging
          manifests: k8s/staging/
          images: ${{ needs.build.outputs.image_tag }}

      - name: Run integration tests
        run: |
          ./scripts/integration-tests.sh staging

  deploy-production:
    needs: [build, deploy-staging]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    environment: production
    steps:
      - uses: actions/checkout@v4

      - name: Deploy to production (canary)
        uses: azure/k8s-deploy@v4
        with:
          namespace: research-lab
          manifests: k8s/production/
          images: ${{ needs.build.outputs.image_tag }}
          strategy: canary
          percentage: 10

      - name: Monitor canary
        run: |
          ./scripts/monitor-canary.sh 10m

      - name: Promote canary
        uses: azure/k8s-deploy@v4
        with:
          namespace: research-lab
          manifests: k8s/production/
          images: ${{ needs.build.outputs.image_tag }}
          strategy: canary
          action: promote
```

### 9.2 Runbooks

```yaml
# Incident Response Runbooks
runbooks:
  - name: High Error Rate
    trigger: HighErrorRate alert
    severity: P1
    steps:
      - action: Acknowledge alert
        command: |
          # Acknowledge in PagerDuty/OpsGenie

      - action: Check service health
        command: |
          kubectl get pods -n research-lab -l app=${SERVICE}
          kubectl logs -n research-lab -l app=${SERVICE} --tail=100

      - action: Check dependencies
        command: |
          kubectl exec -n research-lab deploy/${SERVICE} -- \
            curl -s http://localhost:8080/health/ready

      - action: Check database connectivity
        command: |
          kubectl exec -n research-lab deploy/${SERVICE} -- \
            pg_isready -h postgresql.data.svc

      - action: Check recent deployments
        command: |
          kubectl rollout history deployment/${SERVICE} -n research-lab

      - action: Rollback if needed
        command: |
          kubectl rollout undo deployment/${SERVICE} -n research-lab

  - name: Database Connection Pool Exhausted
    trigger: DatabaseConnectionPoolExhausted alert
    severity: P1
    steps:
      - action: Check active connections
        command: |
          kubectl exec -n data postgresql-0 -- \
            psql -c "SELECT count(*) FROM pg_stat_activity WHERE state = 'active';"

      - action: Check for long-running queries
        command: |
          kubectl exec -n data postgresql-0 -- \
            psql -c "SELECT pid, now() - pg_stat_activity.query_start AS duration, query
                     FROM pg_stat_activity
                     WHERE state != 'idle'
                     ORDER BY duration DESC
                     LIMIT 10;"

      - action: Kill long-running queries if necessary
        command: |
          kubectl exec -n data postgresql-0 -- \
            psql -c "SELECT pg_terminate_backend(pid)
                     FROM pg_stat_activity
                     WHERE duration > interval '5 minutes'
                     AND state != 'idle';"

      - action: Scale up replicas
        command: |
          kubectl scale deployment/${SERVICE} -n research-lab --replicas=+2

  - name: Kafka Consumer Lag
    trigger: KafkaConsumerLag alert
    severity: P2
    steps:
      - action: Check consumer lag
        command: |
          kubectl exec -n data kafka-0 -- \
            kafka-consumer-groups.sh --bootstrap-server localhost:9092 \
            --describe --group research-lab-consumers

      - action: Scale consumers
        command: |
          kubectl scale deployment/event-consumer -n research-lab --replicas=+2

      - action: Check for poison messages
        command: |
          kubectl exec -n data kafka-0 -- \
            kafka-console-consumer.sh --bootstrap-server localhost:9092 \
            --topic research-lab.events.dlq --from-beginning --max-messages 10
```

### 9.3 Disaster Recovery

```yaml
disaster_recovery:
  rpo: 1_hour  # Recovery Point Objective
  rto: 4_hours  # Recovery Time Objective

  backup_strategy:
    postgresql:
      type: continuous_archiving
      wal_archiving:
        destination: s3://research-lab-backups/wal/
        retention: 7_days
      base_backups:
        frequency: daily
        destination: s3://research-lab-backups/base/
        retention: 30_days

    clickhouse:
      type: incremental
      frequency: daily
      destination: s3://research-lab-backups/clickhouse/
      retention: 30_days

    s3:
      type: cross_region_replication
      source: us-east-1
      destination: us-west-2
      versioning: enabled

  recovery_procedures:
    - name: Database Recovery
      steps:
        - Stop all services writing to database
        - Identify point-in-time for recovery
        - Restore from latest base backup
        - Apply WAL archives to target time
        - Verify data integrity
        - Resume services

    - name: Full Cluster Recovery
      steps:
        - Provision new Kubernetes cluster
        - Restore Vault secrets
        - Deploy infrastructure components
        - Restore databases from backup
        - Deploy application services
        - Verify end-to-end functionality
        - Update DNS records

  testing:
    frequency: quarterly
    scope:
      - Database point-in-time recovery
      - Full cluster failover
      - Cross-region failover
    documentation: required
```

---

## Document Metadata

| Field | Value |
|-------|-------|
| **Version** | 1.0.0 |
| **Status** | Draft |
| **SPARC Phase** | Architecture (Part 2 of 2) |
| **Created** | 2025-11-28 |
| **Ecosystem** | LLM DevOps |
| **Previous Part** | Architecture Part 1: System Design & Infrastructure |
| **Next Phase** | Refinement |

---

*This architecture document completes the SPARC Architecture phase for LLM-Research-Lab. The next phase will cover Refinement (iteration and optimization based on implementation feedback).*

---

# PART IV: REFINEMENT

> **SPARC Phase 4**
> Defines quality standards, testing, and compliance

---


---

## Table of Contents

1. [Executive Summary](#1-executive-summary)
2. [Refinement Objectives](#2-refinement-objectives)
3. [Code Quality Standards](#3-code-quality-standards)
4. [Performance Optimization](#4-performance-optimization)
5. [Security Hardening](#5-security-hardening)
6. [Testing Strategy](#6-testing-strategy)
7. [Documentation Standards](#7-documentation-standards)
8. [Review Processes](#8-review-processes)
9. [Iteration Framework](#9-iteration-framework)
10. [Compliance & Audit](#10-compliance--audit)
11. [Release Criteria](#11-release-criteria)

---

## 1. Executive Summary

### 1.1 Purpose

The Refinement phase transforms the initial implementation into enterprise-grade, production-ready software through systematic optimization, hardening, and validation. This specification defines the processes, standards, and criteria that ensure LLM Research Lab meets commercial viability requirements.

### 1.2 Scope

This refinement specification covers:

- **Code Quality**: Rust-specific standards, linting, and static analysis
- **Performance**: Benchmarking, profiling, and optimization strategies
- **Security**: Vulnerability assessment, penetration testing, hardening
- **Testing**: Coverage requirements, test strategies, mutation testing
- **Documentation**: API docs, architecture docs, operational runbooks
- **Review Processes**: Code review, architecture review, security review
- **Iteration**: Continuous improvement cycles and feedback integration

### 1.3 Success Criteria

| Category | Target | Measurement |
|----------|--------|-------------|
| Code Coverage | ≥ 85% | Line + Branch coverage |
| Mutation Score | ≥ 70% | Mutation testing survival |
| Performance | < 100ms p99 | API response latency |
| Security | Zero Critical | OWASP/SANS vulnerabilities |
| Documentation | 100% public API | Rustdoc coverage |
| Technical Debt | < 2% | SonarQube debt ratio |

---

## 2. Refinement Objectives

### 2.1 Primary Objectives

```yaml
objectives:
  enterprise_quality:
    description: "Achieve enterprise-grade code quality"
    targets:
      - Zero compiler warnings with strict flags
      - Zero clippy warnings at pedantic level
      - All unsafe blocks documented and audited
      - Consistent code style via rustfmt

  commercial_viability:
    description: "Ensure commercial deployment readiness"
    targets:
      - Licensing compliance verified
      - Third-party audit completed
      - SLA guarantees achievable
      - Cost optimization validated

  production_readiness:
    description: "Validate production deployment capability"
    targets:
      - Load testing at 10x expected capacity
      - Chaos engineering resilience verified
      - Disaster recovery tested
      - Monitoring coverage complete

  bug_free_implementation:
    description: "Minimize defect density"
    targets:
      - Zero known critical/high severity bugs
      - Defect density < 0.5 per KLOC
      - Regression test suite passing
      - Property-based testing coverage
```

### 2.2 Refinement Phases

```
Phase 4.1: Static Analysis & Code Quality
    │
    ├── Compiler Strict Mode
    ├── Clippy Pedantic Analysis
    ├── Rustfmt Enforcement
    └── Custom Lint Rules
         │
         ▼
Phase 4.2: Dynamic Analysis & Testing
    │
    ├── Unit Test Coverage
    ├── Integration Testing
    ├── Property-Based Testing
    └── Mutation Testing
         │
         ▼
Phase 4.3: Performance Optimization
    │
    ├── Profiling & Benchmarking
    ├── Memory Optimization
    ├── Concurrency Tuning
    └── Database Optimization
         │
         ▼
Phase 4.4: Security Hardening
    │
    ├── Vulnerability Scanning
    ├── Penetration Testing
    ├── Secrets Audit
    └── Dependency Audit
         │
         ▼
Phase 4.5: Documentation & Polish
    │
    ├── API Documentation
    ├── Architecture Docs
    ├── Operational Runbooks
    └── User Guides
         │
         ▼
Phase 4.6: Release Validation
    │
    ├── Release Candidate Build
    ├── Acceptance Testing
    ├── Performance Baseline
    └── Go/No-Go Decision
```

---

## 3. Code Quality Standards

### 3.1 Rust Compiler Configuration

```toml
# Cargo.toml - Workspace Profile Configuration

[profile.dev]
debug = true
opt-level = 0
overflow-checks = true
debug-assertions = true
panic = "unwind"

[profile.release]
opt-level = 3
lto = "fat"
codegen-units = 1
panic = "abort"
strip = "symbols"
debug = false
overflow-checks = false

[profile.release-with-debug]
inherits = "release"
debug = true
strip = "none"

[profile.bench]
inherits = "release"
debug = true
```

```rust
// lib.rs - Crate-level lint configuration

#![deny(
    // Safety
    unsafe_code,
    unsafe_op_in_unsafe_fn,

    // Correctness
    unused_must_use,
    unused_results,
    clippy::unwrap_used,
    clippy::expect_used,
    clippy::panic,

    // Documentation
    missing_docs,
    missing_debug_implementations,

    // Style
    nonstandard_style,
    rust_2018_idioms,
    rust_2021_compatibility,
)]

#![warn(
    // Best practices
    clippy::pedantic,
    clippy::nursery,
    clippy::cargo,

    // Performance
    clippy::perf,

    // Complexity
    clippy::cognitive_complexity,
)]

#![allow(
    // Intentional exceptions with justification
    clippy::module_name_repetitions, // Common in domain modeling
    clippy::too_many_arguments,      // Complex constructors allowed
)]
```

### 3.2 Clippy Configuration

```toml
# .clippy.toml

# Complexity thresholds
cognitive-complexity-threshold = 15
type-complexity-threshold = 250

# Naming conventions
allowed-prefixes = ["to_", "as_", "into_", "from_", "try_"]

# Documentation
missing-docs-in-crate-items = true

# Performance
trivial-copy-size-limit = 8
pass-by-value-size-limit = 256

# Disallowed patterns
disallowed-methods = [
    { path = "std::env::var", reason = "Use config system instead" },
    { path = "std::thread::sleep", reason = "Use tokio::time::sleep in async context" },
    { path = "std::fs::read_to_string", reason = "Use tokio::fs for async I/O" },
]

disallowed-types = [
    { path = "std::sync::Mutex", reason = "Use tokio::sync::Mutex for async" },
    { path = "std::sync::RwLock", reason = "Use tokio::sync::RwLock for async" },
]
```

### 3.3 Code Formatting Standards

```toml
# rustfmt.toml

edition = "2021"
max_width = 100
hard_tabs = false
tab_spaces = 4

# Imports
imports_granularity = "Module"
group_imports = "StdExternalCrate"
reorder_imports = true
reorder_modules = true

# Functions
fn_params_layout = "Tall"
fn_single_line = false

# Control flow
control_brace_style = "AlwaysSameLine"
brace_style = "SameLineWhere"

# Structs and enums
struct_lit_single_line = true
enum_discrim_align_threshold = 20

# Comments
wrap_comments = true
comment_width = 80
normalize_comments = true
normalize_doc_attributes = true

# Formatting strictness
error_on_line_overflow = true
error_on_unformatted = true
```

### 3.4 Custom Lint Rules

```rust
// build-tools/custom-lints/src/lib.rs

use rustc_lint::{LateContext, LateLintPass, LintContext, LintPass};
use rustc_ast::ast;

declare_lint! {
    /// Ensures all public async functions have timeout handling
    pub ASYNC_WITHOUT_TIMEOUT,
    Warn,
    "async functions should have timeout handling"
}

declare_lint! {
    /// Ensures database queries use parameterized statements
    pub RAW_SQL_QUERY,
    Deny,
    "raw SQL queries are not allowed; use parameterized queries"
}

declare_lint! {
    /// Ensures error types implement proper traits
    pub INCOMPLETE_ERROR_TYPE,
    Deny,
    "error types must implement Error, Debug, Display, Send, and Sync"
}

declare_lint! {
    /// Ensures sensitive data types are marked appropriately
    pub UNPROTECTED_SENSITIVE_DATA,
    Deny,
    "sensitive data must use SecretString or Zeroizing wrapper"
}

declare_lint_pass!(CustomLints => [
    ASYNC_WITHOUT_TIMEOUT,
    RAW_SQL_QUERY,
    INCOMPLETE_ERROR_TYPE,
    UNPROTECTED_SENSITIVE_DATA,
]);

impl<'tcx> LateLintPass<'tcx> for CustomLints {
    fn check_fn(
        &mut self,
        cx: &LateContext<'tcx>,
        kind: FnKind<'tcx>,
        decl: &'tcx FnDecl<'tcx>,
        body: &'tcx Body<'tcx>,
        span: Span,
        def_id: LocalDefId,
    ) {
        // Implementation for async timeout checking
        if is_async_fn(kind) && !has_timeout_handling(body) {
            cx.span_lint(
                ASYNC_WITHOUT_TIMEOUT,
                span,
                "async function missing timeout; wrap with tokio::time::timeout",
            );
        }
    }

    fn check_expr(&mut self, cx: &LateContext<'tcx>, expr: &'tcx Expr<'tcx>) {
        // Implementation for SQL injection prevention
        if is_raw_sql_call(expr) {
            cx.span_lint(
                RAW_SQL_QUERY,
                expr.span,
                "use sqlx::query! or parameterized queries",
            );
        }
    }
}
```

### 3.5 Code Quality Metrics

```yaml
# sonar-project.properties equivalent

quality_gates:
  coverage:
    line_coverage: ">= 85%"
    branch_coverage: ">= 80%"

  duplication:
    duplicated_lines: "< 3%"
    duplicated_blocks: "< 5"

  maintainability:
    technical_debt_ratio: "< 2%"
    code_smells: "< 50"
    cognitive_complexity: "< 15 per function"

  reliability:
    bugs: "0"
    vulnerabilities: "0"

  security:
    security_hotspots_reviewed: "100%"
    security_rating: "A"

metrics_collection:
  tools:
    - name: "cargo-tarpaulin"
      purpose: "Code coverage"
      config: "--all-features --workspace --out Xml"

    - name: "cargo-audit"
      purpose: "Vulnerability scanning"
      config: "--deny warnings"

    - name: "cargo-outdated"
      purpose: "Dependency freshness"
      config: "--workspace"

    - name: "cargo-bloat"
      purpose: "Binary size analysis"
      config: "--release --crates"

    - name: "tokei"
      purpose: "Lines of code analysis"
      config: "--output json"
```

---

## 4. Performance Optimization

### 4.1 Benchmarking Framework

```rust
// benches/experiment_benchmarks.rs

use criterion::{
    black_box, criterion_group, criterion_main,
    Criterion, BenchmarkId, Throughput, PlotConfiguration,
};
use llm_research_lab::experiment::{ExperimentTracker, CreateExperimentRequest};
use tokio::runtime::Runtime;

fn benchmark_experiment_creation(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let tracker = rt.block_on(setup_tracker());

    let mut group = c.benchmark_group("experiment_creation");
    group.throughput(Throughput::Elements(1));
    group.sample_size(1000);
    group.measurement_time(Duration::from_secs(30));

    // Baseline benchmark
    group.bench_function("create_experiment_baseline", |b| {
        b.to_async(&rt).iter(|| async {
            let request = CreateExperimentRequest::default();
            black_box(tracker.create_experiment(request).await)
        })
    });

    // Parameterized benchmarks
    for size in [10, 100, 1000, 10000].iter() {
        group.bench_with_input(
            BenchmarkId::new("create_with_parameters", size),
            size,
            |b, &size| {
                b.to_async(&rt).iter(|| async {
                    let request = generate_request_with_params(size);
                    black_box(tracker.create_experiment(request).await)
                })
            },
        );
    }

    group.finish();
}

fn benchmark_concurrent_runs(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let tracker = rt.block_on(setup_tracker());

    let mut group = c.benchmark_group("concurrent_runs");
    group.plot_config(PlotConfiguration::default().summary_scale(AxisScale::Logarithmic));

    for concurrency in [1, 10, 50, 100, 500].iter() {
        group.throughput(Throughput::Elements(*concurrency as u64));
        group.bench_with_input(
            BenchmarkId::new("parallel_run_creation", concurrency),
            concurrency,
            |b, &concurrency| {
                b.to_async(&rt).iter(|| async {
                    let futures: Vec<_> = (0..concurrency)
                        .map(|_| tracker.start_run(experiment_id))
                        .collect();
                    black_box(futures::future::join_all(futures).await)
                })
            },
        );
    }

    group.finish();
}

criterion_group!(
    name = benches;
    config = Criterion::default()
        .with_plots()
        .significance_level(0.01)
        .noise_threshold(0.03);
    targets = benchmark_experiment_creation, benchmark_concurrent_runs
);
criterion_main!(benches);
```

### 4.2 Performance Targets

```yaml
# performance_targets.yaml

api_latency:
  experiment_create:
    p50: 10ms
    p95: 50ms
    p99: 100ms
    max: 500ms

  experiment_get:
    p50: 5ms
    p95: 20ms
    p99: 50ms
    max: 200ms

  run_start:
    p50: 15ms
    p95: 75ms
    p99: 150ms
    max: 750ms

  metric_log:
    p50: 2ms
    p95: 10ms
    p99: 25ms
    max: 100ms

  artifact_upload_1mb:
    p50: 100ms
    p95: 500ms
    p99: 1000ms
    max: 5000ms

throughput:
  experiments_per_second: 1000
  runs_per_second: 5000
  metrics_per_second: 100000
  artifacts_per_minute: 1000

resource_utilization:
  cpu_per_pod: "< 80%"
  memory_per_pod: "< 85%"
  disk_iops: "< 10000"
  network_bandwidth: "< 1 Gbps"

database_performance:
  query_latency_p99: "< 50ms"
  connection_pool_utilization: "< 80%"
  index_hit_ratio: "> 99%"
  deadlock_rate: "0"
```

### 4.3 Profiling Configuration

```rust
// profiling/src/lib.rs

use std::sync::atomic::{AtomicBool, Ordering};
use tracing::{span, Level, Instrument};

/// Conditional profiling that can be enabled at runtime
pub struct Profiler {
    enabled: AtomicBool,
    pprof_guard: Option<pprof::ProfilerGuard<'static>>,
}

impl Profiler {
    pub fn new() -> Self {
        Self {
            enabled: AtomicBool::new(false),
            pprof_guard: None,
        }
    }

    /// Start CPU profiling
    pub fn start_cpu_profile(&mut self) -> Result<(), ProfileError> {
        if self.enabled.load(Ordering::Relaxed) {
            return Err(ProfileError::AlreadyRunning);
        }

        let guard = pprof::ProfilerGuardBuilder::default()
            .frequency(1000)
            .blocklist(&["libc", "libpthread"])
            .build()?;

        self.pprof_guard = Some(guard);
        self.enabled.store(true, Ordering::Relaxed);
        Ok(())
    }

    /// Stop CPU profiling and generate report
    pub fn stop_cpu_profile(&mut self) -> Result<pprof::Report, ProfileError> {
        let guard = self.pprof_guard.take()
            .ok_or(ProfileError::NotRunning)?;

        self.enabled.store(false, Ordering::Relaxed);
        Ok(guard.report().build()?)
    }

    /// Generate flamegraph SVG
    pub fn generate_flamegraph(&self, report: &pprof::Report) -> Result<Vec<u8>, ProfileError> {
        let mut buffer = Vec::new();
        report.flamegraph(&mut buffer)?;
        Ok(buffer)
    }
}

/// Memory profiling using jemalloc
#[cfg(feature = "memory-profiling")]
pub mod memory {
    use jemalloc_ctl::{stats, epoch};

    pub struct MemoryStats {
        pub allocated: usize,
        pub resident: usize,
        pub active: usize,
        pub mapped: usize,
        pub retained: usize,
    }

    pub fn collect_stats() -> Result<MemoryStats, MemoryError> {
        epoch::advance()?;

        Ok(MemoryStats {
            allocated: stats::allocated::read()?,
            resident: stats::resident::read()?,
            active: stats::active::read()?,
            mapped: stats::mapped::read()?,
            retained: stats::retained::read()?,
        })
    }

    /// Dump heap profile to file
    pub fn dump_heap_profile(path: &str) -> Result<(), MemoryError> {
        jemalloc_ctl::prof::dump::write(path.as_bytes())?;
        Ok(())
    }
}

/// Async task profiling
pub mod async_profiler {
    use tokio::runtime::RuntimeMetrics;

    #[derive(Debug, Clone)]
    pub struct AsyncMetrics {
        pub num_workers: usize,
        pub num_blocking_threads: usize,
        pub active_tasks: u64,
        pub injection_queue_depth: usize,
        pub total_park_count: u64,
        pub total_steal_count: u64,
    }

    pub fn collect_async_metrics(metrics: &RuntimeMetrics) -> AsyncMetrics {
        AsyncMetrics {
            num_workers: metrics.num_workers(),
            num_blocking_threads: metrics.num_blocking_threads(),
            active_tasks: metrics.active_tasks_count() as u64,
            injection_queue_depth: metrics.injection_queue_depth(),
            total_park_count: (0..metrics.num_workers())
                .map(|i| metrics.worker_park_count(i))
                .sum(),
            total_steal_count: (0..metrics.num_workers())
                .map(|i| metrics.worker_steal_count(i))
                .sum(),
        }
    }
}
```

### 4.4 Optimization Strategies

```rust
// optimization_guidelines.rs

/// Memory Optimization Patterns

// 1. Use SmallVec for small collections
use smallvec::SmallVec;
type Tags = SmallVec<[String; 8]>;  // Stack-allocated for ≤8 items

// 2. Use Cow for optional ownership
use std::borrow::Cow;
fn process_name(name: Cow<'_, str>) -> ProcessedName {
    // Avoids allocation if name doesn't need modification
    if needs_normalization(&name) {
        ProcessedName(normalize(name.into_owned()))
    } else {
        ProcessedName(name.into_owned())
    }
}

// 3. Use bytes::Bytes for zero-copy slicing
use bytes::Bytes;
struct ArtifactChunk {
    data: Bytes,  // Reference-counted, cheap to clone
}

// 4. Preallocate collections
fn collect_metrics(count_hint: usize) -> Vec<Metric> {
    let mut metrics = Vec::with_capacity(count_hint);
    // ... populate
    metrics.shrink_to_fit();  // Release excess capacity
    metrics
}

/// Concurrency Optimization Patterns

// 1. Use dashmap for concurrent maps
use dashmap::DashMap;
type ConcurrentCache<K, V> = DashMap<K, V, ahash::RandomState>;

// 2. Batch operations for reduced lock contention
impl MetricBatcher {
    async fn batch_insert(&self, metrics: Vec<Metric>) -> Result<(), Error> {
        // Single lock acquisition for batch
        let mut guard = self.buffer.lock().await;
        guard.extend(metrics);

        if guard.len() >= BATCH_THRESHOLD {
            let batch = std::mem::take(&mut *guard);
            drop(guard);  // Release lock before I/O
            self.flush(batch).await?;
        }
        Ok(())
    }
}

// 3. Use channels for backpressure
use tokio::sync::mpsc;
async fn create_bounded_processor(capacity: usize) -> (Sender, JoinHandle<()>) {
    let (tx, mut rx) = mpsc::channel(capacity);  // Bounded channel

    let handle = tokio::spawn(async move {
        while let Some(item) = rx.recv().await {
            process(item).await;
        }
    });

    (tx, handle)
}

/// Database Optimization Patterns

// 1. Use prepared statements
impl ExperimentRepository {
    pub async fn new(pool: PgPool) -> Result<Self, Error> {
        // Prepare statements at startup
        let get_stmt = sqlx::query("SELECT * FROM experiments WHERE id = $1")
            .persistent(true);
        let list_stmt = sqlx::query("SELECT * FROM experiments WHERE project_id = $1 LIMIT $2 OFFSET $3")
            .persistent(true);

        Ok(Self { pool, get_stmt, list_stmt })
    }
}

// 2. Use COPY for bulk inserts
async fn bulk_insert_metrics(pool: &PgPool, metrics: &[Metric]) -> Result<(), Error> {
    let mut copy = pool.copy_in_raw(
        "COPY metrics (run_id, name, value, step, timestamp) FROM STDIN WITH (FORMAT BINARY)"
    ).await?;

    for metric in metrics {
        copy.send(metric.to_binary()).await?;
    }

    copy.finish().await?;
    Ok(())
}

// 3. Use covering indexes
/*
CREATE INDEX CONCURRENTLY idx_metrics_covering
ON metrics (run_id, name, step)
INCLUDE (value, timestamp);
*/
```

### 4.5 Load Testing Configuration

```yaml
# k6/load_test.js configuration equivalent

load_test_scenarios:
  smoke:
    description: "Verify system under minimal load"
    vus: 1
    duration: "1m"
    thresholds:
      http_req_duration: "p(99) < 500"
      http_req_failed: "rate < 0.01"

  average:
    description: "Normal operating conditions"
    stages:
      - duration: "2m"
        target: 50
      - duration: "5m"
        target: 50
      - duration: "2m"
        target: 0
    thresholds:
      http_req_duration: "p(95) < 200"
      http_req_failed: "rate < 0.01"

  stress:
    description: "Find system limits"
    stages:
      - duration: "2m"
        target: 100
      - duration: "5m"
        target: 100
      - duration: "2m"
        target: 200
      - duration: "5m"
        target: 200
      - duration: "2m"
        target: 300
      - duration: "5m"
        target: 300
      - duration: "10m"
        target: 0
    thresholds:
      http_req_duration: "p(99) < 1000"
      http_req_failed: "rate < 0.05"

  spike:
    description: "Test sudden traffic spikes"
    stages:
      - duration: "10s"
        target: 100
      - duration: "1m"
        target: 100
      - duration: "10s"
        target: 1000
      - duration: "3m"
        target: 1000
      - duration: "10s"
        target: 100
      - duration: "3m"
        target: 100
      - duration: "10s"
        target: 0

  soak:
    description: "Extended duration reliability test"
    vus: 100
    duration: "4h"
    thresholds:
      http_req_duration: "p(99) < 500"
      http_req_failed: "rate < 0.001"

  breakpoint:
    description: "Find maximum capacity"
    executor: "ramping-arrival-rate"
    startRate: 50
    timeUnit: "1s"
    preAllocatedVUs: 500
    maxVUs: 2000
    stages:
      - duration: "2m"
        target: 50
      - duration: "10m"
        target: 500
      - duration: "10m"
        target: 1000
      - duration: "10m"
        target: 2000
```

---

## 5. Security Hardening

### 5.1 Security Audit Checklist

```yaml
# security_audit_checklist.yaml

authentication:
  - id: AUTH-001
    check: "JWT tokens use RS256 or ES256 algorithm"
    severity: critical
    automated: true

  - id: AUTH-002
    check: "Token expiration is enforced (max 1 hour)"
    severity: critical
    automated: true

  - id: AUTH-003
    check: "Refresh tokens are rotated on use"
    severity: high
    automated: true

  - id: AUTH-004
    check: "Failed login attempts are rate limited"
    severity: high
    automated: true

  - id: AUTH-005
    check: "Password hashing uses Argon2id"
    severity: critical
    automated: true

authorization:
  - id: AUTHZ-001
    check: "RBAC policies are enforced on all endpoints"
    severity: critical
    automated: true

  - id: AUTHZ-002
    check: "Resource ownership is validated"
    severity: critical
    automated: true

  - id: AUTHZ-003
    check: "Cross-tenant data access is prevented"
    severity: critical
    automated: true

  - id: AUTHZ-004
    check: "Admin actions require MFA"
    severity: high
    automated: false

data_protection:
  - id: DATA-001
    check: "All PII is encrypted at rest"
    severity: critical
    automated: true

  - id: DATA-002
    check: "TLS 1.3 enforced for all connections"
    severity: critical
    automated: true

  - id: DATA-003
    check: "Secrets are stored in Vault, not config"
    severity: critical
    automated: true

  - id: DATA-004
    check: "Database connections use mTLS"
    severity: high
    automated: true

  - id: DATA-005
    check: "Sensitive data is scrubbed from logs"
    severity: critical
    automated: true

input_validation:
  - id: INPUT-001
    check: "All inputs are validated and sanitized"
    severity: critical
    automated: true

  - id: INPUT-002
    check: "SQL injection prevention verified"
    severity: critical
    automated: true

  - id: INPUT-003
    check: "Path traversal prevention verified"
    severity: critical
    automated: true

  - id: INPUT-004
    check: "Request size limits enforced"
    severity: high
    automated: true

  - id: INPUT-005
    check: "Content-Type validation enforced"
    severity: medium
    automated: true

dependency_security:
  - id: DEP-001
    check: "No known vulnerabilities in dependencies"
    severity: critical
    automated: true
    tool: "cargo-audit"

  - id: DEP-002
    check: "Dependencies are pinned to specific versions"
    severity: medium
    automated: true

  - id: DEP-003
    check: "No deprecated dependencies in use"
    severity: low
    automated: true
    tool: "cargo-outdated"

  - id: DEP-004
    check: "License compatibility verified"
    severity: high
    automated: true
    tool: "cargo-deny"
```

### 5.2 Vulnerability Scanning

```rust
// security/vulnerability_scanner.rs

use std::process::Command;

/// Comprehensive security scanning orchestrator
pub struct SecurityScanner {
    config: SecurityConfig,
}

impl SecurityScanner {
    /// Run all security scans
    pub async fn run_full_scan(&self) -> SecurityReport {
        let (
            audit_result,
            deny_result,
            semgrep_result,
            trivy_result,
        ) = tokio::join!(
            self.run_cargo_audit(),
            self.run_cargo_deny(),
            self.run_semgrep(),
            self.run_trivy(),
        );

        SecurityReport {
            cargo_audit: audit_result,
            cargo_deny: deny_result,
            semgrep: semgrep_result,
            trivy: trivy_result,
            timestamp: Utc::now(),
        }
    }

    /// Scan Rust dependencies for vulnerabilities
    async fn run_cargo_audit(&self) -> AuditResult {
        let output = Command::new("cargo")
            .args(["audit", "--json", "--deny", "warnings"])
            .output()
            .await?;

        parse_audit_output(&output.stdout)
    }

    /// Check dependency licenses and bans
    async fn run_cargo_deny(&self) -> DenyResult {
        let output = Command::new("cargo")
            .args(["deny", "check", "--format", "json"])
            .output()
            .await?;

        parse_deny_output(&output.stdout)
    }

    /// Static analysis with Semgrep
    async fn run_semgrep(&self) -> SemgrepResult {
        let output = Command::new("semgrep")
            .args([
                "--config", "p/rust",
                "--config", "p/security-audit",
                "--config", "p/owasp-top-ten",
                "--json",
                ".",
            ])
            .output()
            .await?;

        parse_semgrep_output(&output.stdout)
    }

    /// Container vulnerability scanning
    async fn run_trivy(&self) -> TrivyResult {
        let output = Command::new("trivy")
            .args([
                "image",
                "--format", "json",
                "--severity", "CRITICAL,HIGH",
                &self.config.container_image,
            ])
            .output()
            .await?;

        parse_trivy_output(&output.stdout)
    }
}

/// cargo-deny configuration
// deny.toml
/*
[advisories]
db-path = "~/.cargo/advisory-db"
db-urls = ["https://github.com/rustsec/advisory-db"]
vulnerability = "deny"
unmaintained = "warn"
yanked = "deny"
notice = "warn"

[licenses]
allow = [
    "MIT",
    "Apache-2.0",
    "Apache-2.0 WITH LLVM-exception",
    "BSD-2-Clause",
    "BSD-3-Clause",
    "ISC",
    "Zlib",
    "MPL-2.0",
]
confidence-threshold = 0.8
copyleft = "deny"
allow-osi-fsf-free = "neither"
default = "deny"

[bans]
multiple-versions = "warn"
wildcards = "deny"
highlight = "all"
deny = [
    { name = "openssl", wrappers = ["native-tls"] },
    { name = "openssl-sys", wrappers = ["native-tls"] },
]
skip = []
skip-tree = []

[sources]
unknown-registry = "deny"
unknown-git = "deny"
allow-registry = ["https://github.com/rust-lang/crates.io-index"]
*/
```

### 5.3 Penetration Testing Plan

```yaml
# pentest_plan.yaml

scope:
  in_scope:
    - "All API endpoints (REST and gRPC)"
    - "Authentication and authorization systems"
    - "File upload/download functionality"
    - "WebSocket connections"
    - "Database access patterns"
    - "Inter-service communication"

  out_of_scope:
    - "Third-party services (AWS, GCP infrastructure)"
    - "Denial of service testing in production"
    - "Social engineering"
    - "Physical security"

test_cases:
  authentication:
    - name: "JWT Token Manipulation"
      description: "Attempt to forge or tamper with JWT tokens"
      tools: ["jwt_tool", "burp"]

    - name: "Session Fixation"
      description: "Test for session fixation vulnerabilities"
      tools: ["burp", "custom_scripts"]

    - name: "Credential Stuffing"
      description: "Test rate limiting effectiveness"
      tools: ["hydra", "custom_scripts"]

  authorization:
    - name: "Horizontal Privilege Escalation"
      description: "Access other users' resources"
      tools: ["burp", "autorize"]

    - name: "Vertical Privilege Escalation"
      description: "Elevate to admin privileges"
      tools: ["burp", "custom_scripts"]

    - name: "IDOR Testing"
      description: "Test for insecure direct object references"
      tools: ["burp", "autorize"]

  injection:
    - name: "SQL Injection"
      description: "Test all input fields for SQL injection"
      tools: ["sqlmap", "burp"]

    - name: "NoSQL Injection"
      description: "Test for NoSQL injection vulnerabilities"
      tools: ["nosqlmap", "burp"]

    - name: "Command Injection"
      description: "Test for OS command injection"
      tools: ["commix", "burp"]

    - name: "LDAP Injection"
      description: "Test for LDAP injection if applicable"
      tools: ["custom_scripts"]

  api_security:
    - name: "Mass Assignment"
      description: "Test for mass assignment vulnerabilities"
      tools: ["burp", "custom_scripts"]

    - name: "Rate Limiting Bypass"
      description: "Attempt to bypass rate limiting"
      tools: ["custom_scripts"]

    - name: "API Versioning Attacks"
      description: "Test deprecated API versions"
      tools: ["burp"]

  file_handling:
    - name: "Path Traversal"
      description: "Test for directory traversal vulnerabilities"
      tools: ["dotdotpwn", "burp"]

    - name: "Malicious File Upload"
      description: "Upload malicious files and test execution"
      tools: ["burp", "custom_payloads"]

    - name: "XXE Injection"
      description: "Test for XML external entity injection"
      tools: ["xxeinjector", "burp"]

reporting:
  format: "OWASP Testing Guide"
  severity_levels:
    critical: "Immediate remediation required"
    high: "Remediate within 7 days"
    medium: "Remediate within 30 days"
    low: "Remediate within 90 days"
    informational: "Consider for future improvements"
```

### 5.4 Security Hardening Implementation

```rust
// security/hardening.rs

use tower::ServiceBuilder;
use tower_http::{
    cors::CorsLayer,
    request_id::MakeRequestUuid,
    sensitive_headers::SetSensitiveHeadersLayer,
    timeout::TimeoutLayer,
    limit::RequestBodyLimitLayer,
};

/// Apply security hardening middleware stack
pub fn security_middleware_stack() -> impl Layer<S> {
    ServiceBuilder::new()
        // Request ID for tracing
        .layer(MakeRequestUuid::default())

        // Rate limiting
        .layer(RateLimitLayer::new(
            NonZeroU32::new(1000).unwrap(),
            Duration::from_secs(60),
        ))

        // Request timeout
        .layer(TimeoutLayer::new(Duration::from_secs(30)))

        // Request body size limit
        .layer(RequestBodyLimitLayer::new(10 * 1024 * 1024)) // 10MB

        // Sensitive header scrubbing
        .layer(SetSensitiveHeadersLayer::new([
            header::AUTHORIZATION,
            header::COOKIE,
            HeaderName::from_static("x-api-key"),
        ]))

        // CORS configuration
        .layer(
            CorsLayer::new()
                .allow_origin(AllowOrigin::list(allowed_origins()))
                .allow_methods([Method::GET, Method::POST, Method::PUT, Method::DELETE])
                .allow_headers([header::CONTENT_TYPE, header::AUTHORIZATION])
                .max_age(Duration::from_secs(3600))
        )

        // Security headers
        .layer(SecurityHeadersLayer::default())

        // Request validation
        .layer(RequestValidationLayer::new())
}

/// Security headers middleware
pub struct SecurityHeadersLayer;

impl<S> Layer<S> for SecurityHeadersLayer {
    type Service = SecurityHeadersService<S>;

    fn layer(&self, inner: S) -> Self::Service {
        SecurityHeadersService { inner }
    }
}

impl<S, B> Service<Request<B>> for SecurityHeadersService<S>
where
    S: Service<Request<B>, Response = Response<BoxBody>>,
{
    type Response = Response<BoxBody>;
    type Error = S::Error;
    type Future = SecurityHeadersFuture<S::Future>;

    fn poll_ready(&mut self, cx: &mut Context<'_>) -> Poll<Result<(), Self::Error>> {
        self.inner.poll_ready(cx)
    }

    fn call(&mut self, req: Request<B>) -> Self::Future {
        SecurityHeadersFuture {
            inner: self.inner.call(req),
        }
    }
}

impl<F> Future for SecurityHeadersFuture<F>
where
    F: Future<Output = Result<Response<BoxBody>, E>>,
{
    type Output = F::Output;

    fn poll(self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Self::Output> {
        let this = self.project();
        match this.inner.poll(cx) {
            Poll::Ready(Ok(mut response)) => {
                let headers = response.headers_mut();

                // Prevent clickjacking
                headers.insert(
                    HeaderName::from_static("x-frame-options"),
                    HeaderValue::from_static("DENY"),
                );

                // XSS protection
                headers.insert(
                    HeaderName::from_static("x-content-type-options"),
                    HeaderValue::from_static("nosniff"),
                );

                // Strict transport security
                headers.insert(
                    header::STRICT_TRANSPORT_SECURITY,
                    HeaderValue::from_static("max-age=31536000; includeSubDomains; preload"),
                );

                // Content security policy
                headers.insert(
                    HeaderName::from_static("content-security-policy"),
                    HeaderValue::from_static(
                        "default-src 'self'; script-src 'self'; style-src 'self'; img-src 'self' data:; font-src 'self'; connect-src 'self'; frame-ancestors 'none';"
                    ),
                );

                // Referrer policy
                headers.insert(
                    HeaderName::from_static("referrer-policy"),
                    HeaderValue::from_static("strict-origin-when-cross-origin"),
                );

                // Permissions policy
                headers.insert(
                    HeaderName::from_static("permissions-policy"),
                    HeaderValue::from_static(
                        "accelerometer=(), camera=(), geolocation=(), gyroscope=(), magnetometer=(), microphone=(), payment=(), usb=()"
                    ),
                );

                Poll::Ready(Ok(response))
            }
            other => other,
        }
    }
}

/// Input sanitization utilities
pub mod sanitization {
    use validator::Validate;

    /// Sanitize string input
    pub fn sanitize_string(input: &str) -> String {
        input
            .chars()
            .filter(|c| !c.is_control() || *c == '\n' || *c == '\t')
            .take(MAX_STRING_LENGTH)
            .collect()
    }

    /// Validate and sanitize file path
    pub fn sanitize_path(input: &str) -> Result<PathBuf, ValidationError> {
        let path = PathBuf::from(input);

        // Check for path traversal
        if path.components().any(|c| matches!(c, Component::ParentDir)) {
            return Err(ValidationError::PathTraversal);
        }

        // Normalize the path
        let normalized = path.canonicalize()
            .map_err(|_| ValidationError::InvalidPath)?;

        // Verify within allowed directory
        if !normalized.starts_with(&*ALLOWED_BASE_PATH) {
            return Err(ValidationError::PathOutsideAllowed);
        }

        Ok(normalized)
    }

    /// SQL identifier sanitization
    pub fn sanitize_identifier(input: &str) -> Result<String, ValidationError> {
        let re = Regex::new(r"^[a-zA-Z_][a-zA-Z0-9_]*$").unwrap();

        if !re.is_match(input) {
            return Err(ValidationError::InvalidIdentifier);
        }

        if input.len() > 63 {
            return Err(ValidationError::IdentifierTooLong);
        }

        Ok(input.to_string())
    }
}
```

---

## 6. Testing Strategy

### 6.1 Test Pyramid

```
                    ┌─────────────┐
                    │   E2E (5%)  │  - Full system tests
                    │  20 tests   │  - Critical user flows
                    ├─────────────┤
                    │             │
                    │ Integration │  - Service interactions
                    │   (15%)     │  - Database integration
                    │  60 tests   │  - External API mocks
                    │             │
                    ├─────────────┤
                    │             │
                    │             │
                    │    Unit     │  - Pure functions
                    │   (80%)     │  - Business logic
                    │  320 tests  │  - Edge cases
                    │             │
                    │             │
                    └─────────────┘
```

### 6.2 Unit Testing Standards

```rust
// testing/unit_test_patterns.rs

use proptest::prelude::*;
use rstest::*;

/// Unit test organization
#[cfg(test)]
mod tests {
    use super::*;

    // Group 1: Happy path tests
    mod happy_path {
        use super::*;

        #[test]
        fn create_experiment_with_valid_data_succeeds() {
            // Arrange
            let request = CreateExperimentRequest {
                name: "test-experiment".to_string(),
                project_id: ProjectId::new(),
                ..Default::default()
            };
            let tracker = ExperimentTracker::new_in_memory();

            // Act
            let result = tracker.create_experiment(request);

            // Assert
            assert!(result.is_ok());
            let experiment = result.unwrap();
            assert_eq!(experiment.name, "test-experiment");
            assert_eq!(experiment.status, ExperimentStatus::Created);
        }
    }

    // Group 2: Error handling tests
    mod error_handling {
        use super::*;

        #[test]
        fn create_experiment_with_empty_name_fails() {
            // Arrange
            let request = CreateExperimentRequest {
                name: "".to_string(),
                ..Default::default()
            };
            let tracker = ExperimentTracker::new_in_memory();

            // Act
            let result = tracker.create_experiment(request);

            // Assert
            assert!(matches!(
                result,
                Err(ExperimentError::Validation(ValidationError::EmptyName))
            ));
        }

        #[test]
        fn create_experiment_with_invalid_characters_fails() {
            let request = CreateExperimentRequest {
                name: "test\x00experiment".to_string(),
                ..Default::default()
            };
            let tracker = ExperimentTracker::new_in_memory();

            let result = tracker.create_experiment(request);

            assert!(matches!(
                result,
                Err(ExperimentError::Validation(ValidationError::InvalidCharacters(_)))
            ));
        }
    }

    // Group 3: Edge cases
    mod edge_cases {
        use super::*;

        #[test]
        fn create_experiment_at_name_length_boundary() {
            // Test at exactly MAX_NAME_LENGTH
            let name = "a".repeat(MAX_NAME_LENGTH);
            let request = CreateExperimentRequest {
                name,
                ..Default::default()
            };
            let tracker = ExperimentTracker::new_in_memory();

            let result = tracker.create_experiment(request);
            assert!(result.is_ok());

            // Test at MAX_NAME_LENGTH + 1
            let name = "a".repeat(MAX_NAME_LENGTH + 1);
            let request = CreateExperimentRequest {
                name,
                ..Default::default()
            };

            let result = tracker.create_experiment(request);
            assert!(matches!(
                result,
                Err(ExperimentError::Validation(ValidationError::NameTooLong { .. }))
            ));
        }
    }

    // Group 4: Property-based tests
    mod properties {
        use super::*;

        proptest! {
            #[test]
            fn experiment_id_roundtrip(id in any::<u128>()) {
                let experiment_id = ExperimentId::from_raw(id);
                let serialized = experiment_id.to_string();
                let deserialized: ExperimentId = serialized.parse().unwrap();
                prop_assert_eq!(experiment_id, deserialized);
            }

            #[test]
            fn valid_names_always_accepted(name in "[a-zA-Z][a-zA-Z0-9_-]{0,254}") {
                let request = CreateExperimentRequest {
                    name: name.clone(),
                    ..Default::default()
                };
                let tracker = ExperimentTracker::new_in_memory();

                let result = tracker.create_experiment(request);
                prop_assert!(result.is_ok(), "Valid name rejected: {}", name);
            }

            #[test]
            fn metric_computation_is_deterministic(
                values in prop::collection::vec(any::<f64>(), 1..1000)
            ) {
                let metric = AccuracyMetric::new();
                let input = MetricInput::new(values.clone());

                let result1 = metric.compute(&input);
                let result2 = metric.compute(&input);

                prop_assert_eq!(result1, result2);
            }
        }
    }

    // Group 5: Parameterized tests
    mod parameterized {
        use super::*;

        #[rstest]
        #[case("valid-name", true)]
        #[case("valid_name", true)]
        #[case("ValidName123", true)]
        #[case("", false)]
        #[case(" ", false)]
        #[case("name with spaces", false)]
        #[case("name\twith\ttabs", false)]
        #[case("name\nwith\nnewlines", false)]
        fn experiment_name_validation(#[case] name: &str, #[case] expected_valid: bool) {
            let result = validate_experiment_name(name);
            assert_eq!(result.is_ok(), expected_valid, "Name: {:?}", name);
        }

        #[rstest]
        #[case(ExperimentStatus::Created, ExperimentStatus::Running, true)]
        #[case(ExperimentStatus::Running, ExperimentStatus::Completed, true)]
        #[case(ExperimentStatus::Running, ExperimentStatus::Failed, true)]
        #[case(ExperimentStatus::Completed, ExperimentStatus::Running, false)]
        #[case(ExperimentStatus::Failed, ExperimentStatus::Running, false)]
        fn status_transition_validation(
            #[case] from: ExperimentStatus,
            #[case] to: ExperimentStatus,
            #[case] expected_valid: bool,
        ) {
            let result = validate_status_transition(from, to);
            assert_eq!(result.is_ok(), expected_valid);
        }
    }
}
```

### 6.3 Integration Testing

```rust
// tests/integration/experiment_integration_tests.rs

use llm_research_lab::test_utils::*;
use sqlx::PgPool;
use testcontainers::{clients::Cli, images::postgres::Postgres};

/// Integration test fixtures
struct TestFixture {
    db_pool: PgPool,
    redis_client: redis::Client,
    kafka_producer: rdkafka::producer::FutureProducer,
    _containers: TestContainers,
}

impl TestFixture {
    async fn new() -> Self {
        let containers = TestContainers::new();

        // Start PostgreSQL container
        let pg_container = containers.run(Postgres::default());
        let pg_port = pg_container.get_host_port_ipv4(5432);
        let pg_url = format!("postgres://postgres:postgres@localhost:{}/postgres", pg_port);

        // Run migrations
        let db_pool = PgPool::connect(&pg_url).await.unwrap();
        sqlx::migrate!("./migrations").run(&db_pool).await.unwrap();

        // Start Redis container
        let redis_container = containers.run(Redis::default());
        let redis_port = redis_container.get_host_port_ipv4(6379);
        let redis_client = redis::Client::open(format!("redis://localhost:{}", redis_port)).unwrap();

        // Start Kafka container
        let kafka_container = containers.run(Kafka::default());
        let kafka_port = kafka_container.get_host_port_ipv4(9092);
        let kafka_producer = create_kafka_producer(&format!("localhost:{}", kafka_port));

        Self {
            db_pool,
            redis_client,
            kafka_producer,
            _containers: containers,
        }
    }
}

#[tokio::test]
async fn test_experiment_lifecycle_integration() {
    // Arrange
    let fixture = TestFixture::new().await;
    let tracker = ExperimentTracker::new(
        fixture.db_pool.clone(),
        fixture.redis_client.clone(),
        fixture.kafka_producer.clone(),
    );

    // Act: Create experiment
    let experiment = tracker.create_experiment(CreateExperimentRequest {
        name: "integration-test".to_string(),
        project_id: ProjectId::new(),
        ..Default::default()
    }).await.unwrap();

    // Assert: Experiment created
    assert_eq!(experiment.status, ExperimentStatus::Created);

    // Act: Start run
    let run = tracker.start_run(experiment.id).await.unwrap();
    assert_eq!(run.status, RunStatus::Running);

    // Act: Log metrics
    for i in 0..100 {
        tracker.log_metric(run.id, MetricLog {
            name: "loss".to_string(),
            value: 1.0 / (i as f64 + 1.0),
            step: i,
        }).await.unwrap();
    }

    // Act: Complete run
    let completed_run = tracker.complete_run(run.id).await.unwrap();
    assert_eq!(completed_run.status, RunStatus::Completed);

    // Assert: Metrics persisted
    let metrics = tracker.get_run_metrics(run.id).await.unwrap();
    assert_eq!(metrics.len(), 100);

    // Assert: Event published to Kafka
    let events = consume_events(&fixture.kafka_producer, "experiment-events", 3).await;
    assert!(events.iter().any(|e| matches!(e, Event::ExperimentCreated { .. })));
    assert!(events.iter().any(|e| matches!(e, Event::RunStarted { .. })));
    assert!(events.iter().any(|e| matches!(e, Event::RunCompleted { .. })));
}

#[tokio::test]
async fn test_concurrent_runs_integration() {
    let fixture = TestFixture::new().await;
    let tracker = Arc::new(ExperimentTracker::new(
        fixture.db_pool.clone(),
        fixture.redis_client.clone(),
        fixture.kafka_producer.clone(),
    ));

    let experiment = tracker.create_experiment(CreateExperimentRequest {
        name: "concurrent-test".to_string(),
        ..Default::default()
    }).await.unwrap();

    // Start 50 concurrent runs
    let handles: Vec<_> = (0..50)
        .map(|_| {
            let tracker = tracker.clone();
            let exp_id = experiment.id;
            tokio::spawn(async move {
                let run = tracker.start_run(exp_id).await?;

                // Log some metrics
                for i in 0..10 {
                    tracker.log_metric(run.id, MetricLog {
                        name: "accuracy".to_string(),
                        value: rand::random(),
                        step: i,
                    }).await?;
                }

                tracker.complete_run(run.id).await
            })
        })
        .collect();

    // Wait for all runs to complete
    let results: Vec<_> = futures::future::join_all(handles).await;

    // Assert all succeeded
    assert!(results.iter().all(|r| r.as_ref().unwrap().is_ok()));

    // Assert all runs recorded
    let runs = tracker.list_runs(experiment.id).await.unwrap();
    assert_eq!(runs.len(), 50);
}

#[tokio::test]
async fn test_database_transaction_rollback() {
    let fixture = TestFixture::new().await;
    let tracker = ExperimentTracker::new(
        fixture.db_pool.clone(),
        fixture.redis_client.clone(),
        fixture.kafka_producer.clone(),
    );

    let experiment = tracker.create_experiment(CreateExperimentRequest {
        name: "rollback-test".to_string(),
        ..Default::default()
    }).await.unwrap();

    // Attempt operation that should fail and rollback
    let result = tracker.create_experiment_with_invalid_reference(
        CreateExperimentRequest {
            name: "should-rollback".to_string(),
            parent_id: Some(ExperimentId::new()), // Non-existent parent
            ..Default::default()
        }
    ).await;

    assert!(result.is_err());

    // Verify no partial data committed
    let experiments = tracker.list_experiments().await.unwrap();
    assert_eq!(experiments.len(), 1);
    assert_eq!(experiments[0].name, "rollback-test");
}
```

### 6.4 Mutation Testing

```toml
# mutants.toml

# Mutation testing configuration
[mutants]
timeout_multiplier = 3.0
jobs = 4

# Exclude test code and generated code
exclude_globs = [
    "**/tests/**",
    "**/benches/**",
    "**/target/**",
    "**/*.generated.rs",
]

# Focus on critical paths
include_globs = [
    "src/experiment/**",
    "src/metric/**",
    "src/security/**",
]

# Mutation operators
[operators]
arithmetic = true          # +, -, *, /
boundary = true            # <, <=, >, >=
boolean = true             # &&, ||, !
comparison = true          # ==, !=
constant = true            # Literal value mutations
control_flow = true        # if/else, loops
return_value = true        # Return mutations
```

```rust
// Mutation testing execution script
// scripts/run_mutation_tests.rs

use std::process::Command;

fn main() {
    println!("Running mutation tests...");

    let output = Command::new("cargo")
        .args([
            "mutants",
            "--timeout-multiplier", "3",
            "--jobs", "4",
            "--output", "target/mutants",
        ])
        .output()
        .expect("Failed to run mutation tests");

    println!("{}", String::from_utf8_lossy(&output.stdout));

    // Parse results
    let results = parse_mutation_results("target/mutants/results.json");

    // Calculate mutation score
    let score = results.killed as f64 / results.total as f64 * 100.0;

    println!("\nMutation Testing Results:");
    println!("  Total mutants: {}", results.total);
    println!("  Killed: {}", results.killed);
    println!("  Survived: {}", results.survived);
    println!("  Timeout: {}", results.timeout);
    println!("  Mutation Score: {:.1}%", score);

    // Fail if below threshold
    if score < 70.0 {
        eprintln!("\nMutation score {:.1}% is below threshold of 70%", score);
        std::process::exit(1);
    }
}
```

### 6.5 End-to-End Testing

```rust
// tests/e2e/experiment_e2e_tests.rs

use cucumber::{given, when, then, World};
use reqwest::Client;

#[derive(Debug, Default, World)]
pub struct ExperimentWorld {
    client: Option<Client>,
    base_url: String,
    auth_token: Option<String>,
    last_response: Option<Response>,
    experiment_id: Option<String>,
    run_id: Option<String>,
}

#[given("I am authenticated as a researcher")]
async fn authenticate_researcher(world: &mut ExperimentWorld) {
    let client = Client::new();
    let response = client
        .post(&format!("{}/auth/login", world.base_url))
        .json(&json!({
            "email": "researcher@example.com",
            "password": "test-password"
        }))
        .send()
        .await
        .unwrap();

    let body: AuthResponse = response.json().await.unwrap();
    world.auth_token = Some(body.token);
    world.client = Some(client);
}

#[when(expr = "I create an experiment named {string}")]
async fn create_experiment(world: &mut ExperimentWorld, name: String) {
    let response = world.client.as_ref().unwrap()
        .post(&format!("{}/api/v1/experiments", world.base_url))
        .bearer_auth(world.auth_token.as_ref().unwrap())
        .json(&json!({
            "name": name,
            "description": "E2E test experiment"
        }))
        .send()
        .await
        .unwrap();

    world.last_response = Some(response);
}

#[then("the experiment should be created successfully")]
async fn experiment_created(world: &mut ExperimentWorld) {
    let response = world.last_response.take().unwrap();
    assert_eq!(response.status(), 201);

    let body: ExperimentResponse = response.json().await.unwrap();
    world.experiment_id = Some(body.id);
    assert_eq!(body.status, "created");
}

#[when("I start a new run")]
async fn start_run(world: &mut ExperimentWorld) {
    let response = world.client.as_ref().unwrap()
        .post(&format!(
            "{}/api/v1/experiments/{}/runs",
            world.base_url,
            world.experiment_id.as_ref().unwrap()
        ))
        .bearer_auth(world.auth_token.as_ref().unwrap())
        .json(&json!({}))
        .send()
        .await
        .unwrap();

    world.last_response = Some(response);
}

#[then("the run should be started successfully")]
async fn run_started(world: &mut ExperimentWorld) {
    let response = world.last_response.take().unwrap();
    assert_eq!(response.status(), 201);

    let body: RunResponse = response.json().await.unwrap();
    world.run_id = Some(body.id);
    assert_eq!(body.status, "running");
}

#[when(expr = "I log {int} metrics")]
async fn log_metrics(world: &mut ExperimentWorld, count: i32) {
    for i in 0..count {
        let response = world.client.as_ref().unwrap()
            .post(&format!(
                "{}/api/v1/runs/{}/metrics",
                world.base_url,
                world.run_id.as_ref().unwrap()
            ))
            .bearer_auth(world.auth_token.as_ref().unwrap())
            .json(&json!({
                "name": "loss",
                "value": 1.0 / (i as f64 + 1.0),
                "step": i
            }))
            .send()
            .await
            .unwrap();

        assert_eq!(response.status(), 204);
    }
}

#[when("I complete the run")]
async fn complete_run(world: &mut ExperimentWorld) {
    let response = world.client.as_ref().unwrap()
        .patch(&format!(
            "{}/api/v1/runs/{}",
            world.base_url,
            world.run_id.as_ref().unwrap()
        ))
        .bearer_auth(world.auth_token.as_ref().unwrap())
        .json(&json!({
            "status": "completed"
        }))
        .send()
        .await
        .unwrap();

    world.last_response = Some(response);
}

#[then(expr = "I should be able to retrieve all {int} metrics")]
async fn verify_metrics(world: &mut ExperimentWorld, expected_count: i32) {
    let response = world.client.as_ref().unwrap()
        .get(&format!(
            "{}/api/v1/runs/{}/metrics",
            world.base_url,
            world.run_id.as_ref().unwrap()
        ))
        .bearer_auth(world.auth_token.as_ref().unwrap())
        .send()
        .await
        .unwrap();

    assert_eq!(response.status(), 200);

    let body: MetricsResponse = response.json().await.unwrap();
    assert_eq!(body.metrics.len(), expected_count as usize);
}

// Feature file: features/experiment.feature
/*
Feature: Experiment Lifecycle

  Scenario: Complete experiment workflow
    Given I am authenticated as a researcher
    When I create an experiment named "e2e-test-experiment"
    Then the experiment should be created successfully
    When I start a new run
    Then the run should be started successfully
    When I log 100 metrics
    And I complete the run
    Then I should be able to retrieve all 100 metrics
*/
```

---

## 7. Documentation Standards

### 7.1 API Documentation

#### 7.1.1 Rustdoc Configuration

**Crate-Level Attributes** (`src/lib.rs`):

```rust
#![doc = include_str!("../README.md")]
#![doc(html_logo_url = "https://llm-research-lab.io/logo.svg")]
#![doc(html_favicon_url = "https://llm-research-lab.io/favicon.ico")]
#![warn(missing_docs)]
#![warn(rustdoc::broken_intra_doc_links)]
#![warn(rustdoc::private_intra_doc_links)]
#![deny(rustdoc::invalid_codeblock_attributes)]
```

**Documentation Build Configuration** (`Cargo.toml`):

```toml
[package.metadata.docs.rs]
all-features = true
rustdoc-args = ["--cfg", "docsrs"]
targets = ["x86_64-unknown-linux-gnu"]

[features]
default = []
full = ["postgres", "s3", "metrics"]
```

#### 7.1.2 Public API Documentation Patterns

**Module Documentation**:

```rust
//! # Experiment Tracking Module
//!
//! Provides core functionality for ML experiment lifecycle management.
//!
//! ## Core Components
//!
//! - [`ExperimentTracker`]: Primary interface for experiment operations
//! - [`ExperimentRun`]: Individual experiment execution tracking
//! - [`MetricCollector`]: Time-series metric aggregation
//!
//! ## Quick Start
//!
//! ```rust
//! use llm_research_lab::experiment::ExperimentTracker;
//!
//! # async fn example() -> Result<(), Box<dyn std::error::Error>> {
//! let tracker = ExperimentTracker::new(db_pool).await?;
//! let experiment = tracker.create("my-experiment").await?;
//! # Ok(())
//! # }
//! ```
```

**Struct Documentation**:

```rust
/// Tracks ML experiments with metric logging and artifact storage.
///
/// # Thread Safety
///
/// All methods use `&self` and are safe for concurrent access via `Arc<ExperimentTracker>`.
///
/// # Examples
///
/// ```rust
/// # use llm_research_lab::experiment::*;
/// # async fn example(db: DbPool) -> Result<(), ExperimentError> {
/// let tracker = ExperimentTracker::new(db).await?;
/// let run = tracker.start_run("exp-1").await?;
/// tracker.log_metric(run.id, "loss", 0.42).await?;
/// # Ok(())
/// # }
/// ```
///
/// # Errors
///
/// Returns [`ExperimentError`] for validation failures, not-found conditions,
/// or database errors.
#[derive(Clone)]
pub struct ExperimentTracker { /* ... */ }
```

#### 7.1.3 Doc Test Requirements

**Coverage Mandate**: All public APIs require at least one passing doc test.

**Doc Test Patterns**:

```rust
/// Logs a metric value for an experiment run.
///
/// # Arguments
///
/// * `run_id` - Unique run identifier
/// * `name` - Metric name (e.g., "loss", "accuracy")
/// * `value` - Metric value
///
/// # Examples
///
/// ```rust
/// # use llm_research_lab::experiment::*;
/// # async fn test(tracker: &ExperimentTracker) -> Result<(), ExperimentError> {
/// tracker.log_metric("run_123", "loss", 0.25).await?;
/// tracker.log_metric("run_123", "accuracy", 0.95).await?;
/// # Ok(())
/// # }
/// ```
///
/// ```rust,should_panic
/// # use llm_research_lab::experiment::*;
/// # async fn test(tracker: &ExperimentTracker) {
/// // Invalid run ID should panic
/// tracker.log_metric("invalid", "loss", 0.0).await.unwrap();
/// # }
/// ```
pub async fn log_metric(&self, run_id: &str, name: &str, value: f64)
    -> Result<(), ExperimentError> { /* ... */ }
```

**Test Execution**:

```bash
# Run all doc tests
cargo test --doc

# Test specific module docs
cargo test --doc experiment

# Check for broken doc links
cargo doc --no-deps --document-private-items
```

#### 7.1.4 OpenAPI/Swagger Synchronization

**Automated Schema Generation** (`build.rs`):

```rust
use utoipa::OpenApi;

#[derive(OpenApi)]
#[openapi(
    info(
        title = "LLM Research Lab API",
        version = "1.0.0",
        description = "ML experiment tracking and management"
    ),
    paths(
        api::experiments::create,
        api::experiments::get,
        api::runs::start,
        api::metrics::log,
    ),
    components(schemas(
        Experiment,
        ExperimentRun,
        MetricLog,
        ApiError,
    )),
    tags(
        (name = "experiments", description = "Experiment management"),
        (name = "runs", description = "Run tracking"),
        (name = "metrics", description = "Metric logging")
    )
)]
struct ApiDoc;

fn main() {
    let doc = ApiDoc::openapi();
    std::fs::write("openapi.json", doc.to_pretty_json().unwrap()).unwrap();
}
```

**CI/CD Validation** (`.github/workflows/docs.yml`):

```yaml
- name: Validate OpenAPI Sync
  run: |
    cargo build
    diff <(jq -S . openapi.json) <(jq -S . docs/api/openapi.json)
```

---

### 7.2 Architecture Documentation

#### 7.2.1 ADR Template

**File**: `docs/adr/NNNN-title-slug.md`

```markdown
# ADR-NNNN: [Title]

**Status**: [Proposed | Accepted | Deprecated | Superseded by ADR-XXXX]
**Date**: YYYY-MM-DD
**Deciders**: [Names/Roles]
**Context**: [Technical Story | Issue #NNN]

## Context and Problem Statement

[Describe the context and the problem requiring a decision. Include constraints,
requirements, and business drivers.]

## Decision Drivers

- [Driver 1: e.g., Performance requirements]
- [Driver 2: e.g., Developer experience]
- [Driver 3: e.g., Cost constraints]

## Considered Options

1. **Option A**: [Name]
2. **Option B**: [Name]
3. **Option C**: [Name]

## Decision Outcome

**Chosen Option**: "[Option X]"

**Rationale**: [Why this option was selected over alternatives]

### Implementation Plan

1. [Step 1]
2. [Step 2]
3. [Step 3]

### Consequences

**Positive**:
- [Benefit 1]
- [Benefit 2]

**Negative**:
- [Trade-off 1]
- [Trade-off 2]

**Neutral**:
- [Consideration 1]

### Risks and Mitigations

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|---------|------------|
| [Risk 1] | Medium | High | [Mitigation strategy] |

## Detailed Analysis

### Option A: [Name]

**Description**: [How it works]

**Pros**:
- [Pro 1]

**Cons**:
- [Con 1]

**Cost**: [Implementation cost estimate]

### Option B: [Name]

[Same structure as Option A]

## References

- [Link to RFC/Design Doc]
- [Related ADR-XXXX]
- [External resource]

## Changelog

- YYYY-MM-DD: Status changed to Accepted
- YYYY-MM-DD: Initial draft
```

#### 7.2.2 C4 Diagram Standards

**Level 1: System Context** (`docs/architecture/c4/context.puml`):

```plantuml
@startuml
!include https://raw.githubusercontent.com/plantuml-stdlib/C4-PlantUML/master/C4_Context.puml

LAYOUT_WITH_LEGEND()

title System Context - LLM Research Lab

Person(researcher, "ML Researcher", "Tracks experiments and analyzes results")
Person(engineer, "ML Engineer", "Deploys models and manages infrastructure")

System(llm_lab, "LLM Research Lab", "Experiment tracking, metric collection, and model versioning")

System_Ext(s3, "Object Storage", "Stores artifacts and model checkpoints")
System_Ext(postgres, "PostgreSQL", "Metadata and time-series storage")
System_Ext(auth, "Auth Provider", "User authentication (OAuth2/OIDC)")

Rel(researcher, llm_lab, "Tracks experiments", "HTTPS/gRPC")
Rel(engineer, llm_lab, "Deploys models", "HTTPS/CLI")
Rel(llm_lab, s3, "Stores artifacts", "S3 API")
Rel(llm_lab, postgres, "Persists metadata", "PostgreSQL wire protocol")
Rel(llm_lab, auth, "Authenticates users", "OIDC")

@enduml
```

**Level 2: Container Diagram** (`docs/architecture/c4/container.puml`):

```plantuml
@startuml
!include https://raw.githubusercontent.com/plantuml-stdlib/C4-PlantUML/master/C4_Container.puml

title Container Diagram - LLM Research Lab

Person(user, "User")

System_Boundary(llm_lab, "LLM Research Lab") {
    Container(web_ui, "Web UI", "React + TypeScript", "Dashboard for experiment visualization")
    Container(api_gateway, "API Gateway", "Rust/Axum", "REST and gRPC endpoints")
    Container(experiment_service, "Experiment Service", "Rust", "Core experiment tracking logic")
    Container(metric_service, "Metric Service", "Rust", "Time-series metric aggregation")
    Container(artifact_service, "Artifact Service", "Rust", "Content-addressable storage")
    ContainerDb(postgres, "Metadata DB", "PostgreSQL 15", "Experiments, runs, metrics")
    ContainerDb(timescale, "Time-Series DB", "TimescaleDB", "High-frequency metrics")
}

System_Ext(s3, "S3 Storage")

Rel(user, web_ui, "Uses", "HTTPS")
Rel(web_ui, api_gateway, "API calls", "HTTPS/REST")
Rel(api_gateway, experiment_service, "Routes requests", "gRPC")
Rel(api_gateway, metric_service, "Routes requests", "gRPC")
Rel(experiment_service, postgres, "Reads/writes", "SQL")
Rel(metric_service, timescale, "Writes metrics", "SQL")
Rel(artifact_service, s3, "Stores objects", "S3 API")

@enduml
```

**Level 3: Component Diagram** (for critical services only).

**Rendering**:

```bash
# Install PlantUML
brew install plantuml

# Generate diagrams
plantuml -tsvg docs/architecture/c4/*.puml -o ../diagrams
```

#### 7.2.3 Sequence Diagrams

**Critical Path Documentation** (`docs/architecture/sequences/experiment-creation.puml`):

```plantuml
@startuml
title Experiment Creation Flow

actor User
participant "API Gateway" as API
participant "Experiment Service" as Exp
participant "PostgreSQL" as DB
participant "Metric Service" as Metric
participant "Message Bus" as MQ

User -> API: POST /v1/experiments
activate API

API -> API: Validate JWT
API -> API: Check rate limit

API -> Exp: CreateExperiment(request)
activate Exp

Exp -> DB: BEGIN TRANSACTION
Exp -> DB: INSERT INTO experiments
DB --> Exp: experiment_id

Exp -> DB: INSERT INTO experiment_tags
Exp -> DB: COMMIT

Exp -> MQ: Publish ExperimentCreated event
activate MQ
MQ --> Exp: ack
deactivate MQ

Exp --> API: Experiment response
deactivate Exp

API --> User: 201 Created
deactivate API

MQ -> Metric: ExperimentCreated event
activate Metric
Metric -> DB: Initialize metric tables
deactivate Metric

@enduml
```

---

### 7.3 Operational Runbooks

#### 7.3.1 Runbook Template

**File**: `docs/runbooks/CATEGORY-incident-name.md`

```markdown
# Runbook: [Incident/Task Name]

**Category**: [Incident Response | Deployment | Maintenance]
**Severity**: [P1 Critical | P2 High | P3 Medium | P4 Low]
**Alert**: `[AlertName]` (if incident-driven)
**Estimated Time**: [X minutes]

## Overview

[Brief description of the issue, task, or operational procedure]

**When to Use This Runbook**:
- [Trigger condition 1]
- [Trigger condition 2]

## Prerequisites

- [ ] Access to production Kubernetes cluster
- [ ] Database credentials (stored in 1Password vault: `prod-db`)
- [ ] On-call rotation verified via PagerDuty

## Symptoms and Indicators

**Primary Indicators**:
- [Metric/log pattern indicating issue]
- [User-reported symptom]

**Secondary Indicators**:
- [Related metric degradation]
- [Downstream system impact]

## Immediate Actions (First 5 Minutes)

### 1. Assess Severity

```bash
# Check service health
kubectl get pods -n llm-research-lab -l app=experiment-service

# Check recent error rate
kubectl logs -n llm-research-lab -l app=experiment-service --since=5m | grep ERROR | wc -l
```

**Decision Tree**:
- Error rate > 10%: Escalate to P1, page on-call
- Error rate 1-10%: Continue investigation (P2)
- Error rate < 1%: Monitor (P3)

### 2. Collect Diagnostic Data

```bash
# Capture current state
kubectl describe pods -n llm-research-lab -l app=experiment-service > /tmp/pod-state.txt

# Export recent logs
kubectl logs -n llm-research-lab -l app=experiment-service --since=15m > /tmp/service-logs.txt

# Check metrics snapshot
curl -s http://experiment-service:9090/metrics > /tmp/metrics-snapshot.txt
```

## Diagnostic Procedures

### Scenario A: [Common Issue Pattern]

**Symptoms**:
- [Specific symptom]

**Root Cause**:
- [Likely cause]

**Diagnosis**:

```bash
# Command to confirm root cause
kubectl exec -it experiment-service-0 -- curl localhost:9090/debug/pprof/heap > heap.prof
```

**Resolution**:

```bash
# Step-by-step fix
kubectl scale deployment experiment-service --replicas=5
```

**Validation**:

```bash
# Verify fix
curl -f https://api.llm-research-lab.io/health || echo "Still failing"
```

### Scenario B: [Another Common Pattern]

[Same structure as Scenario A]

## Resolution Steps

### Step 1: [Action Name]

**Objective**: [What this step achieves]

**Commands**:

```bash
# Production command
kubectl apply -f k8s/experiment-service-scaled.yaml
```

**Expected Outcome**: [What should happen]

**Rollback Procedure** (if needed):

```bash
kubectl rollout undo deployment/experiment-service
```

### Step 2: [Next Action]

[Same structure]

## Escalation Path

**When to Escalate**:
- Issue persists > 15 minutes
- Multiple services affected
- Data loss risk identified

**Escalation Contacts**:
1. On-call SRE: via PagerDuty
2. Engineering Manager: Slack #platform-incidents
3. VP Engineering: For customer-impacting P1 incidents

**Escalation Template**:

```
INCIDENT: [Name]
SEVERITY: [P1/P2]
STARTED: [Timestamp]
SYMPTOMS: [Brief description]
ACTIONS TAKEN: [What's been tried]
CURRENT STATE: [Status]
```

## Post-Resolution

### Verification Checklist

- [ ] Service health dashboard green
- [ ] Error rate returned to baseline (< 0.1%)
- [ ] Customer-facing metrics normal
- [ ] All alerts resolved

### Communication

**Internal**:

```
#platform-incidents: RESOLVED - [Incident name]
Duration: [X minutes]
Impact: [Brief impact statement]
RCA: [Link to incident doc]
```

**External** (if customer-impacting):

```
Subject: [Resolved] Service Disruption - [Timestamp]

Dear Customers,

We have resolved the service disruption that occurred at [time].

Impact: [What was affected]
Duration: [How long]
Root Cause: [High-level explanation]

We apologize for any inconvenience. A detailed post-mortem will be shared within 48 hours.
```

### Post-Incident Actions

- [ ] Create incident ticket: `INC-YYYY-NNNN`
- [ ] Schedule post-mortem within 48 hours
- [ ] Document timeline in incident tracker
- [ ] Update runbook if new patterns discovered

## Prevention and Monitoring

**Preventive Measures**:
- [Action to prevent recurrence]
- [Monitoring improvement]

**New Alerts to Add**:

```yaml
# Prometheus alert rule
- alert: ExperimentServiceLatencyP95High
  expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 0.5
  for: 5m
  labels:
    severity: warning
  annotations:
    summary: "Experiment service P95 latency high"
```

## References

- [Link to related ADR]
- [Monitoring dashboard URL]
- [Relevant documentation]

## Changelog

- YYYY-MM-DD: Updated with new diagnostic command (Jane Doe)
- YYYY-MM-DD: Initial version (John Smith)
```

#### 7.3.2 Incident Response Procedures

**Severity Classification**:

| Level | Description | Response Time | Example |
|-------|-------------|---------------|---------|
| P1 | Critical service outage | < 15 min | API completely down |
| P2 | Major degradation | < 1 hour | High error rate (> 10%) |
| P3 | Minor issue | < 4 hours | Slow dashboard load |
| P4 | Cosmetic/low impact | < 24 hours | UI typo |

**Standard Operating Procedure**:

1. **Alert Receipt** - Acknowledge in PagerDuty within 5 minutes
2. **Initial Assessment** - Determine severity (use decision tree)
3. **Incident Declaration** - Create Slack channel `#inc-YYYY-MM-DD-name`
4. **Investigation** - Follow relevant runbook
5. **Resolution** - Implement fix and validate
6. **Communication** - Update stakeholders
7. **Post-Mortem** - Schedule within 48 hours (P1/P2 only)

#### 7.3.3 Deployment Procedures

**Standard Deployment Runbook** (`docs/runbooks/deploy-production.md`):

```markdown
# Production Deployment Procedure

## Pre-Deployment Checklist

- [ ] All tests passing in CI/CD
- [ ] Security scan completed (no critical vulnerabilities)
- [ ] Performance benchmarks within 5% of baseline
- [ ] Database migrations tested in staging
- [ ] Rollback plan documented
- [ ] Deployment window approved (check #ops-calendar)
- [ ] On-call engineer notified

## Deployment Steps

### 1. Pre-Deployment Smoke Test

```bash
# Run staging validation
./scripts/validate-staging.sh

# Expected output: "All checks passed"
```

### 2. Database Migration (if required)

```bash
# Backup production database
./scripts/backup-db.sh production

# Run migration with monitoring
kubectl exec -it postgres-0 -- psql < migrations/V023__add_experiment_tags.sql

# Validate migration
kubectl exec -it postgres-0 -- psql -c "SELECT version FROM schema_migrations ORDER BY version DESC LIMIT 1;"
```

### 3. Deploy Application

```bash
# Deploy with gradual rollout
kubectl set image deployment/experiment-service \
  experiment-service=llm-lab/experiment-service:v1.2.3

# Monitor rollout
kubectl rollout status deployment/experiment-service --timeout=10m
```

### 4. Post-Deployment Validation

```bash
# Health check
for i in {1..5}; do
  curl -f https://api.llm-research-lab.io/health && echo " - Check $i passed" || exit 1
  sleep 2
done

# Smoke test critical endpoints
./scripts/smoke-test-production.sh
```

### 5. Monitoring

Monitor for 15 minutes:
- Error rate dashboard: https://grafana.llm-lab.io/d/errors
- Latency P99 < 200ms
- No new error patterns in logs

## Rollback Procedure

**Trigger**: Error rate > 1% OR P99 latency > 500ms for 5 minutes

```bash
# Immediate rollback
kubectl rollout undo deployment/experiment-service

# Verify rollback
kubectl rollout status deployment/experiment-service

# Validate service health
curl -f https://api.llm-research-lab.io/health
```

## Communication

**Pre-Deployment** (30 min before):

```
#platform-updates: Deployment starting at [HH:MM UTC]
Service: experiment-service v1.2.3
Expected duration: 10 minutes
Potential impact: None (rolling update)
```

**Post-Deployment**:

```
#platform-updates: Deployment complete
Service: experiment-service v1.2.3
Status: Successful
Validation: All checks passed
```
```

---
## 8. Review Processes

### 8.1 Code Review Checklist

```yaml
# code_review_checklist.yaml

categories:
  correctness:
    - id: COR-001
      item: "All error cases handled appropriately"
      required: true

    - id: COR-002
      item: "Edge cases considered and tested"
      required: true

    - id: COR-003
      item: "No unwrap/expect on fallible operations"
      required: true

    - id: COR-004
      item: "Async operations properly awaited"
      required: true

    - id: COR-005
      item: "Resources properly cleaned up (Drop, RAII)"
      required: true

  security:
    - id: SEC-001
      item: "No hardcoded secrets or credentials"
      required: true

    - id: SEC-002
      item: "Input validation on all external data"
      required: true

    - id: SEC-003
      item: "SQL queries use parameterized statements"
      required: true

    - id: SEC-004
      item: "Sensitive data not logged"
      required: true

    - id: SEC-005
      item: "Authorization checks on protected resources"
      required: true

  performance:
    - id: PERF-001
      item: "No N+1 query patterns"
      required: true

    - id: PERF-002
      item: "Appropriate use of async/await"
      required: true

    - id: PERF-003
      item: "Collections pre-allocated when size known"
      required: false

    - id: PERF-004
      item: "No unnecessary clones"
      required: false

    - id: PERF-005
      item: "Database indexes exist for query patterns"
      required: true

  maintainability:
    - id: MAINT-001
      item: "Code follows project style guide"
      required: true

    - id: MAINT-002
      item: "Functions are focused and single-purpose"
      required: true

    - id: MAINT-003
      item: "Public APIs are documented"
      required: true

    - id: MAINT-004
      item: "No commented-out code"
      required: true

    - id: MAINT-005
      item: "Magic numbers replaced with constants"
      required: true

  testing:
    - id: TEST-001
      item: "Unit tests cover new functionality"
      required: true

    - id: TEST-002
      item: "Error paths have test coverage"
      required: true

    - id: TEST-003
      item: "Integration tests for service boundaries"
      required: true

    - id: TEST-004
      item: "No flaky tests introduced"
      required: true

    - id: TEST-005
      item: "Test names describe behavior"
      required: true
```

### 8.2 Architecture Review

```yaml
# architecture_review_process.yaml

triggers:
  - "New service introduction"
  - "Changes to service boundaries"
  - "New external dependency"
  - "Database schema changes affecting multiple services"
  - "Security-critical changes"
  - "Changes affecting SLA"

review_board:
  required_participants:
    - "Principal Engineer"
    - "Security Engineer"
    - "SRE Representative"
  optional_participants:
    - "Product Manager"
    - "Team Lead"

review_criteria:
  scalability:
    - "Can the solution scale to 10x current load?"
    - "Are there horizontal scaling bottlenecks?"
    - "Is data partitioning strategy defined?"

  reliability:
    - "What is the failure domain?"
    - "How does the system degrade under failure?"
    - "What is the recovery time objective?"
    - "Are circuit breakers implemented?"

  security:
    - "What is the threat model?"
    - "How is authentication/authorization handled?"
    - "What data needs encryption?"
    - "Are there compliance implications?"

  operability:
    - "How is the system monitored?"
    - "What alerts are needed?"
    - "How is the system debugged?"
    - "What runbooks are needed?"

  cost:
    - "What is the infrastructure cost?"
    - "Are there cost optimization opportunities?"
    - "What is the cost scaling factor?"

deliverables:
  - "Architecture Decision Record (ADR)"
  - "Updated system diagrams"
  - "Capacity planning document"
  - "Security review sign-off"
  - "Runbook drafts"

timeline:
  request_submission: "Day 0"
  initial_review: "Day 1-2"
  revision_period: "Day 3-5"
  final_approval: "Day 6-7"
  implementation_start: "After approval"
```

### 8.3 Security Review

```yaml
# security_review_process.yaml

classification:
  critical:
    description: "Authentication, authorization, cryptography changes"
    required_reviewers: 2
    sla: "24 hours"

  high:
    description: "External API integration, data handling changes"
    required_reviewers: 1
    sla: "48 hours"

  medium:
    description: "Internal service changes, configuration changes"
    required_reviewers: 1
    sla: "72 hours"

  low:
    description: "Documentation, non-functional changes"
    required_reviewers: 0
    sla: "N/A"

review_areas:
  authentication:
    - "Token validation logic"
    - "Session management"
    - "Password handling"
    - "MFA implementation"

  authorization:
    - "Permission checks"
    - "Role definitions"
    - "Resource ownership"
    - "Cross-tenant isolation"

  data_protection:
    - "Encryption at rest"
    - "Encryption in transit"
    - "Key management"
    - "Data retention"

  input_validation:
    - "Injection prevention"
    - "Size limits"
    - "Type validation"
    - "Encoding handling"

  audit_logging:
    - "Security events captured"
    - "PII handling in logs"
    - "Log integrity"
    - "Retention compliance"

tools:
  automated:
    - "Semgrep security rules"
    - "cargo-audit"
    - "Trivy container scanning"
    - "OWASP ZAP"

  manual:
    - "Threat modeling"
    - "Code walkthrough"
    - "Penetration testing"
```

---

## 9. Iteration Framework

### 9.1 Feedback Integration Process

```yaml
# feedback_integration.yaml

sources:
  automated:
    - source: "CI/CD Pipeline"
      signals:
        - "Test failures"
        - "Coverage regression"
        - "Performance regression"
        - "Security scan findings"
      integration: "Automatic ticket creation"

    - source: "Production Monitoring"
      signals:
        - "Error rate increase"
        - "Latency regression"
        - "Resource utilization"
      integration: "Alert → Investigation → Fix"

    - source: "User Analytics"
      signals:
        - "Feature usage patterns"
        - "Error encounters"
        - "Performance complaints"
      integration: "Weekly review → Prioritization"

  manual:
    - source: "Code Review"
      signals:
        - "Design concerns"
        - "Technical debt"
        - "Missing tests"
      integration: "Must-fix before merge"

    - source: "Architecture Review"
      signals:
        - "Scalability concerns"
        - "Security gaps"
        - "Operational issues"
      integration: "ADR updates → Implementation"

    - source: "User Feedback"
      signals:
        - "Feature requests"
        - "Bug reports"
        - "Usability issues"
      integration: "Triage → Backlog → Sprint"

iteration_cycles:
  daily:
    - "CI/CD feedback review"
    - "Critical bug triage"
    - "Blocker resolution"

  weekly:
    - "Performance metrics review"
    - "Security scan review"
    - "Technical debt assessment"

  bi_weekly:
    - "Sprint retrospective"
    - "Architecture review"
    - "Capacity planning"

  monthly:
    - "Major version planning"
    - "Dependency updates"
    - "Documentation audit"

  quarterly:
    - "Security audit"
    - "Performance baseline"
    - "Technology radar review"
```

### 9.2 Continuous Improvement Metrics

```yaml
# improvement_metrics.yaml

code_quality:
  metrics:
    - name: "Code Coverage"
      target: ">= 85%"
      trend: "Increasing"
      measurement: "Tarpaulin coverage report"

    - name: "Mutation Score"
      target: ">= 70%"
      trend: "Increasing"
      measurement: "cargo-mutants output"

    - name: "Technical Debt Ratio"
      target: "< 2%"
      trend: "Decreasing"
      measurement: "SonarQube analysis"

    - name: "Cyclomatic Complexity"
      target: "< 15 per function"
      trend: "Stable"
      measurement: "Rust analyzer"

performance:
  metrics:
    - name: "API P99 Latency"
      target: "< 100ms"
      trend: "Stable/Decreasing"
      measurement: "Prometheus metrics"

    - name: "Throughput"
      target: "> 5000 req/s"
      trend: "Increasing"
      measurement: "Load test results"

    - name: "Error Rate"
      target: "< 0.1%"
      trend: "Decreasing"
      measurement: "Prometheus metrics"

reliability:
  metrics:
    - name: "Uptime"
      target: "> 99.9%"
      trend: "Stable"
      measurement: "Synthetic monitoring"

    - name: "MTTR"
      target: "< 30 minutes"
      trend: "Decreasing"
      measurement: "Incident tracking"

    - name: "Change Failure Rate"
      target: "< 5%"
      trend: "Decreasing"
      measurement: "Deployment tracking"

developer_experience:
  metrics:
    - name: "Build Time"
      target: "< 5 minutes"
      trend: "Stable"
      measurement: "CI metrics"

    - name: "Test Execution Time"
      target: "< 10 minutes"
      trend: "Stable"
      measurement: "CI metrics"

    - name: "Time to First Review"
      target: "< 4 hours"
      trend: "Decreasing"
      measurement: "GitHub metrics"
```

### 9.3 Technical Debt Management

```yaml
# technical_debt_management.yaml

classification:
  architecture:
    description: "Structural issues requiring significant refactoring"
    examples:
      - "Monolith components needing extraction"
      - "Missing abstraction layers"
      - "Circular dependencies"
    priority: "High"
    budget: "20% of sprint capacity"

  code:
    description: "Code-level issues affecting maintainability"
    examples:
      - "Complex functions needing refactoring"
      - "Missing error handling"
      - "Code duplication"
    priority: "Medium"
    budget: "10% of sprint capacity"

  test:
    description: "Testing gaps and quality issues"
    examples:
      - "Missing test coverage"
      - "Flaky tests"
      - "Missing integration tests"
    priority: "Medium"
    budget: "10% of sprint capacity"

  documentation:
    description: "Documentation gaps and outdated content"
    examples:
      - "Missing API documentation"
      - "Outdated architecture diagrams"
      - "Missing runbooks"
    priority: "Low"
    budget: "5% of sprint capacity"

  dependency:
    description: "Outdated or problematic dependencies"
    examples:
      - "Security vulnerabilities"
      - "Deprecated libraries"
      - "Version conflicts"
    priority: "High"
    budget: "Addressed immediately"

tracking:
  tool: "GitHub Issues with 'tech-debt' label"
  review_cadence: "Weekly"

  fields:
    - "Category"
    - "Impact (1-5)"
    - "Effort (1-5)"
    - "Risk if not addressed"
    - "Proposed solution"

  prioritization:
    formula: "Priority = Impact × (1/Effort) × Risk"
    threshold: "Address items with score > 3"

paydown_strategies:
  boy_scout_rule:
    description: "Leave code better than you found it"
    application: "Small improvements during feature work"

  dedicated_sprints:
    description: "Periodic sprints focused on debt reduction"
    frequency: "Every 4th sprint"

  continuous_allocation:
    description: "Reserve capacity for debt in every sprint"
    allocation: "15-20% of velocity"

  opportunistic:
    description: "Address debt when touching related code"
    application: "During bug fixes and features"
```

---

## 10. Compliance & Audit

### 10.1 Compliance Requirements

```yaml
# compliance_requirements.yaml

frameworks:
  soc2:
    applicable: true
    controls:
      - id: "CC6.1"
        description: "Logical and physical access controls"
        implementation: "RBAC, mTLS, API keys"
        evidence: "Access logs, RBAC policies"

      - id: "CC6.6"
        description: "System boundaries protection"
        implementation: "Network policies, WAF"
        evidence: "Network diagrams, firewall rules"

      - id: "CC7.1"
        description: "Change management"
        implementation: "GitOps, PR reviews"
        evidence: "Commit history, review records"

      - id: "CC7.2"
        description: "System monitoring"
        implementation: "Prometheus, Grafana, alerting"
        evidence: "Dashboards, alert history"

  gdpr:
    applicable: true
    controls:
      - id: "Art.5"
        description: "Data processing principles"
        implementation: "Data minimization, purpose limitation"
        evidence: "Data flow diagrams, privacy impact assessment"

      - id: "Art.17"
        description: "Right to erasure"
        implementation: "Data deletion API, retention policies"
        evidence: "Deletion logs, policy documentation"

      - id: "Art.32"
        description: "Security of processing"
        implementation: "Encryption, access controls"
        evidence: "Security architecture docs, audit logs"

      - id: "Art.33"
        description: "Breach notification"
        implementation: "Incident response procedures"
        evidence: "Incident runbooks, notification templates"

  hipaa:
    applicable: "If handling PHI"
    controls:
      - id: "164.312(a)(1)"
        description: "Access control"
        implementation: "RBAC, MFA"
        evidence: "Access policies, authentication logs"

      - id: "164.312(b)"
        description: "Audit controls"
        implementation: "Comprehensive audit logging"
        evidence: "Audit log exports"

      - id: "164.312(c)(1)"
        description: "Integrity"
        implementation: "SHA-256 checksums, immutable logs"
        evidence: "Integrity verification reports"

      - id: "164.312(e)(1)"
        description: "Transmission security"
        implementation: "TLS 1.3, mTLS"
        evidence: "TLS configuration, certificate management"

audit_schedule:
  internal:
    frequency: "Quarterly"
    scope: "Security controls, access reviews"
    responsible: "Security team"

  external:
    frequency: "Annually"
    scope: "SOC2 Type II"
    responsible: "External auditor"

  penetration_test:
    frequency: "Annually"
    scope: "Full application and infrastructure"
    responsible: "Third-party security firm"
```

### 10.2 Audit Trail Implementation

```rust
// audit/audit_trail.rs

use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};

/// Comprehensive audit event
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AuditEvent {
    /// Unique event identifier
    pub id: AuditEventId,

    /// When the event occurred
    pub timestamp: DateTime<Utc>,

    /// Type of event
    pub event_type: AuditEventType,

    /// Actor who performed the action
    pub actor: AuditActor,

    /// Resource affected
    pub resource: AuditResource,

    /// Action performed
    pub action: AuditAction,

    /// Outcome of the action
    pub outcome: AuditOutcome,

    /// Request context
    pub context: AuditContext,

    /// Additional metadata
    pub metadata: serde_json::Value,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum AuditEventType {
    Authentication,
    Authorization,
    DataAccess,
    DataModification,
    DataDeletion,
    ConfigurationChange,
    SecurityEvent,
    SystemEvent,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AuditActor {
    /// Actor type
    pub actor_type: ActorType,

    /// Unique identifier
    pub id: String,

    /// Display name
    pub name: Option<String>,

    /// Email if applicable
    pub email: Option<String>,

    /// Roles at time of action
    pub roles: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum ActorType {
    User,
    ServiceAccount,
    System,
    Anonymous,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AuditResource {
    /// Resource type
    pub resource_type: String,

    /// Resource identifier
    pub id: String,

    /// Resource name
    pub name: Option<String>,

    /// Parent resource if applicable
    pub parent: Option<Box<AuditResource>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AuditAction {
    /// Action name
    pub name: String,

    /// HTTP method if applicable
    pub method: Option<String>,

    /// Endpoint if applicable
    pub endpoint: Option<String>,

    /// Parameters (sanitized)
    pub parameters: serde_json::Value,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AuditOutcome {
    /// Success or failure
    pub status: OutcomeStatus,

    /// Error code if failed
    pub error_code: Option<String>,

    /// Error message if failed
    pub error_message: Option<String>,

    /// Duration in milliseconds
    pub duration_ms: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AuditContext {
    /// Request ID for correlation
    pub request_id: String,

    /// Session ID if applicable
    pub session_id: Option<String>,

    /// Client IP address
    pub ip_address: String,

    /// User agent
    pub user_agent: Option<String>,

    /// Geographic location
    pub geo_location: Option<GeoLocation>,
}

/// Audit logger implementation
pub struct AuditLogger {
    writer: Arc<dyn AuditWriter>,
    sanitizer: DataSanitizer,
}

impl AuditLogger {
    /// Log an audit event
    pub async fn log(&self, event: AuditEvent) -> Result<(), AuditError> {
        // Sanitize sensitive data
        let sanitized_event = self.sanitizer.sanitize(event);

        // Write to audit log
        self.writer.write(sanitized_event).await?;

        // For critical events, also write to immutable store
        if sanitized_event.is_critical() {
            self.writer.write_immutable(sanitized_event).await?;
        }

        Ok(())
    }

    /// Query audit events
    pub async fn query(&self, filter: AuditFilter) -> Result<Vec<AuditEvent>, AuditError> {
        self.writer.query(filter).await
    }
}

/// Data sanitizer for audit logs
pub struct DataSanitizer {
    patterns: Vec<SanitizationPattern>,
}

impl DataSanitizer {
    pub fn sanitize(&self, mut event: AuditEvent) -> AuditEvent {
        // Sanitize parameters
        event.action.parameters = self.sanitize_value(event.action.parameters);

        // Sanitize metadata
        event.metadata = self.sanitize_value(event.metadata);

        event
    }

    fn sanitize_value(&self, value: serde_json::Value) -> serde_json::Value {
        match value {
            serde_json::Value::Object(map) => {
                let sanitized: serde_json::Map<String, serde_json::Value> = map
                    .into_iter()
                    .map(|(k, v)| {
                        let sanitized_v = if self.is_sensitive_key(&k) {
                            serde_json::Value::String("[REDACTED]".to_string())
                        } else {
                            self.sanitize_value(v)
                        };
                        (k, sanitized_v)
                    })
                    .collect();
                serde_json::Value::Object(sanitized)
            }
            serde_json::Value::Array(arr) => {
                serde_json::Value::Array(
                    arr.into_iter().map(|v| self.sanitize_value(v)).collect()
                )
            }
            serde_json::Value::String(s) => {
                serde_json::Value::String(self.sanitize_string(&s))
            }
            other => other,
        }
    }

    fn is_sensitive_key(&self, key: &str) -> bool {
        let sensitive_keys = [
            "password", "secret", "token", "api_key", "apikey",
            "authorization", "credential", "private_key", "ssn",
            "credit_card", "cvv", "pin",
        ];

        let lower = key.to_lowercase();
        sensitive_keys.iter().any(|&s| lower.contains(s))
    }

    fn sanitize_string(&self, s: &str) -> String {
        let mut result = s.to_string();

        for pattern in &self.patterns {
            result = pattern.regex.replace_all(&result, pattern.replacement.as_str()).to_string();
        }

        result
    }
}
```

---

## 11. Release Criteria

### 11.1 Release Readiness Checklist

```yaml
# release_checklist.yaml

code_quality:
  - id: RQ-001
    item: "All tests passing"
    required: true
    automated: true

  - id: RQ-002
    item: "Code coverage >= 85%"
    required: true
    automated: true

  - id: RQ-003
    item: "No critical/high clippy warnings"
    required: true
    automated: true

  - id: RQ-004
    item: "Code review approved by 2+ reviewers"
    required: true
    automated: false

  - id: RQ-005
    item: "No TODO/FIXME in critical paths"
    required: true
    automated: true

security:
  - id: RS-001
    item: "Security scan passed (cargo-audit)"
    required: true
    automated: true

  - id: RS-002
    item: "No known vulnerabilities in dependencies"
    required: true
    automated: true

  - id: RS-003
    item: "Penetration test completed (major releases)"
    required: "Major releases"
    automated: false

  - id: RS-004
    item: "Security review sign-off"
    required: true
    automated: false

  - id: RS-005
    item: "Secrets scanning passed"
    required: true
    automated: true

performance:
  - id: RP-001
    item: "Performance benchmarks within targets"
    required: true
    automated: true

  - id: RP-002
    item: "Load testing completed"
    required: true
    automated: true

  - id: RP-003
    item: "No performance regression > 10%"
    required: true
    automated: true

  - id: RP-004
    item: "Memory leak testing passed"
    required: true
    automated: true

documentation:
  - id: RD-001
    item: "API documentation complete"
    required: true
    automated: true

  - id: RD-002
    item: "CHANGELOG updated"
    required: true
    automated: false

  - id: RD-003
    item: "Migration guide (breaking changes)"
    required: "If breaking changes"
    automated: false

  - id: RD-004
    item: "Runbooks updated"
    required: true
    automated: false

operations:
  - id: RO-001
    item: "Monitoring dashboards configured"
    required: true
    automated: false

  - id: RO-002
    item: "Alerting rules configured"
    required: true
    automated: false

  - id: RO-003
    item: "Rollback procedure tested"
    required: true
    automated: false

  - id: RO-004
    item: "Feature flags configured"
    required: true
    automated: false

sign_offs:
  - role: "Engineering Lead"
    required: true

  - role: "QA Lead"
    required: true

  - role: "Security Engineer"
    required: true

  - role: "SRE Lead"
    required: true

  - role: "Product Manager"
    required: true
```

### 11.2 Release Process

```yaml
# release_process.yaml

versioning:
  scheme: "Semantic Versioning (SemVer)"
  format: "MAJOR.MINOR.PATCH"

  rules:
    major:
      - "Breaking API changes"
      - "Major architecture changes"
      - "Removal of deprecated features"

    minor:
      - "New features (backward compatible)"
      - "Deprecation announcements"
      - "Significant performance improvements"

    patch:
      - "Bug fixes"
      - "Security patches"
      - "Minor performance improvements"

release_types:
  regular:
    cadence: "Every 2 weeks"
    freeze_period: "2 days before release"
    testing_period: "3 days"

  hotfix:
    trigger: "Critical security or availability issue"
    process: "Expedited review and testing"
    testing_period: "4 hours minimum"

  major:
    cadence: "Quarterly"
    freeze_period: "1 week before release"
    testing_period: "2 weeks"

environments:
  development:
    deployment: "Continuous"
    approval: "None"
    testing: "Unit + Integration"

  staging:
    deployment: "Daily"
    approval: "Automated gates"
    testing: "Full suite + E2E"

  production:
    deployment: "Scheduled release windows"
    approval: "Manual approval"
    testing: "Smoke tests + Canary"

deployment_strategy:
  type: "Blue-Green with Canary"

  canary:
    initial_percentage: 5
    increment: 10
    interval: "10 minutes"
    success_criteria:
      - "Error rate < 0.1%"
      - "Latency p99 < baseline + 10%"
      - "No critical alerts"

  rollback:
    automatic_trigger:
      - "Error rate > 1%"
      - "Latency p99 > baseline + 50%"
      - "Critical alert fired"
    manual_trigger: "Any team member"
    time_to_rollback: "< 5 minutes"

post_release:
  monitoring_period: "24 hours enhanced monitoring"

  activities:
    - "Verify all services healthy"
    - "Review error rates"
    - "Check performance metrics"
    - "Monitor user feedback"

  documentation:
    - "Update release notes"
    - "Archive deployment artifacts"
    - "Update incident contacts"
```

### 11.3 Go/No-Go Decision Matrix

```yaml
# go_nogo_matrix.yaml

categories:
  blockers:
    description: "Issues that prevent release"
    examples:
      - "Critical security vulnerability"
      - "Data loss risk"
      - "Core functionality broken"
      - "Compliance violation"
    decision: "NO-GO until resolved"

  high_severity:
    description: "Significant issues requiring evaluation"
    examples:
      - "Performance degradation > 20%"
      - "Non-critical feature broken"
      - "Significant UX regression"
    decision: "Evaluate risk vs. delay cost"
    threshold: "Max 2 high severity issues"

  medium_severity:
    description: "Issues that should be tracked"
    examples:
      - "Minor performance regression"
      - "Edge case bugs"
      - "Documentation gaps"
    decision: "GO with documented known issues"
    threshold: "Max 5 medium severity issues"

  low_severity:
    description: "Minor issues"
    examples:
      - "Cosmetic issues"
      - "Minor logging improvements"
    decision: "GO"
    threshold: "No limit"

decision_framework:
  inputs:
    - "Release readiness checklist completion"
    - "Issue severity counts"
    - "Risk assessment"
    - "Business urgency"
    - "Stakeholder availability"

  process:
    1: "Review release checklist"
    2: "Categorize open issues"
    3: "Assess risk of known issues"
    4: "Evaluate business impact of delay"
    5: "Make GO/NO-GO recommendation"
    6: "Obtain required sign-offs"

  decision_authority:
    go: "Engineering Lead + Product Manager"
    no_go: "Any stakeholder can block"
    override: "VP Engineering"

rollback_criteria:
  automatic:
    - "Error rate > 5x baseline"
    - "Service unavailability"
    - "Data integrity issues detected"

  manual:
    - "Significant negative user feedback"
    - "Performance below SLA"
    - "Security concern identified"
```

---

## Appendix A: Tool Configuration Summary

| Tool | Purpose | Configuration File |
|------|---------|-------------------|
| rustfmt | Code formatting | `rustfmt.toml` |
| clippy | Linting | `.clippy.toml` |
| cargo-audit | Security scanning | N/A (CLI) |
| cargo-deny | Dependency checks | `deny.toml` |
| cargo-tarpaulin | Code coverage | `tarpaulin.toml` |
| cargo-mutants | Mutation testing | `mutants.toml` |
| criterion | Benchmarking | Inline |
| semgrep | Static analysis | `.semgrep.yml` |
| trivy | Container scanning | `trivy.yaml` |

---

## Appendix B: Refinement Timeline

| Phase | Duration | Key Activities |
|-------|----------|---------------|
| 4.1 Static Analysis | 1 week | Lint configuration, code cleanup |
| 4.2 Dynamic Testing | 2 weeks | Test coverage, mutation testing |
| 4.3 Performance | 1 week | Benchmarking, optimization |
| 4.4 Security | 2 weeks | Scanning, penetration testing |
| 4.5 Documentation | 1 week | API docs, runbooks |
| 4.6 Release Validation | 1 week | RC build, acceptance testing |

**Total Estimated Duration**: 8 weeks

---

## Document Control

| Version | Date | Author | Changes |
|---------|------|--------|---------|
| 1.0.0 | 2025-01-XX | LLM DevOps Team | Initial release |

---

*This document is part of the SPARC planning methodology for LLM Research Lab.*
---

### Additional Refinement: Compliance & Release Criteria

### 10.1 Regulatory Compliance

#### SOC2 Type II Compliance Checklist

```yaml
soc2_controls:
  access_control:
    - id: CC6.1
      control: "Logical and physical access controls"
      evidence:
        - RBAC policy documentation
        - Access control matrix
        - Authentication logs
        - MFA enforcement logs

    - id: CC6.2
      control: "Prior to issuing system credentials"
      evidence:
        - User provisioning workflow
        - Approval records
        - Background check completion

    - id: CC6.6
      control: "Encryption of confidential data"
      evidence:
        - TLS 1.3 configuration
        - Data-at-rest encryption
        - Key management documentation

  system_operations:
    - id: CC7.1
      control: "System capacity monitoring"
      evidence:
        - Prometheus dashboards
        - Capacity planning reports
        - Auto-scaling configurations

    - id: CC7.2
      control: "System monitoring and alerting"
      evidence:
        - Alert definitions
        - Incident response logs
        - SLA compliance reports

  change_management:
    - id: CC8.1
      control: "Change management procedures"
      evidence:
        - PR approval workflows
        - Change review board minutes
        - Deployment logs
        - Rollback procedures

  risk_mitigation:
    - id: CC9.1
      control: "Risk assessment process"
      evidence:
        - Quarterly risk assessments
        - Threat modeling documents
        - Penetration test reports
```

#### GDPR Compliance Checklist

```yaml
gdpr_requirements:
  data_principles:
    - id: Art.5.1.a
      requirement: "Lawfulness, fairness, transparency"
      implementation:
        - Privacy policy published
        - Consent management system
        - Data processing register
      validation: "Legal review + audit trail"

    - id: Art.5.1.c
      requirement: "Data minimization"
      implementation:
        - Data retention policies
        - Automatic data purging
        - Collection justification
      validation: "Data flow audit"

    - id: Art.5.1.f
      requirement: "Integrity and confidentiality"
      implementation:
        - Encryption (TLS 1.3, AES-256)
        - Access logging
        - Integrity checksums
      validation: "Security scan + pen test"

  data_subject_rights:
    - id: Art.15
      requirement: "Right of access"
      api_endpoint: "GET /api/v1/users/{id}/data"
      sla: "Response within 30 days"

    - id: Art.16
      requirement: "Right to rectification"
      api_endpoint: "PATCH /api/v1/users/{id}"
      sla: "Update within 24 hours"

    - id: Art.17
      requirement: "Right to erasure"
      api_endpoint: "DELETE /api/v1/users/{id}"
      sla: "Deletion within 72 hours"
      implementation:
        - Soft delete with grace period
        - Hard delete cascade
        - Backup purging (90 days)

  breach_notification:
    - id: Art.33
      requirement: "Authority notification"
      sla: "Within 72 hours of awareness"
      procedure: "docs/runbooks/breach-notification.md"

    - id: Art.34
      requirement: "Individual notification"
      sla: "Without undue delay"
      template: "templates/breach-notification-email.txt"
```

### 10.2 Audit Trail Requirements

#### Mandatory Audit Events

```yaml
audit_events:
  authentication:
    - event: user_login
      fields: [timestamp, user_id, ip_address, user_agent, success]
      retention: 2_years

    - event: login_failure
      fields: [timestamp, username_attempt, ip_address, reason]
      retention: 2_years

    - event: password_change
      fields: [timestamp, user_id, initiated_by, ip_address]
      retention: 7_years

    - event: mfa_enrollment
      fields: [timestamp, user_id, method, ip_address]
      retention: 7_years

  authorization:
    - event: permission_granted
      fields: [timestamp, user_id, resource, permission, granted_by]
      retention: 7_years

    - event: permission_denied
      fields: [timestamp, user_id, resource, attempted_action, reason]
      retention: 2_years

    - event: role_assignment
      fields: [timestamp, user_id, role, assigned_by]
      retention: 7_years

  data_access:
    - event: data_read
      fields: [timestamp, user_id, resource_type, resource_id, query]
      retention: 1_year
      pii_alert: true

    - event: data_export
      fields: [timestamp, user_id, export_type, record_count, destination]
      retention: 7_years

    - event: bulk_operation
      fields: [timestamp, user_id, operation, record_count, filter]
      retention: 3_years

  data_modification:
    - event: data_create
      fields: [timestamp, user_id, resource_type, resource_id]
      retention: 7_years

    - event: data_update
      fields: [timestamp, user_id, resource_type, resource_id, fields_changed]
      retention: 7_years
      diff_storage: true

    - event: data_delete
      fields: [timestamp, user_id, resource_type, resource_id, soft_delete]
      retention: 10_years

  system_events:
    - event: configuration_change
      fields: [timestamp, user_id, component, setting, old_value, new_value]
      retention: 7_years

    - event: deployment
      fields: [timestamp, version, deployed_by, environment, commit_sha]
      retention: 10_years

    - event: security_scan
      fields: [timestamp, scan_type, findings_count, severity_distribution]
      retention: 5_years
```

#### Audit Log Format

```json
{
  "version": "1.0",
  "event_id": "evt_1234567890abcdef",
  "timestamp": "2025-01-15T10:30:45.123Z",
  "event_type": "data_update",
  "actor": {
    "type": "user",
    "id": "usr_abc123",
    "email": "user@example.com",
    "roles": ["researcher", "admin"]
  },
  "resource": {
    "type": "experiment",
    "id": "exp_xyz789",
    "parent": {
      "type": "project",
      "id": "prj_456"
    }
  },
  "action": {
    "method": "PATCH",
    "endpoint": "/api/v1/experiments/exp_xyz789",
    "fields_changed": ["status", "results"],
    "previous_values": {
      "status": "running"
    },
    "new_values": {
      "status": "completed"
    }
  },
  "context": {
    "request_id": "req_abcd1234",
    "session_id": "ses_xyz456",
    "ip_address": "203.0.113.42",
    "user_agent": "Mozilla/5.0...",
    "geo": {
      "country": "US",
      "region": "CA"
    }
  },
  "outcome": {
    "status": "success",
    "duration_ms": 245,
    "response_code": 200
  },
  "metadata": {
    "correlation_id": "cor_123abc",
    "trace_id": "trc_456def"
  }
}
```

#### Audit Storage Requirements

```yaml
storage:
  primary:
    backend: "PostgreSQL with append-only table"
    replication: "Synchronous to 2 replicas"
    backup: "Daily to S3 with versioning"

  immutable_archive:
    backend: "AWS S3 with Object Lock"
    retention: "COMPLIANCE mode, 7 years"
    integrity: "SHA-256 checksums stored separately"

  search_index:
    backend: "Elasticsearch"
    retention: "90 days hot, 2 years warm"

access_control:
  read: ["security_admin", "compliance_officer", "auditor"]
  export: ["compliance_officer", "auditor"]
  delete: ["PROHIBITED - retention only"]

query_limits:
  rate_limit: "100 requests per minute per user"
  result_set: "Max 10,000 records per query"
  export_size: "Max 1GB per export"
```

### 10.3 Security Compliance

#### OWASP Top 10 Verification

```yaml
owasp_verification:
  A01_broken_access_control:
    controls:
      - "RBAC enforced at API gateway"
      - "Resource-level authorization checks"
      - "Horizontal privilege escalation tests"
    validation:
      - tool: "OWASP ZAP"
        scan: "Active scan with authentication"
      - test: "Automated privilege escalation tests"
      - review: "Manual code review of auth logic"
    frequency: "Every release"

  A02_cryptographic_failures:
    controls:
      - "TLS 1.3 minimum"
      - "AES-256-GCM for data at rest"
      - "PBKDF2 (100k iterations) for passwords"
    validation:
      - tool: "testssl.sh"
        scan: "TLS configuration"
      - tool: "cargo-audit"
        scan: "Cryptographic library versions"
    frequency: "Weekly"

  A03_injection:
    controls:
      - "Parameterized queries (no string concat)"
      - "ORM with prepared statements"
      - "Input validation whitelist"
    validation:
      - tool: "sqlmap"
        scan: "SQL injection attempts"
      - tool: "semgrep"
        rules: "injection-detection.yaml"
    frequency: "Every PR + Weekly"

  A04_insecure_design:
    controls:
      - "Threat modeling per feature"
      - "Security architecture review"
      - "Defense in depth"
    validation:
      - review: "Security design review"
      - test: "Abuse case testing"
    frequency: "Per major feature"

  A05_security_misconfiguration:
    controls:
      - "Hardened container images"
      - "Least privilege IAM roles"
      - "Security headers enforced"
    validation:
      - tool: "trivy"
        scan: "Container configuration"
      - tool: "kube-bench"
        scan: "K8s security posture"
    frequency: "Daily"

  A06_vulnerable_components:
    controls:
      - "Dependency scanning in CI"
      - "Auto-update non-breaking patches"
      - "Vendor security advisories monitored"
    validation:
      - tool: "cargo-audit"
      - tool: "Dependabot"
      - tool: "Snyk"
    frequency: "Every commit"

  A07_authentication_failures:
    controls:
      - "MFA enforced for production access"
      - "Password complexity requirements"
      - "Rate limiting on auth endpoints"
    validation:
      - test: "Brute force protection tests"
      - test: "Session management tests"
    frequency: "Every release"

  A08_software_data_integrity:
    controls:
      - "Signed container images"
      - "Artifact checksums verified"
      - "Code signing for releases"
    validation:
      - tool: "cosign"
        verify: "Image signatures"
      - process: "Supply chain security review"
    frequency: "Every release"

  A09_logging_monitoring_failures:
    controls:
      - "Centralized logging (Loki)"
      - "Security event alerting"
      - "Audit trail immutability"
    validation:
      - test: "Log injection attempts"
      - review: "Alert coverage review"
    frequency: "Quarterly"

  A10_ssrf:
    controls:
      - "URL whitelist for external requests"
      - "Network segmentation"
      - "Private IP blocking"
    validation:
      - tool: "OWASP ZAP"
        scan: "SSRF detection"
      - test: "Manual SSRF testing"
    frequency: "Every release"
```

#### Vulnerability SLA

```yaml
vulnerability_sla:
  critical:
    definition: "CVSS >= 9.0 or active exploitation"
    response_time: "1 hour"
    remediation_time: "24 hours"
    actions:
      - "Immediate incident declared"
      - "Emergency patch process"
      - "Rollback if patch unavailable"
    escalation: "VP Engineering + CISO"

  high:
    definition: "CVSS 7.0-8.9"
    response_time: "4 hours"
    remediation_time: "7 days"
    actions:
      - "Prioritize in current sprint"
      - "Workaround if available"
      - "Customer notification if exposed"
    escalation: "Engineering Lead"

  medium:
    definition: "CVSS 4.0-6.9"
    response_time: "1 business day"
    remediation_time: "30 days"
    actions:
      - "Schedule in next sprint"
      - "Document workaround"
    escalation: "Team Lead"

  low:
    definition: "CVSS < 4.0"
    response_time: "5 business days"
    remediation_time: "90 days"
    actions:
      - "Add to backlog"
      - "Fix in regular release cycle"
    escalation: "None"

exception_process:
  approval_required:
    - "Security team assessment"
    - "Risk acceptance document"
    - "Compensating controls defined"
    - "Executive sign-off (High/Critical)"

  review_frequency: "Quarterly"
  max_duration: "1 year"
```

---

## 11. Release Criteria

### 11.1 Quality Gates

#### Development Environment

```yaml
dev_gates:
  automated:
    - gate: "Unit tests passing"
      tool: "cargo test"
      threshold: "100%"
      blocking: true

    - gate: "Code compiles"
      tool: "cargo check"
      threshold: "Zero errors"
      blocking: true

    - gate: "Linting"
      tool: "cargo clippy"
      threshold: "Zero warnings"
      blocking: false

  manual:
    - gate: "Self-review completed"
      checklist: "PR template"
      blocking: true
```

#### Staging Environment

```yaml
staging_gates:
  code_quality:
    - gate: "Test coverage"
      tool: "cargo-tarpaulin"
      threshold: ">= 85%"
      blocking: true

    - gate: "Code complexity"
      tool: "cargo-complexity"
      threshold: "Cyclomatic < 15"
      blocking: false

    - gate: "Dead code detection"
      tool: "cargo-udeps"
      threshold: "Zero unused deps"
      blocking: false

  security:
    - gate: "Dependency audit"
      tool: "cargo-audit"
      threshold: "Zero high/critical"
      blocking: true

    - gate: "Secret scanning"
      tool: "gitleaks"
      threshold: "Zero secrets"
      blocking: true

    - gate: "SAST scan"
      tool: "semgrep"
      threshold: "Zero critical findings"
      blocking: true

  testing:
    - gate: "Integration tests"
      tool: "cargo test --test integration"
      threshold: "100% passing"
      blocking: true

    - gate: "E2E tests"
      tool: "playwright"
      threshold: "100% passing"
      blocking: true

    - gate: "Load testing"
      tool: "k6"
      threshold: "p95 < 200ms, error rate < 0.1%"
      blocking: true

  documentation:
    - gate: "API docs coverage"
      tool: "cargo-doc coverage"
      threshold: ">= 100% public API"
      blocking: true

    - gate: "CHANGELOG updated"
      tool: "Manual check"
      blocking: true

  approvals:
    - "2 engineer approvals"
    - "QA sign-off"
```

#### Production Environment

```yaml
production_gates:
  pre_deployment:
    - gate: "Staging validation"
      duration: "24 hours minimum in staging"
      blocking: true

    - gate: "Performance benchmarks"
      threshold: "No regression > 5%"
      blocking: true

    - gate: "Security scan"
      tool: "trivy + OWASP ZAP"
      threshold: "Zero high/critical"
      blocking: true

    - gate: "Penetration test"
      frequency: "Major releases"
      requirement: "Sign-off from security team"
      blocking: true

  deployment_readiness:
    - gate: "Rollback tested"
      requirement: "Rollback procedure validated in staging"
      blocking: true

    - gate: "Feature flags configured"
      requirement: "All new features behind flags"
      blocking: true

    - gate: "Monitoring configured"
      requirement: "Dashboards + alerts ready"
      blocking: true

    - gate: "Runbooks updated"
      requirement: "Incident response procedures current"
      blocking: true

  approvals:
    required_signoffs:
      - role: "Engineering Lead"
        scope: "Code quality + architecture"

      - role: "QA Lead"
        scope: "Test coverage + validation"

      - role: "Security Engineer"
        scope: "Security posture"

      - role: "SRE Lead"
        scope: "Operational readiness"

      - role: "Product Manager"
        scope: "Feature completeness"

  go_no_go_meeting:
    attendees: "All signoff roles"
    duration: "30 minutes"
    timing: "4 hours before deployment"
    decision_authority: "Engineering Lead (final call)"
```

### 11.2 Release Checklist

```yaml
pre_release_checklist:
  code_preparation:
    - id: RC-001
      item: "All PRs merged to release branch"
      automated: false

    - id: RC-002
      item: "Version number updated (Cargo.toml, CHANGELOG)"
      automated: false

    - id: RC-003
      item: "Git tag created (format: v{MAJOR}.{MINOR}.{PATCH})"
      automated: false

    - id: RC-004
      item: "Release notes drafted"
      automated: false

  testing_validation:
    - id: RT-001
      item: "Full test suite passing"
      automated: true
      command: "cargo test --all-features"

    - id: RT-002
      item: "E2E tests passing in staging"
      automated: true

    - id: RT-003
      item: "Load test completed"
      automated: true
      threshold: "p95 < 200ms @ 1000 RPS"

    - id: RT-004
      item: "Chaos engineering tests passed"
      automated: true
      scenarios: ["pod-failure", "network-delay", "disk-pressure"]

  security_validation:
    - id: RS-001
      item: "Dependency audit clean"
      automated: true
      command: "cargo audit"

    - id: RS-002
      item: "Container scan passed"
      automated: true
      command: "trivy image --severity HIGH,CRITICAL"

    - id: RS-003
      item: "SAST scan clean"
      automated: true
      command: "semgrep --config=auto"

    - id: RS-004
      item: "Secrets scanning passed"
      automated: true
      command: "gitleaks detect"

  infrastructure:
    - id: RI-001
      item: "Database migrations tested"
      automated: false
      validation: "Forward + rollback tested in staging"

    - id: RI-002
      item: "Infrastructure as Code validated"
      automated: true
      command: "terraform plan"

    - id: RI-003
      item: "Backup verification"
      automated: false
      requirement: "Latest backup restore tested"

    - id: RI-004
      item: "Capacity planning reviewed"
      automated: false
      requirement: "Resources sufficient for 2x current load"

  documentation:
    - id: RD-001
      item: "API documentation generated"
      automated: true
      command: "cargo doc --no-deps"

    - id: RD-002
      item: "Migration guide (if breaking changes)"
      automated: false

    - id: RD-003
      item: "Runbooks updated"
      automated: false
      required_docs: ["deployment", "rollback", "incident-response"]

    - id: RD-004
      item: "Release announcement prepared"
      automated: false

  operations:
    - id: RO-001
      item: "Monitoring dashboards configured"
      automated: false
      requirement: "Release-specific dashboard ready"

    - id: RO-002
      item: "Alert thresholds reviewed"
      automated: false

    - id: RO-003
      item: "On-call schedule confirmed"
      automated: false
      requirement: "Engineer available 24h post-release"

    - id: RO-004
      item: "Rollback procedure rehearsed"
      automated: false
      requirement: "Team walkthrough completed"
```

### 11.3 Rollback Procedures

#### Automatic Rollback Triggers

```yaml
automatic_rollback:
  error_rate:
    metric: "http_requests_failed_total / http_requests_total"
    threshold: "> 5%"
    window: "5 minutes"
    condition: "error_rate > baseline * 10"

  latency:
    metric: "http_request_duration_seconds"
    threshold: "p99 > 2000ms"
    window: "5 minutes"
    condition: "p99 > baseline_p99 * 2"

  availability:
    metric: "up"
    threshold: "< 1"
    window: "2 minutes"
    condition: "service_down"

  critical_alerts:
    triggers:
      - "DatabaseConnectionPoolExhausted"
      - "MemoryLeakDetected"
      - "DataIntegrityViolation"
    action: "Immediate rollback"

  health_check:
    endpoint: "/health"
    threshold: "3 consecutive failures"
    action: "Rollback canary deployment"

rollback_process:
  stage_1_detection:
    duration: "0-5 minutes"
    actions:
      - "Alert fires"
      - "Automated health check validation"
      - "Incident declared"

  stage_2_decision:
    duration: "5-10 minutes"
    actions:
      - "On-call engineer notified"
      - "Quick investigation (< 5 min)"
      - "Decision: Debug vs Rollback"

  stage_3_execution:
    duration: "10-15 minutes"
    actions:
      - "Initiate rollback command"
      - "Monitor rollback progress"
      - "Verify previous version health"

  stage_4_verification:
    duration: "15-30 minutes"
    actions:
      - "Confirm metrics returned to normal"
      - "Customer communication"
      - "Post-incident review scheduled"

rollback_commands:
  kubernetes:
    command: "kubectl rollout undo deployment/llm-research-lab"
    verify: "kubectl rollout status deployment/llm-research-lab"

  database:
    approach: "Schema backward compatible (no immediate rollback)"
    emergency: "Restore from last known good backup"

  configuration:
    approach: "GitOps revert commit"
    command: "git revert HEAD && git push"
```

#### Manual Rollback Decision Tree

```yaml
decision_tree:
  symptom_analysis:
    high_error_rate:
      investigate:
        - "Check error logs for patterns"
        - "Identify affected endpoints"
        - "Determine if regression or new issue"
      decision_criteria:
        rollback: "Errors in critical path OR > 1% user impact"
        debug: "Errors in non-critical features AND < 0.1% user impact"

    performance_degradation:
      investigate:
        - "Compare p99 latency to baseline"
        - "Check resource utilization"
        - "Identify slow queries"
      decision_criteria:
        rollback: "Degradation > 50% OR SLA breach"
        debug: "Degradation < 20% AND no SLA impact"

    functionality_issue:
      investigate:
        - "Identify affected feature"
        - "Determine workaround availability"
        - "Assess business impact"
      decision_criteria:
        rollback: "Core feature broken OR data loss risk"
        debug: "Edge case OR workaround available"

  risk_assessment:
    rollback_risk: "Low (previous version proven stable)"
    debug_risk: "High (issue may escalate)"

    time_pressure:
      business_hours: "More time for debugging (2 hours)"
      off_hours: "Faster rollback decision (30 minutes)"

    stakeholder_input:
      engineering: "Technical feasibility"
      product: "Business impact"
      customer_success: "User impact"
```

### 11.4 Post-Release Validation

#### Smoke Tests

```yaml
smoke_tests:
  deployment_verification:
    - test: "Service is reachable"
      endpoint: "GET /health"
      expected: "200 OK"
      timeout: "5s"

    - test: "Authentication working"
      endpoint: "POST /api/v1/auth/login"
      payload: "test_credentials"
      expected: "200 OK + JWT token"

    - test: "Database connectivity"
      endpoint: "GET /health/db"
      expected: "200 OK"

    - test: "Cache connectivity"
      endpoint: "GET /health/cache"
      expected: "200 OK"

  critical_paths:
    - test: "User can create experiment"
      steps:
        - "POST /api/v1/experiments"
        - "GET /api/v1/experiments/{id}"
      expected: "Experiment created and retrievable"

    - test: "Model inference pipeline"
      steps:
        - "POST /api/v1/models/{id}/infer"
      expected: "Inference result returned < 500ms"

    - test: "Data export"
      steps:
        - "POST /api/v1/exports"
        - "GET /api/v1/exports/{id}/download"
      expected: "Export file generated"

  automation:
    tool: "playwright + rust integration"
    execution: "Triggered post-deployment"
    failure_action: "Alert + optional rollback"
```

#### Monitoring Checklist

```yaml
post_release_monitoring:
  immediate_validation:
    duration: "First 30 minutes"
    checks:
      - metric: "error_rate"
        threshold: "< 0.5%"

      - metric: "response_time_p99"
        threshold: "< 300ms"

      - metric: "active_connections"
        threshold: "> 0"

      - metric: "deployment_status"
        threshold: "all_pods_ready"

  short_term_monitoring:
    duration: "First 4 hours"
    checks:
      - metric: "memory_usage"
        threshold: "stable (no leak)"

      - metric: "cpu_usage"
        threshold: "< 70% average"

      - metric: "database_connections"
        threshold: "< 80% pool capacity"

      - metric: "error_logs"
        threshold: "no new error patterns"

  extended_monitoring:
    duration: "24 hours"
    checks:
      - metric: "user_engagement"
        threshold: "comparable to pre-release"

      - metric: "background_jobs"
        threshold: "queue depth normal"

      - metric: "data_consistency"
        threshold: "integrity checks passing"

  canary_analysis:
    tool: "Prometheus + Grafana"
    comparison: "Canary vs baseline"
    metrics:
      - "request_success_rate"
      - "request_duration"
      - "error_count"
    decision:
      promote: "All metrics within 5% of baseline"
      rollback: "Any metric degrades > 10%"
```

#### Production Validation Tests

```yaml
validation_suite:
  functional_tests:
    - name: "Critical API endpoints"
      tests: 25
      coverage: "All authenticated endpoints"

    - name: "Data integrity"
      tests: 15
      coverage: "CRUD operations + relationships"

    - name: "Authentication flows"
      tests: 10
      coverage: "Login, logout, token refresh, MFA"

  performance_tests:
    - name: "Baseline performance"
      tool: "k6"
      duration: "10 minutes"
      rps: 500
      assertions:
        - "p95 < 200ms"
        - "error_rate < 0.1%"

    - name: "Spike test"
      tool: "k6"
      duration: "5 minutes"
      rps_pattern: "100 -> 1000 -> 100"
      assertions:
        - "no_errors"
        - "recovery_time < 30s"

  security_tests:
    - name: "Authentication bypass attempts"
      tool: "custom_scripts"

    - name: "Authorization checks"
      tool: "custom_scripts"

    - name: "Input validation"
      tool: "OWASP ZAP"
```

### 11.5 Version Numbering

#### Semantic Versioning Rules

```yaml
semver_specification:
  format: "MAJOR.MINOR.PATCH"

  major_version:
    increment_when:
      - "Breaking API changes"
      - "Database schema incompatibility"
      - "Configuration format changes"
      - "Major architecture refactor"
    examples:
      - "Removing deprecated endpoints"
      - "Changing authentication mechanism"
      - "Migrating to new database"

  minor_version:
    increment_when:
      - "New features (backward compatible)"
      - "New API endpoints"
      - "Deprecation notices"
      - "Performance improvements"
    examples:
      - "Adding new experiment types"
      - "New export formats"
      - "Enhanced search capabilities"

  patch_version:
    increment_when:
      - "Bug fixes"
      - "Security patches"
      - "Documentation updates"
      - "Minor performance improvements"
    examples:
      - "Fixing calculation error"
      - "Addressing XSS vulnerability"
      - "Correcting API documentation"

  pre_release:
    format: "MAJOR.MINOR.PATCH-{alpha|beta|rc}.{N}"
    examples:
      - "1.2.0-alpha.1"
      - "1.2.0-beta.3"
      - "1.2.0-rc.1"
    stability:
      alpha: "Internal testing, unstable"
      beta: "External testing, feature complete"
      rc: "Release candidate, production-ready pending validation"

  build_metadata:
    format: "MAJOR.MINOR.PATCH+{commit}.{timestamp}"
    example: "1.2.3+a1b2c3d.20250115"
    usage: "Nightly builds, CI artifacts"
```

#### Git Conventions

```yaml
git_workflow:
  branch_naming:
    feature: "feature/{ticket-id}-{short-description}"
    bugfix: "bugfix/{ticket-id}-{short-description}"
    hotfix: "hotfix/{ticket-id}-{short-description}"
    release: "release/v{MAJOR}.{MINOR}"

    examples:
      - "feature/LRL-123-add-gpu-support"
      - "bugfix/LRL-456-fix-memory-leak"
      - "hotfix/LRL-789-patch-auth-bypass"

  commit_messages:
    format: "{type}({scope}): {subject}"

    types:
      - "feat: New feature"
      - "fix: Bug fix"
      - "docs: Documentation"
      - "style: Formatting"
      - "refactor: Code restructuring"
      - "perf: Performance improvement"
      - "test: Test addition/update"
      - "chore: Build/tooling"
      - "security: Security fix"

    examples:
      - "feat(api): add experiment comparison endpoint"
      - "fix(db): resolve connection pool exhaustion"
      - "security(auth): patch JWT validation vulnerability"
      - "perf(inference): optimize model loading by 40%"

    rules:
      - "Subject line < 72 characters"
      - "Imperative mood (add, not added)"
      - "No period at end"
      - "Body wraps at 72 characters"
      - "Separate subject and body with blank line"

  tagging:
    format: "v{MAJOR}.{MINOR}.{PATCH}"

    creation:
      command: "git tag -a v1.2.3 -m 'Release version 1.2.3'"
      push: "git push origin v1.2.3"

    annotation:
      required: true
      content: "Release notes summary"

    signing:
      required: true
      command: "git tag -s v1.2.3 -m 'Release version 1.2.3'"
      verification: "git tag -v v1.2.3"

  release_branch:
    creation: "From main branch at release time"
    naming: "release/v{MAJOR}.{MINOR}"

    workflow:
      1: "Create release branch"
      2: "Update version in Cargo.toml"
      3: "Update CHANGELOG.md"
      4: "Run full test suite"
      5: "Create and push tag"
      6: "Merge to main"
      7: "Merge to develop"

    hotfix_process:
      1: "Branch from release tag"
      2: "Apply fix"
      3: "Increment patch version"
      4: "Create new tag"
      5: "Merge to main and release branch"

version_files:
  - file: "Cargo.toml"
    field: "version = \"1.2.3\""

  - file: "CHANGELOG.md"
    format: |
      ## [1.2.3] - 2025-01-15
      ### Added
      - New features
      ### Changed
      - Modified functionality
      ### Fixed
      - Bug fixes
      ### Security
      - Security patches

  - file: "docs/API.md"
    field: "API Version: v1.2.3"

  - file: "helm/Chart.yaml"
    field: "appVersion: \"1.2.3\""
```

---

## Appendix: Automation Scripts

### Release Automation

```bash
#!/bin/bash
# scripts/release.sh

set -euo pipefail

VERSION=$1
RELEASE_BRANCH="release/v${VERSION}"

# Validate version format
if ! [[ $VERSION =~ ^[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
  echo "Error: Invalid version format. Use MAJOR.MINOR.PATCH"
  exit 1
fi

# Create release branch
git checkout -b "${RELEASE_BRANCH}"

# Update version files
sed -i "s/^version = .*/version = \"${VERSION}\"/" Cargo.toml
./scripts/update_changelog.sh "${VERSION}"

# Run validation
cargo test --all-features
cargo clippy -- -D warnings
cargo audit

# Commit and tag
git add Cargo.toml CHANGELOG.md
git commit -m "chore: release version ${VERSION}"
git tag -s "v${VERSION}" -m "Release version ${VERSION}"

# Push
git push origin "${RELEASE_BRANCH}"
git push origin "v${VERSION}"

echo "Release ${VERSION} prepared. Ready for CI/CD pipeline."
```

### Rollback Automation

```bash
#!/bin/bash
# scripts/rollback.sh

set -euo pipefail

NAMESPACE="production"
DEPLOYMENT="llm-research-lab"

# Get current and previous revision
CURRENT_REVISION=$(kubectl rollout history deployment/${DEPLOYMENT} -n ${NAMESPACE} | tail -2 | head -1 | awk '{print $1}')
PREVIOUS_REVISION=$((CURRENT_REVISION - 1))

# Confirm rollback
echo "Rolling back ${DEPLOYMENT} from revision ${CURRENT_REVISION} to ${PREVIOUS_REVISION}"
read -p "Confirm rollback? (yes/no): " CONFIRM

if [[ $CONFIRM != "yes" ]]; then
  echo "Rollback cancelled"
  exit 0
fi

# Execute rollback
kubectl rollout undo deployment/${DEPLOYMENT} -n ${NAMESPACE}

# Wait for rollback
kubectl rollout status deployment/${DEPLOYMENT} -n ${NAMESPACE} --timeout=5m

# Verify health
./scripts/smoke_tests.sh

echo "Rollback completed successfully"
```

---

**Document Version**: 1.0.0
**Last Updated**: 2025-01-28
**Author**: DevOps & Release Engineering Team

---

# PART V: COMPLETION

> **SPARC Phase 5**
> Defines deployment, operations, and launch procedures

---

## Completion Overview and Objectives

**Ecosystem**: LLM DevOps (24+ Module Platform)

---

## Table of Contents

1. [Executive Summary](#1-executive-summary)
2. [Completion Objectives](#2-completion-objectives)
3. [Deployment Readiness](#3-deployment-readiness) *(Next Section)*
4. [Operational Handoff](#4-operational-handoff) *(Next Section)*
5. [Production Certification](#5-production-certification) *(Next Section)*
6. [Knowledge Transfer](#6-knowledge-transfer) *(Next Section)*
7. [Support Readiness](#7-support-readiness) *(Next Section)*
8. [Commercial Launch](#8-commercial-launch) *(Next Section)*

---

## 1. Executive Summary

### 1.1 Purpose of Completion Phase

The Completion phase represents the final stage of the SPARC methodology, transforming a refined, validated implementation into a commercially operational product ready for enterprise deployment. This phase bridges the gap between engineering excellence and business value delivery by:

- **Operationalizing** the system through production deployment infrastructure and procedures
- **Transferring ownership** from development teams to operations, support, and customer success teams
- **Certifying readiness** through comprehensive production validation and stakeholder sign-off
- **Enabling commercial launch** with go-to-market readiness, pricing models, and customer onboarding
- **Establishing sustainability** via long-term support structures, maintenance procedures, and continuous improvement mechanisms

Unlike previous SPARC phases focused on design and quality, Completion emphasizes **operational viability** and **business readiness**—ensuring the system not only works flawlessly in production but also generates value for customers and stakeholders.

### 1.2 Relationship to Prior SPARC Phases

The Completion phase builds upon and validates the outputs of all previous SPARC phases:

```
Specification (Phase 1)
    ↓ Defined: What to build, why, and for whom
Pseudocode (Phase 2)
    ↓ Designed: Algorithmic logic and core workflows
Architecture (Phase 3)
    ↓ Structured: System design, components, and integrations
Refinement (Phase 4)
    ↓ Hardened: Code quality, performance, security, and testing
Completion (Phase 5)
    ↓ Operationalized: Deployment, handoff, certification, and launch
```

**Key Dependencies from Phase 4 (Refinement)**:
- ✅ Code coverage ≥ 85%, mutation score ≥ 70%
- ✅ Zero critical/high security vulnerabilities
- ✅ Performance targets met (p99 < 100ms)
- ✅ Documentation complete (100% public API coverage)
- ✅ All quality gates passed (static analysis, security scans, load tests)
- ✅ Release candidate approved by technical stakeholders

**Phase 5 Additions**:
- Production infrastructure provisioned and validated
- Operations teams trained and ready to support live systems
- Customer-facing documentation and support channels established
- Commercial licensing, pricing, and legal compliance finalized
- Business stakeholders authorized to proceed with launch

### 1.3 Definition of "Done" for Enterprise Deployment

A system is considered **complete** and ready for enterprise deployment when it satisfies all criteria across five dimensions:

```yaml
completion_criteria:
  technical_readiness:
    - Production infrastructure deployed and operational
    - All services running in production environment
    - Monitoring, alerting, and observability fully configured
    - Disaster recovery tested and validated
    - Performance baselines established in production
    - Security hardening verified in production context
    - Integration endpoints live and tested end-to-end

  operational_readiness:
    - Operations team trained on system architecture
    - Runbooks documented for all operational procedures
    - Incident response procedures established and tested
    - On-call rotation scheduled with trained personnel
    - Escalation paths defined and communicated
    - Change management process integrated with existing workflows
    - Backup and recovery procedures validated

  business_readiness:
    - Pricing model defined and approved
    - Licensing terms finalized and legally reviewed
    - Customer onboarding process documented and tested
    - Sales enablement materials prepared
    - Marketing collateral approved and published
    - Customer support channels staffed and ready
    - Success metrics and KPIs defined with stakeholder buy-in

  compliance_readiness:
    - SOC2 Type II audit completed (if applicable)
    - GDPR compliance verified with legal sign-off
    - Security compliance validated (OWASP, CIS benchmarks)
    - Audit trail requirements implemented and tested
    - Data retention policies enforced
    - Privacy policy published and consent mechanisms active
    - License compliance verified for all dependencies

  stakeholder_acceptance:
    - Technical leadership sign-off on production readiness
    - Operations leadership sign-off on support readiness
    - Business leadership sign-off on commercial viability
    - Legal sign-off on compliance and licensing
    - Executive sponsor approval to proceed with launch
```

**Definition of "Done"**: All items in the above checklist are marked ✅ Complete, with evidence documented and accessible for audit.

### 1.4 Success Criteria for Commercial Viability

Commercial viability is measured across operational performance, customer adoption, and financial metrics:

#### Operational Performance Metrics

| Metric | Target | Measurement Method | Baseline Period |
|--------|--------|-------------------|-----------------|
| **System Uptime** | ≥ 99.9% | Uptime monitoring (Prometheus) | First 30 days |
| **Mean Time to Recovery (MTTR)** | < 30 minutes | Incident response logs | First 90 days |
| **Change Failure Rate** | < 5% | Deployment success rate | First 60 days |
| **Lead Time for Changes** | < 4 hours | CI/CD pipeline metrics | Continuous |
| **API Latency (p99)** | < 100ms | APM tooling (Jaeger/Prometheus) | Continuous |
| **Error Rate** | < 0.1% | Application logs and metrics | Continuous |
| **Customer Support Resolution** | < 24 hours (P1), < 72 hours (P2) | Ticketing system SLAs | First 90 days |

#### Customer Adoption Metrics

| Metric | Target | Measurement Method | Timeline |
|--------|--------|-------------------|----------|
| **Onboarding Success Rate** | ≥ 90% | Customer onboarding funnel | First 6 months |
| **Active Users (Monthly)** | 50+ | User analytics | First 12 months |
| **Feature Adoption Rate** | ≥ 60% | Feature usage telemetry | First 6 months |
| **Customer Satisfaction (CSAT)** | ≥ 4.5/5.0 | Post-interaction surveys | Continuous |
| **Net Promoter Score (NPS)** | ≥ 50 | Quarterly NPS surveys | Quarterly |
| **Documentation Effectiveness** | ≥ 80% | User feedback + support ticket reduction | First 90 days |

#### Financial Metrics

| Metric | Target | Measurement Method | Timeline |
|--------|--------|-------------------|----------|
| **Revenue per Customer** | Defined in pricing model | Billing system | Monthly |
| **Customer Acquisition Cost (CAC)** | < 6 months payback | Sales + marketing spend / new customers | Quarterly |
| **Gross Margin** | ≥ 70% | Revenue - COGS | Quarterly |
| **Monthly Recurring Revenue (MRR)** | Growth trajectory defined | Billing system | Monthly |
| **Churn Rate** | < 5% monthly | Customer retention tracking | Monthly |
| **Infrastructure Cost per User** | Optimized to margin targets | Cloud billing / active users | Monthly |

**Commercial Viability Gate**: System is considered commercially viable when operational metrics meet targets, customer adoption is growing, and financial metrics trend toward profitability within planned timelines.

### 1.5 Key Stakeholders and Sign-Off Requirements

Completion requires explicit approval from stakeholders across technical, operational, business, and executive domains:

```yaml
stakeholder_sign_offs:
  technical_leadership:
    - role: VP Engineering / CTO
      responsibilities:
        - Production architecture review
        - Technical debt assessment
        - Long-term maintainability validation
      sign_off_criteria:
        - Production infrastructure meets scalability requirements
        - Technical documentation complete and accurate
        - No critical known issues in release candidate
      deliverable: "Technical Production Readiness Report"

    - role: Director of Platform Engineering
      responsibilities:
        - Infrastructure provisioning validation
        - Deployment automation review
        - Monitoring and observability verification
      sign_off_criteria:
        - All environments provisioned and tested
        - CI/CD pipelines operational
        - Observability coverage complete
      deliverable: "Infrastructure Certification Document"

    - role: Principal Security Engineer
      responsibilities:
        - Security posture assessment
        - Compliance validation
        - Vulnerability remediation verification
      sign_off_criteria:
        - Zero critical/high vulnerabilities
        - Security compliance checklist complete
        - Penetration testing passed
      deliverable: "Security Certification Report"

  operational_leadership:
    - role: VP Operations / Head of SRE
      responsibilities:
        - Operational readiness assessment
        - On-call readiness verification
        - Runbook completeness review
      sign_off_criteria:
        - Operations team trained and ready
        - Incident response procedures tested
        - Runbooks complete and validated
      deliverable: "Operational Readiness Certification"

    - role: Director of Customer Support
      responsibilities:
        - Support readiness validation
        - Knowledge base review
        - Support tooling verification
      sign_off_criteria:
        - Support team trained on product
        - Knowledge base articles published
        - Ticketing system configured
      deliverable: "Customer Support Readiness Report"

  business_leadership:
    - role: VP Product / Chief Product Officer
      responsibilities:
        - Product-market fit validation
        - Customer onboarding experience review
        - Success metrics approval
      sign_off_criteria:
        - Product meets market requirements
        - Customer onboarding tested and validated
        - Success KPIs defined and measurable
      deliverable: "Product Launch Readiness Report"

    - role: VP Sales / Chief Revenue Officer
      responsibilities:
        - Sales enablement validation
        - Pricing model approval
        - Go-to-market strategy sign-off
      sign_off_criteria:
        - Sales team trained on product value
        - Pricing model competitive and approved
        - GTM plan executable
      deliverable: "Sales Readiness Certification"

    - role: VP Marketing / CMO
      responsibilities:
        - Marketing material review
        - Brand compliance validation
        - Launch campaign approval
      sign_off_criteria:
        - Marketing collateral complete
        - Launch campaign ready to execute
        - Brand guidelines followed
      deliverable: "Marketing Launch Approval"

  legal_and_compliance:
    - role: General Counsel / Head of Legal
      responsibilities:
        - License agreement review
        - Terms of service approval
        - Regulatory compliance sign-off
      sign_off_criteria:
        - Legal terms reviewed and approved
        - Regulatory requirements met
        - Risk assessment completed
      deliverable: "Legal Compliance Certificate"

    - role: Data Protection Officer (DPO)
      responsibilities:
        - GDPR compliance validation
        - Privacy policy review
        - Data handling procedures verification
      sign_off_criteria:
        - GDPR checklist complete
        - Privacy policy published
        - Data subject rights implemented
      deliverable: "Privacy Compliance Report"

  executive_approval:
    - role: CEO / Executive Sponsor
      responsibilities:
        - Final launch authorization
        - Strategic alignment confirmation
        - Resource commitment approval
      sign_off_criteria:
        - All other stakeholders have signed off
        - Business case remains valid
        - Strategic priorities aligned
      deliverable: "Executive Launch Authorization"
      gate: "GO/NO-GO DECISION FOR PRODUCTION LAUNCH"
```

**Sign-Off Process**:
1. **Week -4 to Launch**: Technical stakeholders review and sign off
2. **Week -3 to Launch**: Operational stakeholders review and sign off
3. **Week -2 to Launch**: Business stakeholders review and sign off
4. **Week -1 to Launch**: Legal/compliance stakeholders review and sign off
5. **T-48 hours**: Executive sponsor issues final GO/NO-GO decision

**Escalation**: Any stakeholder can raise a blocker requiring resolution before proceeding. Critical blockers trigger executive review and may delay launch.

---

## 2. Completion Objectives

### 2.1 Primary Objectives

The Completion phase pursues three primary objectives that collectively enable production launch:

#### 2.1.1 Deployment Readiness

**Objective**: Ensure the system can be deployed, operated, and maintained in a production environment with enterprise-grade reliability and performance.

```yaml
deployment_readiness:
  infrastructure_provisioning:
    description: "Production infrastructure fully provisioned and validated"
    deliverables:
      - Infrastructure-as-Code (IaC) manifests deployed
      - Kubernetes clusters configured with production settings
      - Database instances provisioned with replication and backups
      - Message queues (Kafka) configured with production topology
      - Object storage (S3/MinIO) configured with versioning and lifecycle
      - Load balancers and ingress controllers configured
      - DNS records configured and tested
      - TLS certificates issued and installed
    acceptance_criteria:
      - All infrastructure components operational
      - Smoke tests pass in production environment
      - Network connectivity validated end-to-end
      - Security groups and firewall rules verified

  deployment_automation:
    description: "Fully automated deployment pipeline operational"
    deliverables:
      - CI/CD pipeline configured for production deployments
      - Blue-green deployment capability tested
      - Canary deployment strategy validated
      - Automated rollback mechanisms tested
      - Database migration automation verified
      - Configuration management system operational
    acceptance_criteria:
      - Zero-downtime deployments achievable
      - Rollback time < 5 minutes
      - Deployment success rate ≥ 99%
      - All deployment steps automated (no manual intervention)

  observability:
    description: "Complete monitoring, logging, and tracing operational"
    deliverables:
      - Prometheus metrics collection configured
      - Grafana dashboards deployed for all services
      - Distributed tracing (Jaeger) operational
      - Log aggregation (ELK/Loki) configured
      - Alert rules defined and tested
      - On-call escalation integrated (PagerDuty/OpsGenie)
    acceptance_criteria:
      - 100% service coverage in monitoring
      - Alert coverage for all critical paths
      - Log retention policy enforced
      - Trace sampling configured appropriately
      - Dashboards accessible to operations team

  disaster_recovery:
    description: "Disaster recovery and business continuity validated"
    deliverables:
      - Backup automation configured and tested
      - Point-in-time recovery (PITR) validated
      - Cross-region replication configured (if required)
      - Disaster recovery runbook documented
      - Recovery Time Objective (RTO) < 4 hours validated
      - Recovery Point Objective (RPO) < 15 minutes validated
    acceptance_criteria:
      - Full system recovery tested from backup
      - Data integrity verified post-recovery
      - RTO/RPO targets met in DR drill
      - DR runbook executed successfully by operations team
```

**Success Metric**: Deployment readiness is achieved when a production deployment can be executed, monitored, and recovered entirely through automated processes with documented fallback procedures.

#### 2.1.2 Operational Handoff

**Objective**: Transfer ownership and operational responsibility from development teams to operations, support, and business teams with complete knowledge transfer.

```yaml
operational_handoff:
  operations_team_readiness:
    description: "Operations team trained and ready to support production"
    deliverables:
      - System architecture training completed
      - Runbook walkthroughs conducted
      - Incident response drills executed
      - Access provisioning completed
      - Tool training (monitoring, deployment) completed
    acceptance_criteria:
      - Operations team can independently execute common tasks
      - On-call rotation staffed with trained personnel
      - Incident escalation paths validated
      - Operations team confirms readiness

  support_team_readiness:
    description: "Customer support team trained on product and tooling"
    deliverables:
      - Product training sessions completed
      - Knowledge base articles published
      - Support ticketing system configured
      - Customer communication templates prepared
      - Common issue troubleshooting guides documented
    acceptance_criteria:
      - Support team can answer common customer questions
      - Tier 1 support can resolve 70% of issues without escalation
      - Support SLAs defined and systems configured
      - Support team confirms readiness

  knowledge_transfer:
    description: "Comprehensive knowledge transfer to operational teams"
    deliverables:
      - Architecture Decision Records (ADRs) published
      - Code architecture documentation complete
      - API documentation complete and accessible
      - Database schema documentation published
      - Integration guides for all external systems
      - Troubleshooting playbooks for common issues
    acceptance_criteria:
      - Documentation passes review by operations team
      - No critical knowledge gaps identified
      - Operations can diagnose and resolve issues independently
      - Feedback loop established for documentation improvements

  handoff_validation:
    description: "Operational capabilities validated through simulation"
    deliverables:
      - Shadow operations period (1-2 weeks) completed
      - Incident simulation exercises passed
      - Change management workflow tested
      - Escalation procedures validated
      - Post-mortem process tested
    acceptance_criteria:
      - Operations team successfully resolves simulated incidents
      - Handoff checklist 100% complete
      - Operations leadership sign-off obtained
```

**Success Metric**: Operational handoff is complete when operations and support teams can independently manage the system in production without development team involvement for routine operations.

#### 2.1.3 Production Certification

**Objective**: Validate that the system meets all production requirements through comprehensive testing and certification processes.

```yaml
production_certification:
  performance_certification:
    description: "Production performance validated under load"
    tests:
      - Load testing at 1x expected traffic
      - Load testing at 5x expected traffic
      - Spike testing for traffic surges
      - Endurance testing (72-hour soak test)
      - Concurrent user testing
    acceptance_criteria:
      - API latency p99 < 100ms under load
      - Zero errors at 1x expected load
      - Error rate < 0.1% at 5x expected load
      - No memory leaks during endurance test
      - Performance baselines documented

  security_certification:
    description: "Production security validated and certified"
    tests:
      - Penetration testing by third-party firm
      - OWASP Top 10 vulnerability scan
      - Container security scan (Trivy)
      - Secrets scanning (gitleaks)
      - Dependency vulnerability scan (cargo-audit)
      - Network security audit
    acceptance_criteria:
      - Zero critical/high vulnerabilities
      - Penetration test report approved
      - Security compliance checklist complete
      - Security stakeholder sign-off obtained

  reliability_certification:
    description: "Production reliability validated through chaos testing"
    tests:
      - Pod failure simulation (kill random pods)
      - Network latency injection
      - Database failover simulation
      - Message queue failure simulation
      - Disk space exhaustion simulation
    acceptance_criteria:
      - System remains operational during failures
      - No data loss during failure scenarios
      - Auto-recovery mechanisms functional
      - Alert systems triggered appropriately
      - Chaos testing report approved

  compliance_certification:
    description: "Regulatory and compliance requirements validated"
    audits:
      - SOC2 Type II audit (if applicable)
      - GDPR compliance audit
      - Security compliance audit
      - License compliance audit
    acceptance_criteria:
      - All audit findings resolved
      - Compliance documentation complete
      - Legal sign-off obtained
      - Audit reports approved

  integration_certification:
    description: "All external integrations validated in production"
    tests:
      - LLM-Test-Bench integration tested
      - LLM-Analytics-Hub integration tested
      - LLM-Registry integration tested
      - LLM-Data-Vault integration tested
      - External API integrations tested
    acceptance_criteria:
      - All integrations operational
      - End-to-end workflows validated
      - Error handling tested for each integration
      - Integration documentation complete
```

**Success Metric**: Production certification is complete when all tests pass, all audits are approved, and stakeholders issue formal certification documents.

### 2.2 Secondary Objectives

In addition to the primary objectives, Completion pursues secondary objectives that enhance long-term success:

#### 2.2.1 Knowledge Transfer

```yaml
knowledge_transfer:
  documentation_ecosystem:
    - Architecture documentation (system design, component diagrams)
    - API documentation (100% public API coverage)
    - Operational runbooks (common tasks, troubleshooting)
    - User guides (onboarding, tutorials, advanced usage)
    - Developer guides (contribution, extension, customization)
    - Security documentation (threat models, security controls)

  training_programs:
    - Operations team training (system architecture, runbooks)
    - Support team training (product features, common issues)
    - Customer training (webinars, documentation, certification)
    - Sales training (product value, competitive positioning)

  community_building:
    - Internal Slack/Teams channel for knowledge sharing
    - Office hours for questions and support
    - Post-mortem review sessions for continuous learning
    - Documentation feedback loops

  success_criteria:
    - All stakeholders trained and certified
    - Documentation coverage ≥ 95%
    - Knowledge gaps identified and documented
    - Feedback loops operational
```

#### 2.2.2 Documentation Finalization

```yaml
documentation_finalization:
  customer_facing:
    - Getting Started Guide
    - API Reference (OpenAPI spec + examples)
    - Integration Guides
    - Best Practices Guide
    - FAQ and Troubleshooting
    - Release Notes and Changelog

  internal_facing:
    - System Architecture Document
    - Database Schema Documentation
    - Infrastructure Documentation
    - Deployment Procedures
    - Incident Response Playbooks
    - Change Management Procedures

  compliance_documentation:
    - Privacy Policy
    - Terms of Service
    - Security Documentation
    - Audit Reports
    - Compliance Certifications

  success_criteria:
    - All documentation reviewed and approved
    - Documentation accessible and discoverable
    - Search functionality operational
    - Documentation versioned with releases
```

#### 2.2.3 Support Readiness

```yaml
support_readiness:
  support_infrastructure:
    - Ticketing system (Zendesk, Jira Service Desk)
    - Knowledge base (Confluence, Notion)
    - Customer communication channels (email, Slack, chat)
    - Escalation management (PagerDuty, OpsGenie)

  support_processes:
    - Ticket triage and prioritization
    - SLA definitions and tracking
    - Escalation procedures
    - Customer communication templates
    - Feedback collection mechanisms

  support_team_staffing:
    - Tier 1 support staffed and trained
    - Tier 2 support (engineering) on-call rotation
    - Tier 3 support (architecture) escalation path
    - Support hours defined (business hours vs 24/7)

  success_criteria:
    - Support SLAs defined and achievable
    - Support team confirms readiness
    - Knowledge base ≥ 50 articles at launch
    - Escalation paths tested and validated
```

### 2.3 Completion Phases/Stages

The Completion phase is structured into six sequential stages, each with specific deliverables and gates:

```
┌─────────────────────────────────────────────────────────────────┐
│                    COMPLETION PHASE TIMELINE                     │
└─────────────────────────────────────────────────────────────────┘

Phase 5.1: Infrastructure Provisioning (Week 1-2)
├─ Production infrastructure deployed
├─ Monitoring and observability configured
├─ Security hardening applied
└─ Gate: Infrastructure Validation ✓

Phase 5.2: Deployment Automation (Week 2-3)
├─ CI/CD pipelines configured for production
├─ Blue-green deployment tested
├─ Rollback mechanisms validated
└─ Gate: Deployment Certification ✓

Phase 5.3: Operational Handoff (Week 3-5)
├─ Operations team training completed
├─ Runbooks and documentation finalized
├─ Shadow operations period executed
└─ Gate: Operations Readiness ✓

Phase 5.4: Production Certification (Week 5-7)
├─ Load testing and performance validation
├─ Security and penetration testing
├─ Chaos engineering and reliability testing
└─ Gate: Production Certification ✓

Phase 5.5: Business Readiness (Week 7-8)
├─ Customer onboarding tested
├─ Support team trained and ready
├─ Sales enablement completed
└─ Gate: Business Launch Readiness ✓

Phase 5.6: Launch and Validation (Week 8-10)
├─ Production launch executed
├─ Post-launch monitoring and validation
├─ Customer onboarding initiated
└─ Gate: Launch Success Validation ✓
```

#### Phase 5.1: Infrastructure Provisioning

**Duration**: 2 weeks
**Owner**: Platform Engineering Team
**Objective**: Deploy and validate production infrastructure

```yaml
phase_5_1:
  key_activities:
    - Deploy Kubernetes clusters (production + DR)
    - Provision databases (PostgreSQL, ClickHouse)
    - Deploy message queues (Kafka/Redis)
    - Configure object storage (S3/MinIO)
    - Set up networking (load balancers, DNS, TLS)
    - Deploy monitoring stack (Prometheus, Grafana, Jaeger)
    - Configure log aggregation (ELK/Loki)

  deliverables:
    - Infrastructure-as-Code manifests deployed
    - All services operational in production
    - Smoke tests passing
    - Monitoring dashboards accessible
    - Security hardening verified

  gate_criteria:
    - All infrastructure components green
    - Network connectivity validated
    - Security scan passed
    - Platform Engineering sign-off obtained

  blockers_and_risks:
    - Cloud provider outages
    - TLS certificate delays
    - DNS propagation delays
    - Networking configuration errors
```

#### Phase 5.2: Deployment Automation

**Duration**: 1 week
**Owner**: DevOps/MLOps Team
**Objective**: Validate automated deployment pipelines

```yaml
phase_5_2:
  key_activities:
    - Configure CI/CD for production deployments
    - Test blue-green deployment strategy
    - Validate canary deployment process
    - Test automated rollback mechanisms
    - Execute database migration automation
    - Validate configuration management

  deliverables:
    - CI/CD pipelines operational
    - Deployment playbooks documented
    - Rollback tested and validated
    - Deployment metrics dashboard created

  gate_criteria:
    - Zero-downtime deployment demonstrated
    - Rollback time < 5 minutes verified
    - Deployment automation 100% complete
    - DevOps team sign-off obtained

  blockers_and_risks:
    - Pipeline configuration errors
    - Database migration failures
    - Rollback mechanism bugs
    - Integration test failures
```

#### Phase 5.3: Operational Handoff

**Duration**: 2 weeks
**Owner**: Operations/SRE Team
**Objective**: Transfer operational ownership and validate readiness

```yaml
phase_5_3:
  key_activities:
    - Conduct operations team training
    - Execute runbook walkthroughs
    - Perform incident response drills
    - Complete shadow operations period
    - Validate escalation procedures
    - Test on-call rotation

  deliverables:
    - Operations training completed
    - Runbooks validated and approved
    - Incident response procedures tested
    - On-call rotation staffed
    - Handoff checklist complete

  gate_criteria:
    - Operations team confirms readiness
    - Shadow operations successful
    - Incident drills passed
    - Operations leadership sign-off obtained

  blockers_and_risks:
    - Insufficient operations staffing
    - Knowledge gaps in runbooks
    - Training schedule conflicts
    - Incident simulation failures
```

#### Phase 5.4: Production Certification

**Duration**: 2 weeks
**Owner**: QA/Security Team
**Objective**: Certify production readiness through comprehensive testing

```yaml
phase_5_4:
  key_activities:
    - Execute load testing (1x, 5x, spike)
    - Conduct penetration testing
    - Perform chaos engineering tests
    - Run endurance/soak tests
    - Validate security compliance
    - Test integration endpoints

  deliverables:
    - Load test reports
    - Penetration test report
    - Chaos testing report
    - Security certification
    - Performance baselines documented

  gate_criteria:
    - All performance targets met
    - Zero critical/high vulnerabilities
    - Chaos tests passed
    - QA and Security sign-off obtained

  blockers_and_risks:
    - Performance issues discovered
    - Security vulnerabilities found
    - Chaos tests reveal reliability issues
    - Third-party penetration test delays
```

#### Phase 5.5: Business Readiness

**Duration**: 1 week
**Owner**: Product/Business Team
**Objective**: Validate business operations readiness

```yaml
phase_5_5:
  key_activities:
    - Test customer onboarding workflow
    - Train support team
    - Complete sales enablement
    - Finalize pricing and licensing
    - Review marketing materials
    - Validate legal compliance

  deliverables:
    - Customer onboarding tested
    - Support team trained
    - Sales materials ready
    - Pricing model approved
    - Legal terms finalized

  gate_criteria:
    - Onboarding success rate ≥ 90%
    - Support team confirms readiness
    - Sales team confirms readiness
    - Legal sign-off obtained
    - Business leadership approval

  blockers_and_risks:
    - Customer onboarding issues
    - Support team training gaps
    - Legal approval delays
    - Pricing model revisions
```

#### Phase 5.6: Launch and Validation

**Duration**: 2 weeks
**Owner**: Executive Sponsor
**Objective**: Execute production launch and validate success

```yaml
phase_5_6:
  key_activities:
    - Execute GO/NO-GO decision
    - Perform production launch
    - Monitor initial customer onboarding
    - Validate production metrics
    - Address launch issues
    - Conduct post-launch retrospective

  deliverables:
    - Production system live
    - Initial customers onboarded
    - Launch metrics dashboard
    - Post-launch report
    - Lessons learned documented

  gate_criteria:
    - System uptime ≥ 99.9%
    - No critical incidents
    - Customer onboarding successful
    - Success metrics trending positively
    - Post-launch retrospective completed

  blockers_and_risks:
    - Critical production issues
    - Customer onboarding failures
    - Unexpected load patterns
    - Integration failures with partners
```

### 2.4 Timeline Estimates per Phase

```yaml
completion_timeline:
  total_duration: "8-10 weeks"

  detailed_breakdown:
    phase_5_1_infrastructure:
      duration: "2 weeks"
      effort: "120 person-hours"
      team_size: "3 engineers"
      critical_path: true
      dependencies: ["Phase 4 complete"]

    phase_5_2_deployment:
      duration: "1 week"
      effort: "60 person-hours"
      team_size: "2 engineers"
      critical_path: true
      dependencies: ["Phase 5.1 complete"]

    phase_5_3_operational_handoff:
      duration: "2 weeks"
      effort: "80 person-hours"
      team_size: "4 people (ops + dev)"
      critical_path: true
      dependencies: ["Phase 5.2 complete"]
      parallel_activities: ["Support team training"]

    phase_5_4_certification:
      duration: "2 weeks"
      effort: "100 person-hours"
      team_size: "3 engineers + 1 security specialist"
      critical_path: true
      dependencies: ["Phase 5.3 complete"]
      parallel_activities: ["Business readiness preparation"]

    phase_5_5_business_readiness:
      duration: "1 week"
      effort: "40 person-hours"
      team_size: "5 people (product, sales, support, legal)"
      critical_path: false
      dependencies: ["Phase 5.4 complete"]
      can_run_parallel: ["Phase 5.4 (partial)"]

    phase_5_6_launch:
      duration: "2 weeks"
      effort: "80 person-hours"
      team_size: "Full team (dev, ops, support, business)"
      critical_path: true
      dependencies: ["All prior phases complete", "GO decision"]

  risk_buffer:
    description: "Additional time for unforeseen issues"
    duration: "1-2 weeks"
    application: "Applied at discretion of project management"

  fast_track_option:
    description: "Accelerated timeline with increased staffing"
    total_duration: "6 weeks"
    requirements:
      - Increased team size by 50%
      - Pre-provisioned infrastructure
      - Parallel execution of non-dependent phases
      - Executive priority and resource allocation
```

### 2.5 Dependencies on Refinement Phase Completion

Phase 5 (Completion) has strict dependencies on Phase 4 (Refinement) outputs. Launch cannot proceed without verifying these prerequisites:

```yaml
refinement_phase_dependencies:
  quality_gates:
    code_quality:
      - metric: "Code Coverage"
        required: "≥ 85%"
        verification: "cargo-tarpaulin report"
        blocker: true

      - metric: "Mutation Score"
        required: "≥ 70%"
        verification: "cargo-mutants report"
        blocker: true

      - metric: "Clippy Warnings"
        required: "Zero at pedantic level"
        verification: "cargo clippy --all-targets -- -D warnings"
        blocker: true

      - metric: "Compiler Warnings"
        required: "Zero with strict flags"
        verification: "cargo build --release"
        blocker: true

    performance_gates:
      - metric: "API Latency (p99)"
        required: "< 100ms"
        verification: "Load test report (k6 / Criterion)"
        blocker: true

      - metric: "Memory Usage"
        required: "No leaks, stable under load"
        verification: "72-hour soak test report"
        blocker: true

      - metric: "Throughput"
        required: "≥ baseline requirements"
        verification: "Performance benchmark report"
        blocker: false

    security_gates:
      - metric: "Critical/High Vulnerabilities"
        required: "Zero"
        verification: "cargo-audit, Trivy, Semgrep reports"
        blocker: true

      - metric: "Dependency Audit"
        required: "All licenses approved"
        verification: "cargo-deny report"
        blocker: true

      - metric: "Secrets Scanning"
        required: "Zero secrets in code/config"
        verification: "gitleaks scan"
        blocker: true

    documentation_gates:
      - metric: "API Documentation Coverage"
        required: "100% public API"
        verification: "rustdoc output review"
        blocker: true

      - metric: "Architecture Documentation"
        required: "Complete and approved"
        verification: "Architecture review board sign-off"
        blocker: true

      - metric: "Runbook Documentation"
        required: "All operational procedures documented"
        verification: "Operations team review"
        blocker: true

  artifact_deliverables:
    required_artifacts:
      - name: "Release Candidate Build"
        description: "Fully compiled production binary"
        verification: "Tagged release in artifact repository"
        blocker: true

      - name: "Database Migration Scripts"
        description: "Tested and reversible migrations"
        verification: "Migration test report"
        blocker: true

      - name: "Infrastructure-as-Code"
        description: "Production IaC manifests"
        verification: "IaC repository tagged release"
        blocker: true

      - name: "Test Reports"
        description: "Unit, integration, e2e, mutation test reports"
        verification: "Test report archive"
        blocker: true

      - name: "Security Reports"
        description: "Vulnerability scans, penetration test reports"
        verification: "Security report archive"
        blocker: true

      - name: "Performance Baselines"
        description: "Benchmark results and baselines"
        verification: "Performance report"
        blocker: false

  stakeholder_approvals:
    required_approvals:
      - stakeholder: "VP Engineering / CTO"
        approval: "Technical Production Readiness"
        evidence: "Signed readiness report"
        blocker: true

      - stakeholder: "Principal Security Engineer"
        approval: "Security Certification"
        evidence: "Signed security report"
        blocker: true

      - stakeholder: "QA Lead"
        approval: "Testing Certification"
        evidence: "Signed test report"
        blocker: true

      - stakeholder: "Director of Platform Engineering"
        approval: "Infrastructure Certification"
        evidence: "Signed infrastructure report"
        blocker: true

  gate_enforcement:
    policy: "All blocker dependencies must be satisfied before Phase 5 begins"
    verification: "Automated gate check in CI/CD pipeline"
    override_authority: "CTO + Executive Sponsor (documented exception)"
    exception_process:
      - Document reason for override
      - Define remediation plan
      - Obtain executive approval
      - Track technical debt for post-launch resolution
```

**Entry Criteria for Phase 5**: All blocker dependencies from Phase 4 are satisfied, verified, and documented. Any exceptions require executive-level approval with documented remediation plans.

---

**End of Sections 1-2**

*Sections 3-8 (Deployment Readiness, Operational Handoff, Production Certification, Knowledge Transfer, Support Readiness, Commercial Launch) will be developed in subsequent specification documents.*

---

## Document Metadata

| Field | Value |
|-------|-------|
| **Version** | 1.0.0 |
| **Status** | Draft - Sections 1-2 Complete |
| **SPARC Phase** | Phase 5: Completion |
| **Created** | 2025-11-28 |
| **Ecosystem** | LLM DevOps (24+ Module Platform) |
| **Authors** | Technical Program Management Team |
| **Reviewers** | Pending |
| **Next Sections** | 3-8 (Deployment, Handoff, Certification, Knowledge Transfer, Support, Launch) |

---

*This specification document follows the SPARC methodology (Specification → Pseudocode → Architecture → Refinement → Completion). Phase 5 represents the final stage, transforming refined software into a commercially operational product ready for enterprise deployment.*
---

## Deployment Readiness


---

## Table of Contents

- [3.1 Pre-Deployment Checklist](#31-pre-deployment-checklist)
- [3.2 Environment Certification](#32-environment-certification)
- [3.3 Deployment Artifacts](#33-deployment-artifacts)

---

## 3.1 Pre-Deployment Checklist

### 3.1.1 Code Freeze Verification

**Objective**: Ensure the codebase is stable, tagged, and ready for production deployment with no active development branches.

```yaml
code_freeze:
  branch_verification:
    - action: "Verify main branch is stable"
      command: "git checkout main && git pull origin main"
      validation: "No uncommitted changes or conflicts"

    - action: "Tag release candidate"
      command: "git tag -a v1.0.0-rc.1 -m 'Release candidate for production'"
      validation: "Tag created and pushed to origin"

    - action: "Lock main branch"
      method: "GitHub branch protection rules"
      rules:
        - "Require pull request reviews (2 approvers)"
        - "Require status checks to pass"
        - "Restrict pushes to release managers only"

  dependency_lock:
    - action: "Verify Cargo.lock is committed"
      command: "git ls-files | grep Cargo.lock"
      validation: "Cargo.lock present and up-to-date"

    - action: "Pin all dependencies"
      file: "Cargo.toml"
      requirement: "All dependencies use exact versions (=x.y.z)"

    - action: "Audit dependency tree"
      command: "cargo tree --locked > dependency-tree.txt"
      validation: "No version conflicts or ambiguous dependencies"

  build_verification:
    - action: "Clean production build"
      command: "cargo clean && cargo build --release --locked"
      validation: "Build completes with zero warnings"

    - action: "Verify binary reproducibility"
      method: "Rebuild twice and compare checksums"
      validation: "SHA256 checksums match across builds"
```

**Checklist**:
- [ ] Main branch is stable and up-to-date
- [ ] Release tag created (e.g., v1.0.0-rc.1)
- [ ] Branch protection rules enforced
- [ ] Cargo.lock committed and verified
- [ ] All dependencies pinned to exact versions
- [ ] Production build succeeds with zero warnings
- [ ] Binary checksums verified for reproducibility

---

### 3.1.2 Quality Gates Passed

**Objective**: Validate all code quality, security, and performance thresholds are met before deployment.

```yaml
quality_gates:
  code_coverage:
    metric: "Line coverage ≥ 85%"
    command: "cargo tarpaulin --out Xml --output-dir ./coverage"
    validation: "coverage.xml shows ≥85% line coverage"
    blocker: true

  mutation_testing:
    metric: "Mutation score ≥ 70%"
    command: "cargo mutants --output mutants.json"
    validation: "Mutation score ≥70% in mutants.json"
    blocker: true

  static_analysis:
    - tool: "Clippy (pedantic mode)"
      command: "cargo clippy --all-targets --all-features -- -D warnings -W clippy::pedantic"
      validation: "Zero warnings or errors"
      blocker: true

    - tool: "Rustfmt"
      command: "cargo fmt -- --check"
      validation: "All code formatted consistently"
      blocker: true

  security_scanning:
    - tool: "cargo-audit"
      command: "cargo audit --deny warnings"
      validation: "Zero critical/high vulnerabilities"
      blocker: true

    - tool: "Trivy (container scan)"
      command: "trivy image llm-research-lab:latest --severity CRITICAL,HIGH"
      validation: "Zero critical/high vulnerabilities"
      blocker: true

    - tool: "Semgrep"
      command: "semgrep --config=auto --severity=ERROR ."
      validation: "Zero security-relevant errors"
      blocker: true

    - tool: "gitleaks"
      command: "gitleaks detect --no-git --source=."
      validation: "Zero secrets detected"
      blocker: true

  performance_validation:
    - metric: "API latency p99 < 100ms"
      tool: "k6 load tests"
      validation: "Load test report confirms p99 < 100ms"
      blocker: true

    - metric: "Memory stability"
      tool: "72-hour soak test"
      validation: "No memory leaks or degradation"
      blocker: true
```

**Quality Gate Summary**:

| Gate | Metric | Target | Status | Blocker |
|------|--------|--------|--------|---------|
| Code Coverage | Line coverage | ≥85% | ✅ Pass | Yes |
| Mutation Testing | Mutation score | ≥70% | ✅ Pass | Yes |
| Clippy | Warnings | 0 | ✅ Pass | Yes |
| Cargo Audit | Critical/High vulns | 0 | ✅ Pass | Yes |
| Trivy Scan | Container vulns | 0 | ✅ Pass | Yes |
| API Latency | p99 latency | <100ms | ✅ Pass | Yes |

---

### 3.1.3 Documentation Complete

**Objective**: Ensure all customer-facing and internal documentation is complete, accurate, and accessible.

```yaml
documentation_completeness:
  api_documentation:
    - artifact: "OpenAPI 3.0 specification"
      location: "docs/api/openapi.yaml"
      validation: "All endpoints documented with examples"
      coverage: "100% public API"

    - artifact: "rustdoc generated docs"
      command: "cargo doc --no-deps --document-private-items"
      location: "target/doc/llm_research_lab/index.html"
      validation: "All public APIs have doc comments"

  operational_documentation:
    - artifact: "Deployment Runbook"
      location: "docs/operations/deployment.md"
      sections:
        - "Prerequisites and dependencies"
        - "Step-by-step deployment procedure"
        - "Rollback procedure"
        - "Smoke testing checklist"

    - artifact: "Incident Response Playbook"
      location: "docs/operations/incident-response.md"
      sections:
        - "Alert triage and severity classification"
        - "Common failure modes and remediation"
        - "Escalation paths and contact information"

    - artifact: "Monitoring and Observability Guide"
      location: "docs/operations/monitoring.md"
      sections:
        - "Key metrics and dashboard locations"
        - "Alert definitions and thresholds"
        - "Log aggregation queries"

  customer_documentation:
    - artifact: "Getting Started Guide"
      location: "docs/user/getting-started.md"
      validation: "New user can onboard in <30 minutes"

    - artifact: "API Reference"
      location: "docs/user/api-reference.md"
      validation: "All endpoints documented with curl examples"

    - artifact: "Troubleshooting Guide"
      location: "docs/user/troubleshooting.md"
      validation: "Top 10 common issues documented"
```

**Documentation Checklist**:
- [ ] OpenAPI 3.0 specification complete
- [ ] Rustdoc generated with 100% public API coverage
- [ ] Deployment runbook reviewed and validated
- [ ] Incident response playbook approved by SRE
- [ ] Monitoring guide includes all critical dashboards
- [ ] Getting Started guide tested with new users
- [ ] API reference includes examples for all endpoints

---

### 3.1.4 Release Notes Finalized

**Objective**: Communicate changes, improvements, and migration requirements to stakeholders.

```markdown
# Release Notes v1.0.0

## Release Information
- **Version**: 1.0.0
- **Release Date**: 2025-12-15
- **Release Type**: General Availability (GA)
- **SPARC Phase**: Completion (Phase 5)

## What's New

### Experiment Tracking
- Full-lifecycle experiment management with versioning and provenance
- Integration with LLM-Registry for artifact storage
- Real-time metrics streaming to LLM-Analytics-Hub

### Metric Benchmarking
- Standardized framework for custom evaluation metrics
- Statistical significance testing with confidence intervals
- Comparative analysis dashboards

### Dataset Versioning
- Content-addressable storage with cryptographic verification
- Lineage tracking for dataset transformations
- Integration with LLM-Data-Vault for governance

## Breaking Changes
- None (initial GA release)

## Migration Guide
- **New Deployments**: Follow the Getting Started guide in docs/user/getting-started.md
- **Database Schema**: Initial schema deployment via migrations/001_initial_schema.sql

## Known Issues
- None at GA release

## Performance Improvements
- API latency p99 < 100ms under 5x expected load
- Experiment throughput: 200+ experiments/week

## Security Enhancements
- Zero critical/high vulnerabilities (cargo-audit, Trivy, Semgrep)
- TLS 1.3 enforcement for all API endpoints
- mTLS for inter-service communication

## Dependencies
- Rust: 1.75+
- PostgreSQL: 13+
- ClickHouse: 22+
- Kubernetes: 1.24+

## Upgrade Instructions
- Not applicable (initial GA release)

## Documentation
- API Reference: https://docs.llm-research-lab/api
- User Guide: https://docs.llm-research-lab/user-guide
- Operations Manual: https://docs.llm-research-lab/operations

## Support
- GitHub Issues: https://github.com/org/llm-research-lab/issues
- Slack: #llm-research-lab-support
- Email: support@llm-devops.example.com
```

---

### 3.1.5 Stakeholder Approvals

**Objective**: Obtain formal sign-off from all required stakeholders before production deployment.

```yaml
stakeholder_approvals:
  technical_approvals:
    - role: "VP Engineering / CTO"
      deliverable: "Technical Production Readiness Report"
      criteria:
        - "All quality gates passed"
        - "Architecture reviewed and approved"
        - "No critical technical debt"
      status: "Pending"
      sign_off_date: null

    - role: "Principal Security Engineer"
      deliverable: "Security Certification Report"
      criteria:
        - "Zero critical/high vulnerabilities"
        - "Penetration testing passed"
        - "Security compliance checklist complete"
      status: "Pending"
      sign_off_date: null

    - role: "Director of Platform Engineering"
      deliverable: "Infrastructure Certification"
      criteria:
        - "Production infrastructure deployed and tested"
        - "Monitoring and observability operational"
        - "Disaster recovery validated"
      status: "Pending"
      sign_off_date: null

  operational_approvals:
    - role: "VP Operations / Head of SRE"
      deliverable: "Operational Readiness Certification"
      criteria:
        - "Operations team trained"
        - "Runbooks complete and validated"
        - "On-call rotation staffed"
      status: "Pending"
      sign_off_date: null

  business_approvals:
    - role: "VP Product"
      deliverable: "Product Launch Readiness"
      criteria:
        - "Product meets requirements"
        - "Customer onboarding tested"
        - "Success metrics defined"
      status: "Pending"
      sign_off_date: null
```

**Approval Checklist**:
- [ ] VP Engineering sign-off on technical readiness
- [ ] Security Engineer sign-off on security certification
- [ ] Platform Engineering sign-off on infrastructure
- [ ] VP Operations sign-off on operational readiness
- [ ] VP Product sign-off on product launch readiness

---

## 3.2 Environment Certification

### 3.2.1 Production Environment Validation

**Objective**: Verify production infrastructure is provisioned, configured, and operational before deploying application code.

```yaml
production_environment:
  kubernetes_cluster:
    provider: "AWS EKS / GCP GKE / Azure AKS"
    version: "1.28+"
    configuration:
      node_pools:
        - name: "system-pool"
          machine_type: "n2-standard-4"
          min_nodes: 3
          max_nodes: 5
          labels:
            workload: "system"

        - name: "application-pool"
          machine_type: "n2-standard-8"
          min_nodes: 5
          max_nodes: 20
          labels:
            workload: "application"

        - name: "gpu-pool"
          machine_type: "n1-standard-8 (with T4 GPUs)"
          min_nodes: 0
          max_nodes: 10
          labels:
            workload: "gpu-inference"
          taints:
            - key: "nvidia.com/gpu"
              value: "true"
              effect: "NoSchedule"

      addons:
        - "metrics-server"
        - "cluster-autoscaler"
        - "vertical-pod-autoscaler"
        - "cert-manager"
        - "ingress-nginx"

  databases:
    postgresql:
      provider: "AWS RDS / Cloud SQL / Azure Database"
      version: "15.4"
      instance_class: "db.r6g.xlarge"
      storage: "500GB gp3"
      configuration:
        multi_az: true
        backup_retention: "30 days"
        point_in_time_recovery: true
        encryption_at_rest: true
        ssl_mode: "require"
      validation:
        - "pg_isready -h $POSTGRES_HOST -p 5432"
        - "psql -c 'SELECT version();'"

    clickhouse:
      deployment: "Kubernetes StatefulSet"
      version: "23.8"
      replicas: 3
      storage: "1TB per replica (SSD)"
      configuration:
        replication: "Replicated tables with Zookeeper coordination"
        sharding: "3 shards for distributed queries"
        compression: "LZ4"
      validation:
        - "clickhouse-client --query='SELECT version()'"
        - "clickhouse-client --query='SELECT count() FROM system.clusters'"

  message_queue:
    provider: "Redis Streams"
    deployment: "Redis Sentinel cluster"
    configuration:
      replicas: 3
      persistence: "AOF + RDB snapshots"
      max_memory: "16GB per instance"
    validation:
      - "redis-cli ping"
      - "redis-cli info replication"

  object_storage:
    provider: "AWS S3 / GCS / MinIO"
    buckets:
      - name: "llm-research-lab-artifacts"
        versioning: true
        lifecycle_policy: "Transition to Glacier after 90 days"
        encryption: "AES-256"

      - name: "llm-research-lab-datasets"
        versioning: true
        lifecycle_policy: "Retain indefinitely"
        encryption: "AES-256"
    validation:
      - "aws s3 ls s3://llm-research-lab-artifacts"
      - "aws s3api get-bucket-versioning --bucket llm-research-lab-artifacts"

  networking:
    load_balancer:
      type: "Application Load Balancer (Layer 7)"
      tls_termination: true
      certificate: "ACM / Let's Encrypt (cert-manager)"
      health_checks:
        path: "/health"
        interval: "10s"
        timeout: "5s"
        healthy_threshold: 2
        unhealthy_threshold: 3

    dns:
      domain: "api.llm-research-lab.example.com"
      provider: "Route53 / Cloud DNS"
      ttl: "60s"
      validation:
        - "nslookup api.llm-research-lab.example.com"
        - "curl -I https://api.llm-research-lab.example.com/health"

    tls_certificates:
      provider: "cert-manager with Let's Encrypt"
      renewal: "Automatic (30 days before expiry)"
      validation:
        - "kubectl get certificate -n llm-research-lab"
        - "openssl s_client -connect api.llm-research-lab.example.com:443 -servername api.llm-research-lab.example.com"
```

**Validation Checklist**:
- [ ] Kubernetes cluster operational with all node pools
- [ ] PostgreSQL database accessible with TLS
- [ ] ClickHouse cluster operational with replication
- [ ] Redis Sentinel cluster accessible
- [ ] S3 buckets created with versioning enabled
- [ ] Load balancer configured with TLS termination
- [ ] DNS records resolving correctly
- [ ] TLS certificates issued and valid

---

### 3.2.2 Infrastructure-as-Code Verification

**Objective**: Ensure all infrastructure is defined declaratively, version-controlled, and reproducible.

```yaml
iac_verification:
  terraform:
    version: "1.6+"
    modules:
      - name: "networking"
        path: "terraform/modules/networking"
        resources:
          - "VPC with public/private subnets"
          - "NAT gateways for outbound traffic"
          - "Security groups for database access"

      - name: "kubernetes"
        path: "terraform/modules/kubernetes"
        resources:
          - "EKS/GKE/AKS cluster"
          - "Node groups with autoscaling"
          - "IRSA/Workload Identity for service accounts"

      - name: "databases"
        path: "terraform/modules/databases"
        resources:
          - "PostgreSQL RDS instance"
          - "ClickHouse StatefulSet manifests"
          - "Database security groups"

      - name: "storage"
        path: "terraform/modules/storage"
        resources:
          - "S3 buckets with lifecycle policies"
          - "IAM roles for bucket access"

    validation:
      - command: "terraform fmt -check -recursive"
        description: "Verify Terraform code is formatted"

      - command: "terraform validate"
        description: "Validate Terraform syntax"

      - command: "terraform plan -out=tfplan"
        description: "Generate execution plan"

      - command: "terraform show -json tfplan | jq '.resource_changes | length'"
        description: "Verify no unexpected changes"

  helm_charts:
    version: "3.12+"
    charts:
      - name: "llm-research-lab"
        path: "helm/llm-research-lab"
        values_files:
          - "values.production.yaml"
        validation:
          - "helm lint helm/llm-research-lab"
          - "helm template llm-research-lab helm/llm-research-lab -f helm/values.production.yaml | kubeval --strict"

      - name: "monitoring-stack"
        path: "helm/monitoring"
        components:
          - "Prometheus"
          - "Grafana"
          - "Jaeger"
        validation:
          - "helm lint helm/monitoring"
          - "helm template monitoring helm/monitoring | kubeval --strict"

  kubernetes_manifests:
    location: "k8s/"
    structure:
      - "k8s/base/ (common manifests)"
      - "k8s/overlays/production/ (environment-specific)"
    tool: "Kustomize"
    validation:
      - "kustomize build k8s/overlays/production | kubeval --strict"
      - "kustomize build k8s/overlays/production | kubectl apply --dry-run=server -f -"
```

**IaC Validation Commands**:

```bash
# Terraform validation
cd terraform/environments/production
terraform init
terraform fmt -check -recursive
terraform validate
terraform plan -out=tfplan
terraform apply tfplan

# Helm validation
helm lint helm/llm-research-lab
helm template llm-research-lab helm/llm-research-lab \
  -f helm/values.production.yaml \
  --validate

# Kustomize validation
kustomize build k8s/overlays/production | kubeval --strict
kustomize build k8s/overlays/production | kubectl apply --dry-run=server -f -
```

---

### 3.2.3 Secrets Management Audit

**Objective**: Verify all secrets are securely managed, rotated, and accessed through approved mechanisms.

```yaml
secrets_management:
  secrets_provider:
    primary: "AWS Secrets Manager / GCP Secret Manager / Azure Key Vault"
    kubernetes_integration: "External Secrets Operator"
    configuration:
      rotation_policy: "Automatic rotation every 90 days"
      access_control: "IAM-based with least-privilege"
      audit_logging: "All secret access logged to CloudTrail/Cloud Audit Logs"

  secret_categories:
    database_credentials:
      - name: "postgresql-admin-password"
        rotation: "90 days"
        access: "application-service-account only"

      - name: "clickhouse-admin-password"
        rotation: "90 days"
        access: "application-service-account only"

    api_keys:
      - name: "llm-registry-api-key"
        rotation: "180 days"
        access: "application-service-account only"

      - name: "llm-data-vault-api-key"
        rotation: "180 days"
        access: "application-service-account only"

    tls_certificates:
      - name: "mtls-client-certificate"
        rotation: "365 days (automated via cert-manager)"
        access: "ingress-controller, application pods"

    encryption_keys:
      - name: "data-encryption-key (DEK)"
        rotation: "Annual (with re-encryption)"
        access: "application-service-account only"
        key_management: "AWS KMS / Cloud KMS envelope encryption"

  kubernetes_secrets:
    storage_class: "external-secrets (synced from cloud provider)"
    encryption_at_rest: true
    rbac:
      - "ServiceAccount 'llm-research-lab-app' can read secrets in namespace 'llm-research-lab'"
      - "No direct kubectl access to secrets in production"

  validation:
    - action: "Audit secret access logs"
      command: "aws cloudtrail lookup-events --lookup-attributes AttributeKey=ResourceName,AttributeValue=postgresql-admin-password"
      frequency: "Weekly"

    - action: "Verify no hardcoded secrets"
      command: "gitleaks detect --no-git --source=."
      frequency: "Every commit (CI/CD)"

    - action: "Rotate test secrets"
      description: "Verify rotation mechanism works without downtime"
      frequency: "Quarterly"

    - action: "Review secret access policies"
      description: "Ensure least-privilege is maintained"
      frequency: "Quarterly"
```

**Secrets Audit Checklist**:
- [ ] All secrets stored in managed secret service (no K8s native secrets for sensitive data)
- [ ] Automatic rotation enabled for database credentials
- [ ] External Secrets Operator configured and syncing
- [ ] No secrets detected in Git history (gitleaks scan passed)
- [ ] IAM policies enforce least-privilege access
- [ ] Audit logging enabled for all secret access
- [ ] mTLS certificates managed by cert-manager
- [ ] Encryption-at-rest enabled for Kubernetes secrets

---

### 3.2.4 Network Security Validation

**Objective**: Verify network segmentation, firewall rules, and traffic policies enforce zero-trust principles.

```yaml
network_security:
  network_segmentation:
    architecture: "VPC with public, private, and database subnets"
    subnets:
      - name: "public-subnet"
        purpose: "Load balancers, NAT gateways"
        cidr: "10.0.1.0/24"
        internet_access: "Inbound + Outbound"

      - name: "private-subnet"
        purpose: "Application pods"
        cidr: "10.0.10.0/24"
        internet_access: "Outbound only (via NAT)"

      - name: "database-subnet"
        purpose: "PostgreSQL, ClickHouse"
        cidr: "10.0.20.0/24"
        internet_access: "None (isolated)"

    validation:
      - "Verify application pods cannot directly access internet (must use NAT)"
      - "Verify database subnets have no internet gateway route"

  firewall_rules:
    ingress:
      - name: "allow-https"
        protocol: "TCP"
        port: 443
        source: "0.0.0.0/0"
        destination: "Load balancer"

      - name: "allow-internal-app-traffic"
        protocol: "TCP"
        port: "8080, 8081"
        source: "private-subnet"
        destination: "private-subnet"

      - name: "allow-db-access"
        protocol: "TCP"
        port: "5432 (Postgres), 9000 (ClickHouse)"
        source: "private-subnet"
        destination: "database-subnet"

    egress:
      - name: "allow-outbound-https"
        protocol: "TCP"
        port: 443
        source: "private-subnet"
        destination: "0.0.0.0/0 (via NAT)"

      - name: "deny-all-other-egress"
        protocol: "ALL"
        source: "database-subnet"
        destination: "0.0.0.0/0"
        action: "DENY"

    validation:
      - "Attempt database connection from public subnet (should fail)"
      - "Attempt outbound connection from database subnet (should fail)"
      - "Verify application can connect to databases"

  kubernetes_network_policies:
    default_policy: "Deny all ingress/egress"
    policies:
      - name: "allow-app-to-db"
        selector:
          app: "llm-research-lab"
        ingress: []
        egress:
          - to:
              - podSelector:
                  matchLabels:
                    component: "postgresql"
            ports:
              - protocol: "TCP"
                port: 5432

      - name: "allow-ingress-to-app"
        selector:
          app: "llm-research-lab"
        ingress:
          - from:
              - namespaceSelector:
                  matchLabels:
                    name: "ingress-nginx"
            ports:
              - protocol: "TCP"
                port: 8080

    validation:
      - "kubectl exec -it test-pod -- curl http://llm-research-lab-service:8080/health (from unauthorized namespace should fail)"
      - "kubectl exec -it test-pod -- nc -zv postgresql-service 5432 (from unauthorized namespace should fail)"

  tls_enforcement:
    ingress_tls:
      minimum_version: "TLS 1.3"
      cipher_suites:
        - "TLS_AES_128_GCM_SHA256"
        - "TLS_AES_256_GCM_SHA384"
        - "TLS_CHACHA20_POLY1305_SHA256"
      validation:
        - "nmap --script ssl-enum-ciphers -p 443 api.llm-research-lab.example.com"
        - "testssl.sh --severity MEDIUM api.llm-research-lab.example.com"

    service_mesh_mtls:
      provider: "Istio / Linkerd (optional)"
      mode: "STRICT (enforce mTLS for all inter-pod traffic)"
      validation:
        - "istioctl authn tls-check llm-research-lab-pod.llm-research-lab"
```

**Network Security Validation Commands**:

```bash
# Test network segmentation
kubectl run test-pod --image=busybox --restart=Never --rm -it -- \
  nc -zv postgresql-service.llm-research-lab.svc.cluster.local 5432
# Expected: Connection should only succeed from authorized namespaces

# Test TLS configuration
nmap --script ssl-enum-ciphers -p 443 api.llm-research-lab.example.com
# Expected: Only TLS 1.3 with strong ciphers

# Test network policies
kubectl exec -it unauthorized-pod -n default -- \
  curl http://llm-research-lab-service.llm-research-lab.svc.cluster.local:8080/health
# Expected: Connection refused (network policy blocks)
```

**Network Security Checklist**:
- [ ] VPC subnets properly segmented (public, private, database)
- [ ] Database subnets have no internet access
- [ ] Security groups enforce least-privilege access
- [ ] Kubernetes NetworkPolicies deny all by default
- [ ] TLS 1.3 enforced with strong cipher suites
- [ ] mTLS configured for inter-service communication (if using service mesh)
- [ ] Network policy tests passed (unauthorized access blocked)

---

## 3.3 Deployment Artifacts

### 3.3.1 Container Image Specifications

**Objective**: Build optimized, secure, multi-stage Docker images for production deployment.

**Multi-Stage Dockerfile**:

```dockerfile
# syntax=docker/dockerfile:1.4

#############################################
# Stage 1: Build environment
#############################################
FROM rust:1.75-slim-bookworm AS builder

# Install build dependencies
RUN apt-get update && apt-get install -y \
    pkg-config \
    libssl-dev \
    libpq-dev \
    && rm -rf /var/lib/apt/lists/*

# Create non-root user for build
RUN useradd -m -u 10001 builder
USER builder
WORKDIR /app

# Copy dependency manifests and build dependencies first (caching layer)
COPY --chown=builder:builder Cargo.toml Cargo.lock ./
RUN mkdir src && echo "fn main() {}" > src/main.rs
RUN cargo build --release --locked && rm -rf src

# Copy source code and build application
COPY --chown=builder:builder . .
RUN touch src/main.rs && cargo build --release --locked

# Strip debug symbols to reduce binary size
RUN strip target/release/llm-research-lab

#############################################
# Stage 2: Runtime environment
#############################################
FROM debian:bookworm-slim

# Install runtime dependencies only
RUN apt-get update && apt-get install -y \
    ca-certificates \
    libssl3 \
    libpq5 \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Create non-root user for runtime
RUN useradd -m -u 10001 -s /bin/bash appuser

# Copy application binary from builder
COPY --from=builder /app/target/release/llm-research-lab /usr/local/bin/llm-research-lab

# Set ownership and permissions
RUN chown appuser:appuser /usr/local/bin/llm-research-lab \
    && chmod 755 /usr/local/bin/llm-research-lab

# Create application directories
RUN mkdir -p /app/config /app/logs \
    && chown -R appuser:appuser /app

# Switch to non-root user
USER appuser
WORKDIR /app

# Health check
HEALTHCHECK --interval=30s --timeout=5s --start-period=10s --retries=3 \
  CMD curl -f http://localhost:8080/health || exit 1

# Expose application port
EXPOSE 8080

# Run application
ENTRYPOINT ["/usr/local/bin/llm-research-lab"]
CMD ["--config", "/app/config/config.yaml"]
```

**Image Build Commands**:

```bash
# Build image with BuildKit for optimal caching
DOCKER_BUILDKIT=1 docker build \
  --tag llm-research-lab:v1.0.0 \
  --tag llm-research-lab:latest \
  --build-arg RUST_VERSION=1.75 \
  --file Dockerfile \
  .

# Scan image for vulnerabilities
trivy image --severity CRITICAL,HIGH llm-research-lab:v1.0.0

# Inspect image layers and size
docker images llm-research-lab:v1.0.0
docker history llm-research-lab:v1.0.0

# Push to registry
docker tag llm-research-lab:v1.0.0 gcr.io/my-project/llm-research-lab:v1.0.0
docker push gcr.io/my-project/llm-research-lab:v1.0.0
```

**Image Optimization**:

| Optimization | Benefit | Implementation |
|--------------|---------|----------------|
| Multi-stage build | Reduced image size | Separate builder and runtime stages |
| Dependency caching | Faster builds | Copy Cargo.toml/Cargo.lock before source |
| Binary stripping | Smaller image | `strip target/release/llm-research-lab` |
| Minimal base image | Reduced attack surface | debian:bookworm-slim (no unnecessary tools) |
| Non-root user | Security hardening | Run as UID 10001 |
| Layer optimization | Better caching | Group RUN commands, clean apt cache |

**Expected Image Size**: ~150-200MB (vs ~2GB without optimization)

---

### 3.3.2 Helm Chart Requirements

**Objective**: Define declarative Kubernetes deployment manifests with environment-specific overrides.

**Helm Chart Structure**:

```
helm/llm-research-lab/
├── Chart.yaml                 # Chart metadata
├── values.yaml                # Default values
├── values.production.yaml     # Production overrides
├── values.staging.yaml        # Staging overrides
├── templates/
│   ├── _helpers.tpl           # Template helpers
│   ├── deployment.yaml        # Application Deployment
│   ├── service.yaml           # Service definition
│   ├── ingress.yaml           # Ingress configuration
│   ├── configmap.yaml         # Application configuration
│   ├── secret.yaml            # Secret references (External Secrets)
│   ├── hpa.yaml               # HorizontalPodAutoscaler
│   ├── pdb.yaml               # PodDisruptionBudget
│   ├── servicemonitor.yaml    # Prometheus ServiceMonitor
│   └── networkpolicy.yaml     # NetworkPolicy
└── README.md                  # Chart documentation
```

**Chart.yaml**:

```yaml
apiVersion: v2
name: llm-research-lab
description: Helm chart for LLM Research Lab application
type: application
version: 1.0.0
appVersion: "1.0.0"
keywords:
  - llm
  - research
  - experiment-tracking
maintainers:
  - name: Platform Team
    email: platform@example.com
dependencies: []
```

**values.production.yaml** (Production Overrides):

```yaml
# Production configuration for LLM Research Lab

replicaCount: 5

image:
  repository: gcr.io/my-project/llm-research-lab
  pullPolicy: IfNotPresent
  tag: "v1.0.0"

imagePullSecrets:
  - name: gcr-json-key

nameOverride: ""
fullnameOverride: "llm-research-lab"

serviceAccount:
  create: true
  annotations:
    iam.gke.io/gcp-service-account: "llm-research-lab@my-project.iam.gserviceaccount.com"
  name: "llm-research-lab"

podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "8081"
  prometheus.io/path: "/metrics"

podSecurityContext:
  runAsNonRoot: true
  runAsUser: 10001
  fsGroup: 10001
  seccompProfile:
    type: RuntimeDefault

securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
      - ALL
  readOnlyRootFilesystem: true

service:
  type: ClusterIP
  port: 80
  targetPort: 8080
  annotations:
    cloud.google.com/neg: '{"ingress": true}'

ingress:
  enabled: true
  className: "nginx"
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/ssl-protocols: "TLSv1.3"
    nginx.ingress.kubernetes.io/ssl-ciphers: "TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384"
  hosts:
    - host: api.llm-research-lab.example.com
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: llm-research-lab-tls
      hosts:
        - api.llm-research-lab.example.com

resources:
  limits:
    cpu: 2000m
    memory: 4Gi
  requests:
    cpu: 1000m
    memory: 2Gi

autoscaling:
  enabled: true
  minReplicas: 5
  maxReplicas: 20
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

nodeSelector:
  workload: application

tolerations: []

affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                  - llm-research-lab
          topologyKey: kubernetes.io/hostname

livenessProbe:
  httpGet:
    path: /health/liveness
    port: 8080
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

readinessProbe:
  httpGet:
    path: /health/readiness
    port: 8080
  initialDelaySeconds: 10
  periodSeconds: 5
  timeoutSeconds: 3
  failureThreshold: 3

# Environment-specific configuration
config:
  log_level: "info"
  database_url: "postgres://llm-research-lab:PASSWORD@postgres-service:5432/llm_research_lab?sslmode=require"
  clickhouse_url: "http://clickhouse-service:8123"
  redis_url: "redis://redis-sentinel:26379/0"
  s3_bucket: "llm-research-lab-artifacts"
  s3_region: "us-west-2"

# External Secrets Operator integration
externalSecrets:
  enabled: true
  secretStore:
    name: "aws-secrets-manager"
    kind: "ClusterSecretStore"
  secrets:
    - name: "postgresql-credentials"
      remoteRef:
        key: "production/llm-research-lab/postgresql"
    - name: "clickhouse-credentials"
      remoteRef:
        key: "production/llm-research-lab/clickhouse"

podDisruptionBudget:
  enabled: true
  minAvailable: 3

networkPolicy:
  enabled: true
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              name: ingress-nginx
      ports:
        - protocol: TCP
          port: 8080
  egress:
    - to:
        - namespaceSelector: {}
          podSelector:
            matchLabels:
              component: postgresql
      ports:
        - protocol: TCP
          port: 5432
    - to:
        - namespaceSelector: {}
          podSelector:
            matchLabels:
              component: clickhouse
      ports:
        - protocol: TCP
          port: 9000
```

**Helm Deployment Commands**:

```bash
# Validate chart
helm lint helm/llm-research-lab
helm template llm-research-lab helm/llm-research-lab \
  -f helm/values.production.yaml \
  --validate

# Dry-run deployment
helm upgrade --install llm-research-lab helm/llm-research-lab \
  -f helm/values.production.yaml \
  --namespace llm-research-lab \
  --create-namespace \
  --dry-run --debug

# Deploy to production
helm upgrade --install llm-research-lab helm/llm-research-lab \
  -f helm/values.production.yaml \
  --namespace llm-research-lab \
  --create-namespace \
  --wait --timeout 10m

# Verify deployment
kubectl get pods -n llm-research-lab
kubectl rollout status deployment/llm-research-lab -n llm-research-lab
```

---

### 3.3.3 ConfigMaps and Secrets Structure

**Objective**: Manage application configuration and sensitive data securely and declaratively.

**ConfigMap Example** (`templates/configmap.yaml`):

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "llm-research-lab.fullname" . }}-config
  labels:
    {{- include "llm-research-lab.labels" . | nindent 4 }}
data:
  config.yaml: |
    server:
      host: "0.0.0.0"
      port: 8080
      metrics_port: 8081
      shutdown_timeout_secs: 30

    logging:
      level: {{ .Values.config.log_level | quote }}
      format: "json"
      output: "stdout"

    database:
      max_connections: 20
      min_connections: 5
      connection_timeout_secs: 30
      idle_timeout_secs: 600
      max_lifetime_secs: 1800

    clickhouse:
      max_connections: 10
      query_timeout_secs: 60
      compression: "lz4"

    redis:
      max_connections: 10
      connection_timeout_secs: 5
      command_timeout_secs: 10

    s3:
      region: {{ .Values.config.s3_region | quote }}
      bucket: {{ .Values.config.s3_bucket | quote }}
      endpoint: null  # Use default AWS endpoint

    observability:
      metrics_enabled: true
      tracing_enabled: true
      tracing_endpoint: "http://jaeger-collector:14268/api/traces"
      tracing_sample_rate: 0.1

    integrations:
      llm_registry_url: "https://registry.llm-devops.example.com"
      llm_data_vault_url: "https://data-vault.llm-devops.example.com"
      llm_analytics_hub_url: "https://analytics.llm-devops.example.com"
      llm_test_bench_url: "https://test-bench.llm-devops.example.com"
```

**External Secret Example** (`templates/secret.yaml`):

```yaml
{{- if .Values.externalSecrets.enabled }}
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: {{ include "llm-research-lab.fullname" . }}-secrets
  labels:
    {{- include "llm-research-lab.labels" . | nindent 4 }}
spec:
  secretStoreRef:
    name: {{ .Values.externalSecrets.secretStore.name }}
    kind: {{ .Values.externalSecrets.secretStore.kind }}

  target:
    name: {{ include "llm-research-lab.fullname" . }}-secrets
    creationPolicy: Owner

  dataFrom:
    - extract:
        key: production/llm-research-lab/postgresql
    - extract:
        key: production/llm-research-lab/clickhouse
    - extract:
        key: production/llm-research-lab/api-keys

  refreshInterval: 1h
{{- end }}
```

**Secrets Consumption in Deployment** (`templates/deployment.yaml` excerpt):

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "llm-research-lab.fullname" . }}
spec:
  template:
    spec:
      containers:
        - name: {{ .Chart.Name }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
          env:
            # Non-sensitive configuration from ConfigMap
            - name: CONFIG_FILE
              value: "/app/config/config.yaml"

            # Sensitive credentials from External Secrets
            - name: DATABASE_URL
              valueFrom:
                secretKeyRef:
                  name: {{ include "llm-research-lab.fullname" . }}-secrets
                  key: postgresql-url

            - name: CLICKHOUSE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: {{ include "llm-research-lab.fullname" . }}-secrets
                  key: clickhouse-password

            - name: LLM_REGISTRY_API_KEY
              valueFrom:
                secretKeyRef:
                  name: {{ include "llm-research-lab.fullname" . }}-secrets
                  key: registry-api-key

          volumeMounts:
            - name: config
              mountPath: /app/config
              readOnly: true

            - name: tmp
              mountPath: /tmp

      volumes:
        - name: config
          configMap:
            name: {{ include "llm-research-lab.fullname" . }}-config

        - name: tmp
          emptyDir: {}
```

**Configuration Management Best Practices**:

| Category | Best Practice | Implementation |
|----------|---------------|----------------|
| Secrets Storage | Never commit secrets to Git | Use External Secrets Operator |
| Secret Rotation | Automate rotation | AWS Secrets Manager auto-rotation |
| Configuration Versioning | Track config changes | ConfigMaps versioned with Helm releases |
| Environment Separation | Separate configs per environment | values.production.yaml vs values.staging.yaml |
| Least Privilege | Minimal secret access | RBAC policies limit secret access |
| Audit Logging | Log all secret access | Cloud provider audit logs enabled |

---

### 3.3.4 Database Migration Strategy

**Objective**: Manage database schema evolution safely with versioned, reversible migrations.

**Migration Framework**: `sqlx` (Rust-native SQL toolkit with compile-time verification)

**Migration Directory Structure**:

```
migrations/
├── 001_initial_schema.sql
├── 002_add_experiment_metadata.sql
├── 003_add_dataset_versioning.sql
└── 004_add_metrics_benchmarking.sql
```

**Migration Example** (`migrations/001_initial_schema.sql`):

```sql
-- Initial schema for LLM Research Lab
-- Version: 1.0.0
-- Applied: 2025-12-01

-- Enable UUID extension
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";

-- Experiments table
CREATE TABLE experiments (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    name VARCHAR(255) NOT NULL,
    description TEXT,
    status VARCHAR(50) NOT NULL DEFAULT 'pending',
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    created_by VARCHAR(255) NOT NULL,
    metadata JSONB NOT NULL DEFAULT '{}',

    CONSTRAINT valid_status CHECK (status IN ('pending', 'running', 'completed', 'failed', 'cancelled'))
);

CREATE INDEX idx_experiments_status ON experiments(status);
CREATE INDEX idx_experiments_created_at ON experiments(created_at DESC);
CREATE INDEX idx_experiments_created_by ON experiments(created_by);

-- Datasets table
CREATE TABLE datasets (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    name VARCHAR(255) NOT NULL,
    version VARCHAR(50) NOT NULL,
    content_hash VARCHAR(64) NOT NULL UNIQUE,  -- SHA-256 hash
    storage_uri TEXT NOT NULL,
    size_bytes BIGINT NOT NULL,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    metadata JSONB NOT NULL DEFAULT '{}',

    UNIQUE(name, version)
);

CREATE INDEX idx_datasets_content_hash ON datasets(content_hash);
CREATE INDEX idx_datasets_name_version ON datasets(name, version);

-- Metrics table
CREATE TABLE metrics (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    experiment_id UUID NOT NULL REFERENCES experiments(id) ON DELETE CASCADE,
    metric_name VARCHAR(255) NOT NULL,
    metric_value DOUBLE PRECISION NOT NULL,
    timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    metadata JSONB NOT NULL DEFAULT '{}'
);

CREATE INDEX idx_metrics_experiment_id ON metrics(experiment_id);
CREATE INDEX idx_metrics_metric_name ON metrics(metric_name);
CREATE INDEX idx_metrics_timestamp ON metrics(timestamp DESC);

-- Experiment-Dataset associations
CREATE TABLE experiment_datasets (
    experiment_id UUID NOT NULL REFERENCES experiments(id) ON DELETE CASCADE,
    dataset_id UUID NOT NULL REFERENCES datasets(id) ON DELETE CASCADE,
    role VARCHAR(50) NOT NULL,  -- 'train', 'validation', 'test'
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),

    PRIMARY KEY (experiment_id, dataset_id, role)
);

-- Audit log table
CREATE TABLE audit_log (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    table_name VARCHAR(255) NOT NULL,
    record_id UUID NOT NULL,
    action VARCHAR(50) NOT NULL,  -- 'INSERT', 'UPDATE', 'DELETE'
    old_values JSONB,
    new_values JSONB,
    changed_by VARCHAR(255) NOT NULL,
    changed_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_audit_log_table_record ON audit_log(table_name, record_id);
CREATE INDEX idx_audit_log_changed_at ON audit_log(changed_at DESC);

-- Updated_at trigger function
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

-- Apply updated_at trigger to experiments
CREATE TRIGGER update_experiments_updated_at
    BEFORE UPDATE ON experiments
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();
```

**Migration Execution Strategy**:

```yaml
migration_execution:
  tool: "sqlx-cli"

  pre_deployment:
    - action: "Backup database"
      command: "pg_dump -h $POSTGRES_HOST -U $POSTGRES_USER $POSTGRES_DB > backup-$(date +%Y%m%d-%H%M%S).sql"
      retention: "30 days"

    - action: "Validate migrations"
      command: "sqlx migrate info --database-url $DATABASE_URL"
      validation: "Ensure no pending failed migrations"

    - action: "Run migrations (dry-run)"
      command: "sqlx migrate run --database-url $DATABASE_URL --dry-run"
      validation: "Review SQL statements to be executed"

  deployment:
    - action: "Run migrations"
      command: "sqlx migrate run --database-url $DATABASE_URL"
      timeout: "10 minutes"
      validation: "All migrations applied successfully"

    - action: "Verify schema"
      command: "sqlx database check --database-url $DATABASE_URL"
      validation: "Schema matches migration history"

  post_deployment:
    - action: "Verify data integrity"
      queries:
        - "SELECT COUNT(*) FROM experiments;"
        - "SELECT COUNT(*) FROM datasets;"
      validation: "Record counts match expectations"

    - action: "Test application connectivity"
      command: "curl -f http://llm-research-lab-service:8080/health/db"
      validation: "Database health check passes"

  rollback_strategy:
    approach: "Backward-compatible migrations only"
    process:
      - "Write additive migrations (add columns with defaults, create tables)"
      - "Avoid breaking changes (drop column/table in separate release)"
      - "Multi-phase migrations for schema changes"

    rollback_procedure:
      - "Restore from backup if migration fails"
      - "Revert to previous application version"
      - "Document rollback in incident report"
```

**Migration Automation in CI/CD**:

```yaml
# .github/workflows/deploy-production.yaml (excerpt)
jobs:
  deploy:
    steps:
      - name: Backup database
        run: |
          pg_dump -h ${{ secrets.POSTGRES_HOST }} \
                  -U ${{ secrets.POSTGRES_USER }} \
                  ${{ secrets.POSTGRES_DB }} > backup.sql
          aws s3 cp backup.sql s3://llm-research-lab-backups/$(date +%Y%m%d-%H%M%S).sql

      - name: Run database migrations
        run: |
          sqlx migrate run --database-url ${{ secrets.DATABASE_URL }}
        timeout-minutes: 10

      - name: Deploy application
        run: |
          helm upgrade --install llm-research-lab helm/llm-research-lab \
            -f helm/values.production.yaml \
            --namespace llm-research-lab \
            --wait --timeout 10m

      - name: Verify deployment
        run: |
          kubectl rollout status deployment/llm-research-lab -n llm-research-lab
          curl -f https://api.llm-research-lab.example.com/health
```

**Migration Best Practices**:

| Practice | Rationale | Implementation |
|----------|-----------|----------------|
| Backward Compatibility | Enable zero-downtime deployments | Add columns with defaults; avoid dropping columns |
| Idempotency | Safe to re-run migrations | Use `IF NOT EXISTS`, `CREATE OR REPLACE` |
| Transactional | Atomic migration application | Wrap in `BEGIN; ... COMMIT;` |
| Versioned | Track schema evolution | Sequential numbered migrations |
| Tested | Prevent production failures | Run migrations in staging first |
| Documented | Knowledge transfer | Comment migrations with purpose and impact |

---

## Summary

This section covered the critical deployment readiness requirements for LLM-Research-Lab:

**3.1 Pre-Deployment Checklist**: Code freeze verification, quality gates (85% coverage, 70% mutation score, zero critical vulns), documentation completeness, release notes, and stakeholder approvals.

**3.2 Environment Certification**: Production environment validation (Kubernetes, PostgreSQL, ClickHouse, Redis, S3), Infrastructure-as-Code verification (Terraform, Helm, Kustomize), secrets management audit (External Secrets Operator, rotation policies), and network security validation (network policies, TLS 1.3, mTLS).

**3.3 Deployment Artifacts**: Multi-stage Dockerfile (optimized to ~150MB), Helm chart with production values (HPA, PodDisruptionBudget, NetworkPolicy), ConfigMaps/Secrets structure (External Secrets Operator integration), and database migration strategy (sqlx with backward-compatible migrations).

**Key Deliverables**:
- Production-ready container image (<200MB, security-hardened)
- Helm chart with comprehensive production configuration
- Automated database migration pipeline
- Validated production infrastructure
- Stakeholder sign-off on deployment readiness

**Next Steps**: Proceed to Section 4 (Operational Handoff) for knowledge transfer and runbook documentation.

---

**Document Metadata**:

| Field | Value |
|-------|-------|
| **Version** | 1.0.0 |
| **Status** | Specification |
| **SPARC Phase** | Phase 5: Completion |
| **Created** | 2025-11-28 |
| **Ecosystem** | LLM DevOps (24+ Module Platform) |
| **Authors** | Platform Engineering Team |

---

*This specification follows the SPARC methodology (Specification → Pseudocode → Architecture → Refinement → Completion). Phase 5 Section 3 covers deployment readiness for enterprise Kubernetes environments.*

**Ecosystem**: LLM DevOps (24+ Module Platform)

---

## 3.4 Deployment Strategies

### 3.4.1 Blue-Green Deployment Procedure

Blue-green deployment enables zero-downtime releases by maintaining two identical production environments.

#### Kubernetes Blue-Green Manifests

**Service Definition (Stable)**
```yaml
# k8s/service-stable.yaml
apiVersion: v1
kind: Service
metadata:
  name: research-lab-api
  namespace: llm-research-lab
  labels:
    app: research-lab
    tier: api
spec:
  selector:
    app: research-lab
    tier: api
    version: stable  # Points to active environment
  ports:
    - name: http
      port: 8080
      targetPort: 8080
      protocol: TCP
    - name: metrics
      port: 9090
      targetPort: 9090
      protocol: TCP
  type: ClusterIP
```

**Blue Deployment**
```yaml
# k8s/deployment-blue.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: research-lab-api-blue
  namespace: llm-research-lab
  labels:
    app: research-lab
    tier: api
    environment: blue
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: research-lab
      tier: api
      environment: blue
  template:
    metadata:
      labels:
        app: research-lab
        tier: api
        environment: blue
        version: stable  # Will be swapped to 'inactive' during deployment
    spec:
      containers:
      - name: api
        image: harbor.example.com/llm-research-lab/api:v1.2.3
        ports:
        - containerPort: 8080
          name: http
        - containerPort: 9090
          name: metrics
        env:
        - name: RUST_LOG
          value: "info,llm_research_lab=debug"
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: postgres-credentials
              key: url
        - name: KAFKA_BROKERS
          value: "kafka-0.kafka-headless:9092,kafka-1.kafka-headless:9092"
        - name: REDIS_URL
          value: "redis://redis-master:6379"
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health/live
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /health/ready
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 2
        startupProbe:
          httpGet:
            path: /health/startup
            port: 8080
          initialDelaySeconds: 0
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 30
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - research-lab
              topologyKey: kubernetes.io/hostname
```

**Green Deployment** (identical to Blue, with `environment: green` label)

#### Blue-Green Deployment Script

```bash
#!/bin/bash
# scripts/deploy-blue-green.sh

set -euo pipefail

NAMESPACE="llm-research-lab"
NEW_VERSION="${1:?Version required (e.g., v1.2.3)}"
CURRENT_ENV=$(kubectl get service research-lab-api -n $NAMESPACE -o jsonpath='{.spec.selector.version}')

# Determine which environment is currently active
if [[ "$CURRENT_ENV" == "blue" ]]; then
    ACTIVE="blue"
    INACTIVE="green"
else
    ACTIVE="green"
    INACTIVE="blue"
fi

echo "[1/7] Current active environment: $ACTIVE"
echo "[1/7] Deploying to inactive environment: $INACTIVE"

# Update inactive deployment with new version
echo "[2/7] Updating $INACTIVE deployment to $NEW_VERSION"
kubectl set image deployment/research-lab-api-$INACTIVE \
    api=harbor.example.com/llm-research-lab/api:$NEW_VERSION \
    -n $NAMESPACE

# Wait for rollout to complete
echo "[3/7] Waiting for $INACTIVE deployment rollout..."
kubectl rollout status deployment/research-lab-api-$INACTIVE -n $NAMESPACE --timeout=5m

# Run smoke tests against inactive environment
echo "[4/7] Running smoke tests against $INACTIVE environment..."
INACTIVE_POD=$(kubectl get pods -n $NAMESPACE -l environment=$INACTIVE -o jsonpath='{.items[0].metadata.name}')
INACTIVE_IP=$(kubectl get pod $INACTIVE_POD -n $NAMESPACE -o jsonpath='{.status.podIP}')

if ! ./scripts/smoke-tests.sh "http://$INACTIVE_IP:8080"; then
    echo "ERROR: Smoke tests failed on $INACTIVE environment"
    echo "Rolling back $INACTIVE deployment..."
    kubectl rollout undo deployment/research-lab-api-$INACTIVE -n $NAMESPACE
    exit 1
fi

# Switch traffic to inactive environment
echo "[5/7] Switching traffic from $ACTIVE to $INACTIVE..."
kubectl patch service research-lab-api -n $NAMESPACE -p "{\"spec\":{\"selector\":{\"environment\":\"$INACTIVE\"}}}"

# Wait for traffic to stabilize (30 seconds)
echo "[6/7] Waiting for traffic to stabilize..."
sleep 30

# Monitor error rates
echo "[7/7] Monitoring error rates..."
ERROR_RATE=$(kubectl exec -n monitoring prometheus-0 -- \
    wget -q -O- 'http://localhost:9090/api/v1/query?query=sum(rate(http_requests_total{status=~"5..",namespace="'$NAMESPACE'"}[5m]))' \
    | jq -r '.data.result[0].value[1] // "0"')

if (( $(echo "$ERROR_RATE > 0.05" | bc -l) )); then
    echo "ERROR: Error rate ($ERROR_RATE) exceeds 5% threshold"
    echo "Rolling back to $ACTIVE environment..."
    kubectl patch service research-lab-api -n $NAMESPACE -p "{\"spec\":{\"selector\":{\"environment\":\"$ACTIVE\"}}}"
    exit 1
fi

echo "SUCCESS: Blue-green deployment to $INACTIVE completed successfully"
echo "Active environment is now: $INACTIVE"
echo "Previous environment ($ACTIVE) is available for rollback"
```

### 3.4.2 Canary Deployment Stages

Canary deployments gradually shift traffic to the new version: 5% → 25% → 50% → 100%.

#### Canary Kubernetes Manifests

```yaml
# k8s/deployment-canary.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: research-lab-api-canary
  namespace: llm-research-lab
  labels:
    app: research-lab
    tier: api
    track: canary
spec:
  replicas: 1  # Will be scaled up during canary rollout
  selector:
    matchLabels:
      app: research-lab
      tier: api
      track: canary
  template:
    metadata:
      labels:
        app: research-lab
        tier: api
        track: canary
        version: canary
    spec:
      containers:
      - name: api
        image: harbor.example.com/llm-research-lab/api:v1.3.0-canary
        # ... (same configuration as blue/green deployments)

---
apiVersion: v1
kind: Service
metadata:
  name: research-lab-api-canary
  namespace: llm-research-lab
spec:
  selector:
    app: research-lab
    tier: api
    track: canary
  ports:
    - name: http
      port: 8080
      targetPort: 8080
```

#### Canary Ingress with Traffic Splitting

```yaml
# k8s/ingress-canary.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: research-lab-api-canary
  namespace: llm-research-lab
  annotations:
    nginx.ingress.kubernetes.io/canary: "true"
    nginx.ingress.kubernetes.io/canary-weight: "5"  # Start at 5%
    nginx.ingress.kubernetes.io/canary-by-header: "X-Canary"
    nginx.ingress.kubernetes.io/canary-by-header-value: "always"
spec:
  ingressClassName: nginx
  rules:
  - host: api.research-lab.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: research-lab-api-canary
            port:
              number: 8080
  tls:
  - hosts:
    - api.research-lab.example.com
    secretName: api-tls-cert
```

#### Canary Deployment Script

```bash
#!/bin/bash
# scripts/deploy-canary.sh

set -euo pipefail

NAMESPACE="llm-research-lab"
NEW_VERSION="${1:?Version required}"
STAGES=(5 25 50 100)
SOAK_TIME=300  # 5 minutes per stage

echo "Starting canary deployment for version $NEW_VERSION"

# Deploy canary with 1 replica
echo "[1/5] Deploying canary version..."
kubectl set image deployment/research-lab-api-canary \
    api=harbor.example.com/llm-research-lab/api:$NEW_VERSION \
    -n $NAMESPACE

kubectl rollout status deployment/research-lab-api-canary -n $NAMESPACE --timeout=5m

# Iterate through canary stages
for STAGE in "${STAGES[@]}"; do
    echo "[Stage $STAGE%] Setting canary traffic weight to $STAGE%"

    kubectl annotate ingress research-lab-api-canary \
        nginx.ingress.kubernetes.io/canary-weight="$STAGE" \
        -n $NAMESPACE --overwrite

    if [[ $STAGE -eq 100 ]]; then
        echo "[Stage $STAGE%] Canary is now receiving all traffic"
        break
    fi

    echo "[Stage $STAGE%] Soaking for $SOAK_TIME seconds..."
    sleep $SOAK_TIME

    # Check metrics during soak period
    echo "[Stage $STAGE%] Validating canary metrics..."

    ERROR_RATE=$(curl -s 'http://prometheus:9090/api/v1/query?query=sum(rate(http_requests_total{status=~"5..",track="canary",namespace="'$NAMESPACE'"}[5m]))' \
        | jq -r '.data.result[0].value[1] // "0"')

    LATENCY_P99=$(curl -s 'http://prometheus:9090/api/v1/query?query=histogram_quantile(0.99,rate(http_request_duration_seconds_bucket{track="canary",namespace="'$NAMESPACE'"}[5m]))' \
        | jq -r '.data.result[0].value[1] // "0"')

    BASELINE_LATENCY=$(curl -s 'http://prometheus:9090/api/v1/query?query=histogram_quantile(0.99,rate(http_request_duration_seconds_bucket{track="stable",namespace="'$NAMESPACE'"}[5m]))' \
        | jq -r '.data.result[0].value[1] // "0"')

    # Rollback triggers
    if (( $(echo "$ERROR_RATE > 0.05" | bc -l) )); then
        echo "ROLLBACK TRIGGERED: Error rate ($ERROR_RATE) exceeds 5% threshold"
        ./scripts/rollback-canary.sh
        exit 1
    fi

    if (( $(echo "$LATENCY_P99 > $BASELINE_LATENCY * 2" | bc -l) )); then
        echo "ROLLBACK TRIGGERED: p99 latency ($LATENCY_P99s) exceeds 2x baseline ($BASELINE_LATENCY s)"
        ./scripts/rollback-canary.sh
        exit 1
    fi

    echo "[Stage $STAGE%] Metrics validated successfully"
done

# Promote canary to stable
echo "[5/5] Promoting canary to stable..."
kubectl set image deployment/research-lab-api-stable \
    api=harbor.example.com/llm-research-lab/api:$NEW_VERSION \
    -n $NAMESPACE

kubectl rollout status deployment/research-lab-api-stable -n $NAMESPACE --timeout=5m

# Remove canary
kubectl annotate ingress research-lab-api-canary nginx.ingress.kubernetes.io/canary-weight- -n $NAMESPACE
kubectl scale deployment/research-lab-api-canary --replicas=0 -n $NAMESPACE

echo "SUCCESS: Canary deployment completed and promoted to stable"
```

### 3.4.3 Rollback Automation Triggers

#### Automated Rollback Script

```bash
#!/bin/bash
# scripts/rollback-canary.sh

set -euo pipefail

NAMESPACE="llm-research-lab"

echo "ROLLBACK INITIATED: Reverting canary deployment"

# Set canary weight to 0%
kubectl annotate ingress research-lab-api-canary \
    nginx.ingress.kubernetes.io/canary-weight="0" \
    -n $NAMESPACE --overwrite

# Scale down canary
kubectl scale deployment/research-lab-api-canary --replicas=0 -n $NAMESPACE

# Trigger alert
curl -X POST https://pagerduty.example.com/api/v1/incidents \
    -H "Authorization: Token token=YOUR_TOKEN" \
    -H "Content-Type: application/json" \
    -d '{
      "incident": {
        "type": "incident",
        "title": "Canary deployment auto-rollback triggered",
        "service": {"id": "PXXXXXX", "type": "service_reference"},
        "urgency": "high",
        "body": {
          "type": "incident_body",
          "details": "Automated rollback triggered due to threshold violation"
        }
      }
    }'

echo "Rollback completed. Canary traffic set to 0%, replicas scaled to 0"
```

#### Prometheus AlertManager Rules

```yaml
# k8s/prometheus-rules.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-alerts
  namespace: monitoring
data:
  canary-alerts.yaml: |
    groups:
    - name: canary_deployment
      interval: 30s
      rules:
      - alert: CanaryHighErrorRate
        expr: |
          sum(rate(http_requests_total{status=~"5..",track="canary"}[5m]))
          /
          sum(rate(http_requests_total{track="canary"}[5m]))
          > 0.05
        for: 2m
        labels:
          severity: critical
          component: deployment
        annotations:
          summary: "Canary deployment error rate exceeds 5%"
          description: "Error rate: {{ $value | humanizePercentage }}"
          runbook_url: "https://runbooks.example.com/canary-high-error-rate"

      - alert: CanaryHighLatency
        expr: |
          histogram_quantile(0.99,
            rate(http_request_duration_seconds_bucket{track="canary"}[5m])
          ) > 2 *
          histogram_quantile(0.99,
            rate(http_request_duration_seconds_bucket{track="stable"}[5m])
          )
        for: 2m
        labels:
          severity: critical
          component: deployment
        annotations:
          summary: "Canary p99 latency exceeds 2x baseline"
          description: "Canary p99: {{ $value }}s"
          runbook_url: "https://runbooks.example.com/canary-high-latency"

      - alert: CanaryMemoryPressure
        expr: |
          container_memory_usage_bytes{pod=~".*-canary-.*"}
          /
          container_spec_memory_limit_bytes{pod=~".*-canary-.*"}
          > 0.9
        for: 5m
        labels:
          severity: warning
          component: deployment
        annotations:
          summary: "Canary pod memory usage > 90%"
          description: "Memory usage: {{ $value | humanizePercentage }}"
```

### 3.4.4 Zero-Downtime Deployment Verification

#### Pre-Deployment Checklist Script

```bash
#!/bin/bash
# scripts/pre-deployment-check.sh

set -euo pipefail

NAMESPACE="llm-research-lab"
FAILURES=0

echo "Running pre-deployment verification checks..."

# Check database connectivity
echo "[1/6] Verifying database connectivity..."
if ! kubectl exec -n $NAMESPACE deployment/research-lab-api-stable -- \
    sh -c 'timeout 5 pg_isready -h $DATABASE_HOST -p 5432 -U $DATABASE_USER'; then
    echo "FAIL: Database not reachable"
    ((FAILURES++))
else
    echo "PASS: Database connectivity verified"
fi

# Check Kafka availability
echo "[2/6] Verifying Kafka brokers..."
KAFKA_PODS=$(kubectl get pods -n $NAMESPACE -l app=kafka -o jsonpath='{.items[*].status.phase}')
if [[ "$KAFKA_PODS" == *"Running"* ]]; then
    echo "PASS: Kafka brokers running"
else
    echo "FAIL: Kafka brokers not ready"
    ((FAILURES++))
fi

# Check Redis availability
echo "[3/6] Verifying Redis..."
if ! kubectl exec -n $NAMESPACE redis-master-0 -- redis-cli ping | grep -q PONG; then
    echo "FAIL: Redis not responding"
    ((FAILURES++))
else
    echo "PASS: Redis connectivity verified"
fi

# Check current deployment health
echo "[4/6] Verifying current deployment health..."
READY_REPLICAS=$(kubectl get deployment research-lab-api-stable -n $NAMESPACE -o jsonpath='{.status.readyReplicas}')
DESIRED_REPLICAS=$(kubectl get deployment research-lab-api-stable -n $NAMESPACE -o jsonpath='{.spec.replicas}')

if [[ "$READY_REPLICAS" -ne "$DESIRED_REPLICAS" ]]; then
    echo "FAIL: Current deployment not healthy ($READY_REPLICAS/$DESIRED_REPLICAS ready)"
    ((FAILURES++))
else
    echo "PASS: Current deployment healthy ($READY_REPLICAS/$DESIRED_REPLICAS ready)"
fi

# Check error rate in last 10 minutes
echo "[5/6] Checking baseline error rate..."
ERROR_RATE=$(curl -s 'http://prometheus:9090/api/v1/query?query=sum(rate(http_requests_total{status=~"5..",namespace="'$NAMESPACE'"}[10m]))' \
    | jq -r '.data.result[0].value[1] // "0"')

if (( $(echo "$ERROR_RATE > 0.01" | bc -l) )); then
    echo "WARN: Elevated error rate detected ($ERROR_RATE). Proceed with caution."
else
    echo "PASS: Error rate within normal range"
fi

# Check available cluster resources
echo "[6/6] Verifying cluster resources..."
NODE_CPU=$(kubectl top nodes --no-headers | awk '{print $3}' | sed 's/%//' | sort -n | tail -1)
if (( NODE_CPU > 80 )); then
    echo "WARN: Node CPU usage high ($NODE_CPU%)"
else
    echo "PASS: Cluster resources available"
fi

# Final verdict
if [[ $FAILURES -gt 0 ]]; then
    echo "FAILED: Pre-deployment checks failed ($FAILURES failures)"
    exit 1
else
    echo "SUCCESS: All pre-deployment checks passed"
    exit 0
fi
```

---

## 3.5 Smoke Tests

### 3.5.1 Critical Path Validation Tests

Smoke tests validate core functionality after deployment.

#### Rust Smoke Test Framework

```rust
// tests/smoke_tests.rs
use anyhow::{Context, Result};
use reqwest::{Client, StatusCode};
use serde_json::json;
use std::time::Duration;

#[derive(Debug)]
pub struct SmokeTestConfig {
    pub base_url: String,
    pub timeout: Duration,
}

pub struct SmokeTestRunner {
    client: Client,
    config: SmokeTestConfig,
}

impl SmokeTestRunner {
    pub fn new(config: SmokeTestConfig) -> Result<Self> {
        let client = Client::builder()
            .timeout(config.timeout)
            .build()
            .context("Failed to create HTTP client")?;

        Ok(Self { client, config })
    }

    /// Test 1: Health check endpoints
    pub async fn test_health_endpoints(&self) -> Result<()> {
        println!("Testing health endpoints...");

        // Liveness probe
        let response = self.client
            .get(&format!("{}/health/live", self.config.base_url))
            .send()
            .await
            .context("Liveness probe request failed")?;

        assert_eq!(
            response.status(),
            StatusCode::OK,
            "Liveness probe failed"
        );

        // Readiness probe
        let response = self.client
            .get(&format!("{}/health/ready", self.config.base_url))
            .send()
            .await
            .context("Readiness probe request failed")?;

        assert_eq!(
            response.status(),
            StatusCode::OK,
            "Readiness probe failed"
        );

        // Startup probe
        let response = self.client
            .get(&format!("{}/health/startup", self.config.base_url))
            .send()
            .await
            .context("Startup probe request failed")?;

        assert_eq!(
            response.status(),
            StatusCode::OK,
            "Startup probe failed"
        );

        println!("✓ Health endpoints validated");
        Ok(())
    }

    /// Test 2: Experiment creation workflow
    pub async fn test_experiment_creation(&self) -> Result<()> {
        println!("Testing experiment creation...");

        let experiment = json!({
            "name": "smoke-test-experiment",
            "description": "Automated smoke test",
            "model_id": "test-model-v1",
            "dataset_id": "test-dataset-v1",
            "hyperparameters": {
                "learning_rate": 0.001,
                "batch_size": 32
            }
        });

        let response = self.client
            .post(&format!("{}/api/v1/experiments", self.config.base_url))
            .json(&experiment)
            .send()
            .await
            .context("Experiment creation request failed")?;

        assert_eq!(
            response.status(),
            StatusCode::CREATED,
            "Experiment creation failed: {}",
            response.text().await?
        );

        let created_experiment: serde_json::Value = response
            .json()
            .await
            .context("Failed to parse experiment response")?;

        let experiment_id = created_experiment["id"]
            .as_str()
            .context("No experiment ID in response")?;

        println!("✓ Experiment created: {}", experiment_id);
        Ok(())
    }

    /// Test 3: Metrics ingestion
    pub async fn test_metrics_ingestion(&self) -> Result<()> {
        println!("Testing metrics ingestion...");

        let metric = json!({
            "experiment_id": "smoke-test-exp-001",
            "step": 100,
            "metrics": {
                "loss": 0.234,
                "accuracy": 0.876,
                "f1_score": 0.845
            },
            "timestamp": chrono::Utc::now().to_rfc3339()
        });

        let response = self.client
            .post(&format!("{}/api/v1/metrics", self.config.base_url))
            .json(&metric)
            .send()
            .await
            .context("Metrics ingestion request failed")?;

        assert_eq!(
            response.status(),
            StatusCode::ACCEPTED,
            "Metrics ingestion failed"
        );

        println!("✓ Metrics ingested successfully");
        Ok(())
    }

    /// Test 4: Dataset versioning
    pub async fn test_dataset_versioning(&self) -> Result<()> {
        println!("Testing dataset versioning...");

        let dataset = json!({
            "name": "smoke-test-dataset",
            "version": "1.0.0",
            "format": "parquet",
            "size_bytes": 1024000,
            "checksum": "sha256:abc123..."
        });

        let response = self.client
            .post(&format!("{}/api/v1/datasets", self.config.base_url))
            .json(&dataset)
            .send()
            .await
            .context("Dataset creation request failed")?;

        assert!(
            response.status().is_success(),
            "Dataset versioning failed"
        );

        println!("✓ Dataset versioning validated");
        Ok(())
    }

    /// Test 5: Integration endpoints
    pub async fn test_integration_endpoints(&self) -> Result<()> {
        println!("Testing integration endpoints...");

        // Test LLM-Registry integration
        let response = self.client
            .get(&format!("{}/api/v1/integrations/registry/health", self.config.base_url))
            .send()
            .await
            .context("Registry integration health check failed")?;

        assert!(
            response.status().is_success(),
            "LLM-Registry integration unhealthy"
        );

        // Test LLM-Test-Bench integration
        let response = self.client
            .get(&format!("{}/api/v1/integrations/test-bench/health", self.config.base_url))
            .send()
            .await
            .context("Test Bench integration health check failed")?;

        assert!(
            response.status().is_success(),
            "LLM-Test-Bench integration unhealthy"
        );

        println!("✓ Integration endpoints validated");
        Ok(())
    }

    /// Run all smoke tests
    pub async fn run_all(&self) -> Result<()> {
        println!("\n=== Running Smoke Tests ===\n");

        self.test_health_endpoints().await?;
        self.test_experiment_creation().await?;
        self.test_metrics_ingestion().await?;
        self.test_dataset_versioning().await?;
        self.test_integration_endpoints().await?;

        println!("\n=== All Smoke Tests Passed ===\n");
        Ok(())
    }
}

#[tokio::main]
async fn main() -> Result<()> {
    let config = SmokeTestConfig {
        base_url: std::env::var("API_BASE_URL")
            .unwrap_or_else(|_| "http://localhost:8080".to_string()),
        timeout: Duration::from_secs(10),
    };

    let runner = SmokeTestRunner::new(config)?;
    runner.run_all().await?;

    Ok(())
}
```

### 3.5.2 Health Check Endpoint Specification

#### Rust Health Check Implementation

```rust
// src/health.rs
use axum::{
    extract::State,
    http::StatusCode,
    response::IntoResponse,
    routing::get,
    Json, Router,
};
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use tokio_postgres::Client as PgClient;
use redis::Client as RedisClient;

#[derive(Debug, Serialize, Deserialize)]
pub struct HealthStatus {
    pub status: String,
    pub version: String,
    pub timestamp: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub dependencies: Option<DependencyHealth>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct DependencyHealth {
    pub database: ServiceHealth,
    pub redis: ServiceHealth,
    pub kafka: ServiceHealth,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ServiceHealth {
    pub status: String,
    pub latency_ms: Option<u64>,
}

pub struct HealthState {
    pub pg_client: Arc<PgClient>,
    pub redis_client: Arc<RedisClient>,
}

/// Liveness probe - checks if the application is running
/// Returns 200 if the service is alive, regardless of dependencies
pub async fn liveness_handler() -> impl IntoResponse {
    let health = HealthStatus {
        status: "alive".to_string(),
        version: env!("CARGO_PKG_VERSION").to_string(),
        timestamp: chrono::Utc::now().to_rfc3339(),
        dependencies: None,
    };

    (StatusCode::OK, Json(health))
}

/// Readiness probe - checks if the application is ready to serve traffic
/// Returns 200 only if all critical dependencies are healthy
pub async fn readiness_handler(
    State(state): State<Arc<HealthState>>,
) -> impl IntoResponse {
    let start = std::time::Instant::now();

    // Check database
    let db_health = check_database(&state.pg_client).await;

    // Check Redis
    let redis_health = check_redis(&state.redis_client).await;

    // Check Kafka (simplified - actual implementation would check broker connectivity)
    let kafka_health = ServiceHealth {
        status: "healthy".to_string(),
        latency_ms: Some(5),
    };

    let is_ready = db_health.status == "healthy"
        && redis_health.status == "healthy"
        && kafka_health.status == "healthy";

    let status_code = if is_ready {
        StatusCode::OK
    } else {
        StatusCode::SERVICE_UNAVAILABLE
    };

    let health = HealthStatus {
        status: if is_ready { "ready" } else { "not_ready" }.to_string(),
        version: env!("CARGO_PKG_VERSION").to_string(),
        timestamp: chrono::Utc::now().to_rfc3339(),
        dependencies: Some(DependencyHealth {
            database: db_health,
            redis: redis_health,
            kafka: kafka_health,
        }),
    };

    (status_code, Json(health))
}

/// Startup probe - checks if the application has completed initialization
/// Returns 200 when the service is fully initialized
pub async fn startup_handler(
    State(state): State<Arc<HealthState>>,
) -> impl IntoResponse {
    // Check critical initialization dependencies
    let db_initialized = check_database(&state.pg_client).await.status == "healthy";
    let redis_initialized = check_redis(&state.redis_client).await.status == "healthy";

    if db_initialized && redis_initialized {
        (StatusCode::OK, Json(json!({
            "status": "started",
            "version": env!("CARGO_PKG_VERSION"),
            "timestamp": chrono::Utc::now().to_rfc3339()
        })))
    } else {
        (StatusCode::SERVICE_UNAVAILABLE, Json(json!({
            "status": "starting",
            "message": "Still initializing dependencies"
        })))
    }
}

async fn check_database(client: &PgClient) -> ServiceHealth {
    let start = std::time::Instant::now();

    match tokio::time::timeout(
        std::time::Duration::from_secs(2),
        client.simple_query("SELECT 1")
    ).await {
        Ok(Ok(_)) => ServiceHealth {
            status: "healthy".to_string(),
            latency_ms: Some(start.elapsed().as_millis() as u64),
        },
        _ => ServiceHealth {
            status: "unhealthy".to_string(),
            latency_ms: None,
        },
    }
}

async fn check_redis(client: &RedisClient) -> ServiceHealth {
    let start = std::time::Instant::now();

    match client.get_tokio_connection().await {
        Ok(mut conn) => {
            match redis::cmd("PING").query_async::<_, String>(&mut conn).await {
                Ok(_) => ServiceHealth {
                    status: "healthy".to_string(),
                    latency_ms: Some(start.elapsed().as_millis() as u64),
                },
                Err(_) => ServiceHealth {
                    status: "unhealthy".to_string(),
                    latency_ms: None,
                },
            }
        }
        Err(_) => ServiceHealth {
            status: "unhealthy".to_string(),
            latency_ms: None,
        },
    }
}

pub fn health_routes(state: Arc<HealthState>) -> Router {
    Router::new()
        .route("/health/live", get(liveness_handler))
        .route("/health/ready", get(readiness_handler))
        .route("/health/startup", get(startup_handler))
        .with_state(state)
}
```

### 3.5.3 Dependency Connectivity Tests

#### Shell Script for Dependency Validation

```bash
#!/bin/bash
# scripts/test-dependencies.sh

set -euo pipefail

BASE_URL="${1:-http://localhost:8080}"
FAILURES=0

echo "Testing dependency connectivity for $BASE_URL"

# Test PostgreSQL connectivity
echo "[1/5] Testing PostgreSQL..."
if kubectl exec -n llm-research-lab deployment/research-lab-api-stable -- \
    sh -c 'pg_isready -h $DATABASE_HOST -p 5432 -U $DATABASE_USER' &>/dev/null; then
    echo "✓ PostgreSQL: Connected"
else
    echo "✗ PostgreSQL: Failed"
    ((FAILURES++))
fi

# Test Redis connectivity
echo "[2/5] Testing Redis..."
if kubectl exec -n llm-research-lab redis-master-0 -- redis-cli ping | grep -q PONG; then
    echo "✓ Redis: Connected"
else
    echo "✗ Redis: Failed"
    ((FAILURES++))
fi

# Test Kafka connectivity
echo "[3/5] Testing Kafka..."
if kubectl exec -n llm-research-lab kafka-0 -- \
    kafka-broker-api-versions --bootstrap-server localhost:9092 &>/dev/null; then
    echo "✓ Kafka: Connected"
else
    echo "✗ Kafka: Failed"
    ((FAILURES++))
fi

# Test S3/MinIO connectivity
echo "[4/5] Testing Object Storage..."
HEALTH_RESPONSE=$(curl -s "$BASE_URL/api/v1/health/dependencies" | jq -r '.dependencies.object_storage.status')
if [[ "$HEALTH_RESPONSE" == "healthy" ]]; then
    echo "✓ Object Storage: Connected"
else
    echo "✗ Object Storage: Failed"
    ((FAILURES++))
fi

# Test ClickHouse connectivity
echo "[5/5] Testing ClickHouse..."
if kubectl exec -n llm-research-lab clickhouse-0 -- clickhouse-client --query "SELECT 1" &>/dev/null; then
    echo "✓ ClickHouse: Connected"
else
    echo "✗ ClickHouse: Failed"
    ((FAILURES++))
fi

if [[ $FAILURES -gt 0 ]]; then
    echo "\nDependency tests failed: $FAILURES failures"
    exit 1
else
    echo "\n✓ All dependency connectivity tests passed"
    exit 0
fi
```

### 3.5.4 Performance Baseline Verification

#### Rust Performance Baseline Test

```rust
// tests/performance_baseline.rs
use criterion::{black_box, criterion_group, criterion_main, Criterion, BenchmarkId};
use reqwest::Client;
use tokio::runtime::Runtime;

const BASE_URL: &str = "http://localhost:8080";

async fn benchmark_experiment_creation(client: &Client) -> reqwest::Result<()> {
    let experiment = serde_json::json!({
        "name": "perf-test-experiment",
        "model_id": "test-model",
        "dataset_id": "test-dataset"
    });

    client
        .post(&format!("{}/api/v1/experiments", BASE_URL))
        .json(&experiment)
        .send()
        .await?
        .error_for_status()?;

    Ok(())
}

async fn benchmark_metrics_query(client: &Client) -> reqwest::Result<()> {
    client
        .get(&format!("{}/api/v1/metrics?experiment_id=test-exp-001&limit=100", BASE_URL))
        .send()
        .await?
        .error_for_status()?;

    Ok(())
}

fn performance_benchmarks(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let client = Client::new();

    // Benchmark experiment creation (baseline: < 100ms)
    c.bench_function("experiment_creation", |b| {
        b.to_async(&rt).iter(|| async {
            benchmark_experiment_creation(&client).await.unwrap()
        });
    });

    // Benchmark metrics query (baseline: < 50ms)
    c.bench_function("metrics_query", |b| {
        b.to_async(&rt).iter(|| async {
            benchmark_metrics_query(&client).await.unwrap()
        });
    });
}

criterion_group!(benches, performance_benchmarks);
criterion_main!(benches);
```

### 3.5.5 Automated Smoke Test Script

#### Comprehensive Smoke Test Runner

```bash
#!/bin/bash
# scripts/smoke-tests.sh

set -euo pipefail

BASE_URL="${1:-http://localhost:8080}"
TIMEOUT=10
FAILURES=0

echo "========================================="
echo "  LLM Research Lab - Smoke Tests"
echo "  Target: $BASE_URL"
echo "========================================="

# Test 1: Health endpoints
echo -e "\n[Test 1/6] Health Endpoints"
for endpoint in live ready startup; do
    echo -n "  Testing /health/$endpoint... "
    STATUS=$(curl -s -o /dev/null -w "%{http_code}" --max-time $TIMEOUT "$BASE_URL/health/$endpoint")
    if [[ "$STATUS" == "200" ]]; then
        echo "✓ PASS ($STATUS)"
    else
        echo "✗ FAIL ($STATUS)"
        ((FAILURES++))
    fi
done

# Test 2: API authentication
echo -e "\n[Test 2/6] API Authentication"
echo -n "  Testing JWT authentication... "
TOKEN=$(curl -s -X POST "$BASE_URL/api/v1/auth/login" \
    -H "Content-Type: application/json" \
    -d '{"username":"test","password":"test"}' \
    | jq -r '.token // empty')

if [[ -n "$TOKEN" ]]; then
    echo "✓ PASS"
else
    echo "✗ FAIL (No token received)"
    ((FAILURES++))
fi

# Test 3: Experiment workflow
echo -e "\n[Test 3/6] Experiment Creation"
echo -n "  Creating test experiment... "
EXPERIMENT=$(curl -s -X POST "$BASE_URL/api/v1/experiments" \
    -H "Authorization: Bearer $TOKEN" \
    -H "Content-Type: application/json" \
    -d '{
        "name": "smoke-test-exp",
        "model_id": "test-model-v1",
        "dataset_id": "test-dataset-v1"
    }')

EXPERIMENT_ID=$(echo "$EXPERIMENT" | jq -r '.id // empty')
if [[ -n "$EXPERIMENT_ID" ]]; then
    echo "✓ PASS (ID: $EXPERIMENT_ID)"
else
    echo "✗ FAIL"
    ((FAILURES++))
fi

# Test 4: Metrics ingestion
echo -e "\n[Test 4/6] Metrics Ingestion"
echo -n "  Posting metrics... "
METRIC_RESPONSE=$(curl -s -o /dev/null -w "%{http_code}" -X POST "$BASE_URL/api/v1/metrics" \
    -H "Authorization: Bearer $TOKEN" \
    -H "Content-Type: application/json" \
    -d "{
        \"experiment_id\": \"$EXPERIMENT_ID\",
        \"step\": 1,
        \"metrics\": {\"loss\": 0.5, \"accuracy\": 0.85}
    }")

if [[ "$METRIC_RESPONSE" == "202" ]] || [[ "$METRIC_RESPONSE" == "200" ]]; then
    echo "✓ PASS ($METRIC_RESPONSE)"
else
    echo "✗ FAIL ($METRIC_RESPONSE)"
    ((FAILURES++))
fi

# Test 5: Integration endpoints
echo -e "\n[Test 5/6] External Integrations"
for integration in registry test-bench analytics-hub; do
    echo -n "  Testing LLM-$integration integration... "
    STATUS=$(curl -s -o /dev/null -w "%{http_code}" --max-time $TIMEOUT \
        "$BASE_URL/api/v1/integrations/$integration/health")
    if [[ "$STATUS" == "200" ]]; then
        echo "✓ PASS"
    else
        echo "✗ FAIL ($STATUS)"
        ((FAILURES++))
    fi
done

# Test 6: Performance baselines
echo -e "\n[Test 6/6] Performance Baselines"
echo -n "  Measuring API latency... "
LATENCY=$(curl -s -o /dev/null -w "%{time_total}" "$BASE_URL/health/ready")
LATENCY_MS=$(echo "$LATENCY * 1000" | bc)

if (( $(echo "$LATENCY < 0.1" | bc -l) )); then
    echo "✓ PASS (${LATENCY_MS}ms < 100ms)"
else
    echo "⚠ WARN (${LATENCY_MS}ms >= 100ms)"
fi

# Final results
echo -e "\n========================================="
if [[ $FAILURES -eq 0 ]]; then
    echo "  ✓ ALL SMOKE TESTS PASSED"
    echo "========================================="
    exit 0
else
    echo "  ✗ SMOKE TESTS FAILED: $FAILURES failures"
    echo "========================================="
    exit 1
fi
```

---

## Deployment Workflow Summary

### Complete Deployment Pipeline

```bash
#!/bin/bash
# scripts/deploy-production.sh
# Complete deployment workflow integrating all strategies

set -euo pipefail

VERSION="${1:?Version required}"
STRATEGY="${2:-blue-green}"  # Options: blue-green, canary

echo "Starting production deployment: $VERSION (strategy: $STRATEGY)"

# Step 1: Pre-deployment checks
echo "[1/5] Running pre-deployment checks..."
./scripts/pre-deployment-check.sh || {
    echo "Pre-deployment checks failed. Aborting."
    exit 1
}

# Step 2: Deploy based on strategy
echo "[2/5] Executing $STRATEGY deployment..."
case "$STRATEGY" in
    blue-green)
        ./scripts/deploy-blue-green.sh "$VERSION"
        ;;
    canary)
        ./scripts/deploy-canary.sh "$VERSION"
        ;;
    *)
        echo "Unknown strategy: $STRATEGY"
        exit 1
        ;;
esac

# Step 3: Run smoke tests
echo "[3/5] Running smoke tests..."
./scripts/smoke-tests.sh "https://api.research-lab.example.com" || {
    echo "Smoke tests failed. Initiating rollback..."
    ./scripts/rollback-$STRATEGY.sh
    exit 1
}

# Step 4: Verify performance baselines
echo "[4/5] Verifying performance baselines..."
cargo test --release --test performance_baseline || {
    echo "Performance baseline verification failed."
    exit 1
}

# Step 5: Post-deployment validation
echo "[5/5] Post-deployment validation..."
sleep 60  # Wait for metrics to stabilize
./scripts/validate-deployment.sh || {
    echo "Post-deployment validation failed."
    exit 1
}

echo "✓ Production deployment completed successfully: $VERSION"
```

---

## Document Metadata

| Field | Value |
|-------|-------|
| **Version** | 1.0.0 |
| **Status** | Specification |
| **SPARC Phase** | Phase 5: Completion - Section 3.4-3.5 |
| **Created** | 2025-11-28 |
| **Technology Stack** | Rust, Axum, Kubernetes, Prometheus |
| **Dependencies** | PostgreSQL, Redis, Kafka, ClickHouse |

---

**Next Sections**: 3.6-3.8 (Monitoring Integration, Observability, Incident Response)
---

## Production Infrastructure


## Table of Contents

- [4.1 Kubernetes Configuration](#41-kubernetes-configuration)
- [4.2 Database Production Setup](#42-database-production-setup)
- [4.3 Message Queue Production](#43-message-queue-production)
- [4.4 Caching Layer](#44-caching-layer)
- [4.5 Load Balancing & Ingress](#45-load-balancing--ingress)
- [4.6 Service Mesh (Optional)](#46-service-mesh-optional)

---

## 4.1 Kubernetes Configuration

### 4.1.1 Namespace Organization

Isolate environments and components using a structured namespace hierarchy.

#### Namespace Structure

```yaml
# namespaces/base-namespaces.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: llm-research-lab-prod
  labels:
    environment: production
    team: ai-research
    compliance: soc2
    cost-center: research-ops
---
apiVersion: v1
kind: Namespace
metadata:
  name: llm-research-lab-staging
  labels:
    environment: staging
    team: ai-research
---
apiVersion: v1
kind: Namespace
metadata:
  name: llm-research-lab-data
  labels:
    environment: production
    team: data-platform
    compliance: soc2
    pii: "true"
---
apiVersion: v1
kind: Namespace
metadata:
  name: llm-research-lab-monitoring
  labels:
    environment: production
    team: platform
```

#### Namespace Isolation Network Policies

```yaml
# namespaces/network-policies.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: llm-research-lab-prod
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-same-namespace
  namespace: llm-research-lab-prod
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector: {}
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-to-data-layer
  namespace: llm-research-lab-prod
spec:
  podSelector:
    matchLabels:
      tier: backend
  policyTypes:
  - Egress
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          environment: production
    - podSelector:
        matchLabels:
          tier: database
    ports:
    - protocol: TCP
      port: 5432  # PostgreSQL
    - protocol: TCP
      port: 8123  # ClickHouse HTTP
    - protocol: TCP
      port: 9000  # ClickHouse Native
```

### 4.1.2 Resource Quotas and Limits

Prevent resource exhaustion and ensure fair allocation across tenants.

#### Namespace Resource Quotas

```yaml
# quotas/production-quota.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: llm-research-lab-prod
spec:
  hard:
    requests.cpu: "100"
    requests.memory: 200Gi
    limits.cpu: "200"
    limits.memory: 400Gi
    requests.nvidia.com/gpu: "10"
    persistentvolumeclaims: "50"
    requests.storage: 1Ti
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: object-quota
  namespace: llm-research-lab-prod
spec:
  hard:
    pods: "200"
    services: "50"
    configmaps: "100"
    secrets: "100"
    persistentvolumeclaims: "50"
```

#### LimitRange for Default Container Resources

```yaml
# quotas/limit-range.yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: default-limits
  namespace: llm-research-lab-prod
spec:
  limits:
  # Container limits
  - type: Container
    default:
      cpu: "1"
      memory: 2Gi
    defaultRequest:
      cpu: "500m"
      memory: 1Gi
    max:
      cpu: "16"
      memory: 64Gi
    min:
      cpu: "100m"
      memory: 128Mi
  # Pod limits
  - type: Pod
    max:
      cpu: "32"
      memory: 128Gi
      nvidia.com/gpu: "4"
  # PVC limits
  - type: PersistentVolumeClaim
    max:
      storage: 500Gi
    min:
      storage: 1Gi
```

### 4.1.3 Pod Disruption Budgets

Ensure high availability during voluntary disruptions (node drains, upgrades).

```yaml
# disruption/api-pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: llm-api-pdb
  namespace: llm-research-lab-prod
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: llm-research-api
      tier: backend
---
# disruption/worker-pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: experiment-worker-pdb
  namespace: llm-research-lab-prod
spec:
  maxUnavailable: 25%
  selector:
    matchLabels:
      app: experiment-worker
      tier: worker
---
# disruption/postgres-pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: postgres-pdb
  namespace: llm-research-lab-data
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: postgresql
```

### 4.1.4 Horizontal Pod Autoscaler Configuration

Scale based on CPU, memory, and custom metrics.

#### CPU-Based Autoscaling

```yaml
# autoscaling/api-hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: llm-api-hpa
  namespace: llm-research-lab-prod
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: llm-research-api
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 30
      - type: Pods
        value: 4
        periodSeconds: 30
      selectPolicy: Max
```

#### Custom Metrics Autoscaling (Prometheus Adapter)

```yaml
# autoscaling/custom-metrics-hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: experiment-worker-hpa
  namespace: llm-research-lab-prod
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: experiment-worker
  minReplicas: 2
  maxReplicas: 50
  metrics:
  # Scale based on Kafka consumer lag
  - type: External
    external:
      metric:
        name: kafka_consumergroup_lag
        selector:
          matchLabels:
            topic: experiments
      target:
        type: AverageValue
        averageValue: "100"  # Target 100 messages lag per pod
  # Scale based on experiment queue depth
  - type: Pods
    pods:
      metric:
        name: experiment_queue_depth
      target:
        type: AverageValue
        averageValue: "50"
```

### 4.1.5 Priority Classes

Define scheduling priorities for critical vs batch workloads.

```yaml
# priority/priority-classes.yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: system-critical
value: 1000000000
globalDefault: false
description: "Reserved for system-critical components (monitoring, logging)"
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: production-high
value: 100000
globalDefault: false
description: "High priority for production API services"
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: production-normal
value: 10000
globalDefault: true
description: "Default priority for production workloads"
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: batch-low
value: 1000
globalDefault: false
description: "Low priority for batch jobs and experiments"
preemptionPolicy: PreemptLowerPriority
```

#### Example Deployment with Priority

```yaml
# deployments/api-with-priority.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-research-api
  namespace: llm-research-lab-prod
spec:
  replicas: 3
  selector:
    matchLabels:
      app: llm-research-api
  template:
    metadata:
      labels:
        app: llm-research-api
        tier: backend
        version: v1.0.0
    spec:
      priorityClassName: production-high
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - llm-research-api
            topologyKey: kubernetes.io/hostname
      containers:
      - name: api
        image: llm-research-lab/api:v1.0.0
        resources:
          requests:
            cpu: "2"
            memory: 4Gi
          limits:
            cpu: "4"
            memory: 8Gi
        livenessProbe:
          httpGet:
            path: /health/live
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health/ready
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
```

---

## 4.2 Database Production Setup

### 4.2.1 PostgreSQL High Availability (CloudNativePG)

Deploy PostgreSQL with automatic failover and streaming replication.

#### CloudNativePG Cluster Configuration

```yaml
# databases/postgres-cluster.yaml
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: llm-research-postgres
  namespace: llm-research-lab-data
spec:
  instances: 3
  imageName: ghcr.io/cloudnative-pg/postgresql:15.4

  # Storage configuration
  storage:
    size: 500Gi
    storageClass: fast-ssd

  # Replication configuration
  postgresql:
    parameters:
      max_connections: "500"
      shared_buffers: "4GB"
      effective_cache_size: "12GB"
      maintenance_work_mem: "1GB"
      checkpoint_completion_target: "0.9"
      wal_buffers: "16MB"
      default_statistics_target: "100"
      random_page_cost: "1.1"
      effective_io_concurrency: "200"
      work_mem: "8MB"
      min_wal_size: "1GB"
      max_wal_size: "4GB"
      max_worker_processes: "4"
      max_parallel_workers_per_gather: "2"
      max_parallel_workers: "4"
      max_parallel_maintenance_workers: "2"

  # High availability
  enableSuperuserAccess: true
  primaryUpdateStrategy: unsupervised

  # Backup configuration
  backup:
    barmanObjectStore:
      destinationPath: s3://llm-research-backups/postgres
      s3Credentials:
        accessKeyId:
          name: postgres-backup-s3
          key: ACCESS_KEY_ID
        secretAccessKey:
          name: postgres-backup-s3
          key: ACCESS_SECRET_KEY
      wal:
        compression: gzip
        maxParallel: 4
    retentionPolicy: "30d"

  # Monitoring
  monitoring:
    enablePodMonitor: true

  # Resources
  resources:
    requests:
      memory: "8Gi"
      cpu: "4"
    limits:
      memory: "16Gi"
      cpu: "8"

  # Node affinity
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: cnpg.io/cluster
            operator: In
            values:
            - llm-research-postgres
        topologyKey: kubernetes.io/hostname
```

#### Scheduled Backups

```yaml
# databases/postgres-scheduled-backup.yaml
apiVersion: postgresql.cnpg.io/v1
kind: ScheduledBackup
metadata:
  name: llm-postgres-backup
  namespace: llm-research-lab-data
spec:
  schedule: "0 2 * * *"  # Daily at 2 AM
  backupOwnerReference: self
  cluster:
    name: llm-research-postgres
```

#### Point-in-Time Recovery Configuration

```yaml
# databases/postgres-pitr.yaml
# PITR is enabled automatically with WAL archiving
# To restore to a specific point in time, create a new cluster:

apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: llm-research-postgres-restored
  namespace: llm-research-lab-data
spec:
  instances: 3
  imageName: ghcr.io/cloudnative-pg/postgresql:15.4

  bootstrap:
    recovery:
      source: llm-research-postgres
      recoveryTarget:
        targetTime: "2025-11-28 14:30:00.000000+00"

  externalClusters:
  - name: llm-research-postgres
    barmanObjectStore:
      destinationPath: s3://llm-research-backups/postgres
      s3Credentials:
        accessKeyId:
          name: postgres-backup-s3
          key: ACCESS_KEY_ID
        secretAccessKey:
          name: postgres-backup-s3
          key: ACCESS_SECRET_KEY
```

### 4.2.2 ClickHouse Cluster Configuration

Deploy distributed ClickHouse for analytics and time-series data.

#### ClickHouse Operator Installation

```yaml
# databases/clickhouse-operator.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: clickhouse-operator
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: clickhouse-operator
  namespace: clickhouse-operator
spec:
  replicas: 1
  selector:
    matchLabels:
      app: clickhouse-operator
  template:
    metadata:
      labels:
        app: clickhouse-operator
    spec:
      serviceAccountName: clickhouse-operator
      containers:
      - name: clickhouse-operator
        image: altinity/clickhouse-operator:0.23.0
        imagePullPolicy: IfNotPresent
        env:
        - name: OPERATOR_POD_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: OPERATOR_POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: OPERATOR_POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
```

#### ClickHouse Cluster Deployment

```yaml
# databases/clickhouse-cluster.yaml
apiVersion: clickhouse.altinity.com/v1
kind: ClickHouseInstallation
metadata:
  name: llm-research-clickhouse
  namespace: llm-research-lab-data
spec:
  configuration:
    users:
      admin/password: k8s-secret:clickhouse-admin:password
      admin/networks/ip: "::/0"
      readonly/password: k8s-secret:clickhouse-readonly:password
      readonly/profile: readonly
      readonly/networks/ip: "::/0"

    profiles:
      readonly:
        readonly: 1
      default:
        max_memory_usage: 10000000000
        use_uncompressed_cache: 0
        load_balancing: random

    quotas:
      default:
        interval:
          duration: 3600
          queries: 1000
          errors: 100
          result_rows: 1000000000
          read_rows: 1000000000
          execution_time: 3600

    clusters:
      - name: llm-cluster
        layout:
          shardsCount: 3
          replicasCount: 2

    zookeeper:
      nodes:
      - host: zookeeper-0.zookeeper-headless.llm-research-lab-data.svc.cluster.local
        port: 2181
      - host: zookeeper-1.zookeeper-headless.llm-research-lab-data.svc.cluster.local
        port: 2181
      - host: zookeeper-2.zookeeper-headless.llm-research-lab-data.svc.cluster.local
        port: 2181

  defaults:
    templates:
      dataVolumeClaimTemplate: data-volume
      logVolumeClaimTemplate: log-volume
      serviceTemplate: chi-service-template

  templates:
    volumeClaimTemplates:
    - name: data-volume
      spec:
        accessModes:
        - ReadWriteOnce
        storageClassName: fast-ssd
        resources:
          requests:
            storage: 1Ti
    - name: log-volume
      spec:
        accessModes:
        - ReadWriteOnce
        storageClassName: standard
        resources:
          requests:
            storage: 100Gi

    serviceTemplates:
    - name: chi-service-template
      spec:
        type: ClusterIP
        ports:
        - name: http
          port: 8123
        - name: client
          port: 9000
        - name: interserver
          port: 9009

    podTemplates:
    - name: default
      spec:
        containers:
        - name: clickhouse
          image: clickhouse/clickhouse-server:23.8
          resources:
            requests:
              memory: "16Gi"
              cpu: "4"
            limits:
              memory: "32Gi"
              cpu: "8"
```

#### ClickHouse Distributed Table Example

```sql
-- Run on each ClickHouse instance
-- databases/clickhouse-schema.sql

-- Local table on each shard
CREATE TABLE IF NOT EXISTS experiment_metrics_local ON CLUSTER llm-cluster
(
    experiment_id UUID,
    metric_name LowCardinality(String),
    metric_value Float64,
    timestamp DateTime64(3),
    labels Map(String, String),
    INDEX idx_timestamp timestamp TYPE minmax GRANULARITY 3,
    INDEX idx_metric metric_name TYPE set(0) GRANULARITY 1
)
ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/experiment_metrics', '{replica}')
PARTITION BY toYYYYMM(timestamp)
ORDER BY (experiment_id, metric_name, timestamp)
TTL timestamp + INTERVAL 90 DAY
SETTINGS index_granularity = 8192;

-- Distributed table for queries
CREATE TABLE IF NOT EXISTS experiment_metrics ON CLUSTER llm-cluster
AS experiment_metrics_local
ENGINE = Distributed(llm-cluster, default, experiment_metrics_local, rand());
```

### 4.2.3 Connection Pooling (PgBouncer)

Deploy PgBouncer for PostgreSQL connection pooling.

```yaml
# databases/pgbouncer.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: pgbouncer-config
  namespace: llm-research-lab-data
data:
  pgbouncer.ini: |
    [databases]
    llm_research = host=llm-research-postgres-rw.llm-research-lab-data.svc.cluster.local port=5432 dbname=llm_research

    [pgbouncer]
    listen_addr = 0.0.0.0
    listen_port = 5432
    auth_type = scram-sha-256
    auth_file = /etc/pgbouncer/userlist.txt

    pool_mode = transaction
    max_client_conn = 2000
    default_pool_size = 25
    min_pool_size = 10
    reserve_pool_size = 5
    reserve_pool_timeout = 3
    max_db_connections = 100
    max_user_connections = 100

    server_reset_query = DISCARD ALL
    server_check_delay = 10
    server_check_query = SELECT 1

    log_connections = 1
    log_disconnections = 1
    log_pooler_errors = 1
    stats_period = 60

    ignore_startup_parameters = extra_float_digits
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pgbouncer
  namespace: llm-research-lab-data
spec:
  replicas: 3
  selector:
    matchLabels:
      app: pgbouncer
  template:
    metadata:
      labels:
        app: pgbouncer
    spec:
      containers:
      - name: pgbouncer
        image: edoburu/pgbouncer:1.21.0
        ports:
        - containerPort: 5432
          name: postgres
        resources:
          requests:
            cpu: "500m"
            memory: 512Mi
          limits:
            cpu: "2"
            memory: 2Gi
        volumeMounts:
        - name: config
          mountPath: /etc/pgbouncer
        livenessProbe:
          tcpSocket:
            port: 5432
          initialDelaySeconds: 15
          periodSeconds: 10
        readinessProbe:
          tcpSocket:
            port: 5432
          initialDelaySeconds: 5
          periodSeconds: 5
      volumes:
      - name: config
        configMap:
          name: pgbouncer-config
---
apiVersion: v1
kind: Service
metadata:
  name: pgbouncer
  namespace: llm-research-lab-data
spec:
  type: ClusterIP
  ports:
  - port: 5432
    targetPort: 5432
    protocol: TCP
    name: postgres
  selector:
    app: pgbouncer
```

### 4.2.4 Database Monitoring

Deploy Prometheus exporters for database metrics.

```yaml
# monitoring/postgres-exporter.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres-exporter
  namespace: llm-research-lab-data
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgres-exporter
  template:
    metadata:
      labels:
        app: postgres-exporter
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9187"
    spec:
      containers:
      - name: postgres-exporter
        image: prometheuscommunity/postgres-exporter:v0.15.0
        ports:
        - containerPort: 9187
          name: metrics
        env:
        - name: DATA_SOURCE_NAME
          valueFrom:
            secretKeyRef:
              name: postgres-exporter-secret
              key: connection-string
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
---
# monitoring/clickhouse-exporter.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: clickhouse-exporter
  namespace: llm-research-lab-data
spec:
  replicas: 1
  selector:
    matchLabels:
      app: clickhouse-exporter
  template:
    metadata:
      labels:
        app: clickhouse-exporter
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9116"
    spec:
      containers:
      - name: clickhouse-exporter
        image: f1yegor/clickhouse-exporter:latest
        ports:
        - containerPort: 9116
          name: metrics
        env:
        - name: CLICKHOUSE_URI
          value: "http://clickhouse-llm-research-clickhouse.llm-research-lab-data.svc.cluster.local:8123"
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
```

---

## 4.3 Message Queue Production

### 4.3.1 Kafka Cluster Sizing

Deploy production-grade Kafka with Strimzi operator.

#### Strimzi Kafka Cluster

```yaml
# kafka/kafka-cluster.yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: llm-research-kafka
  namespace: llm-research-lab-prod
spec:
  kafka:
    version: 3.6.0
    replicas: 5

    listeners:
    - name: plain
      port: 9092
      type: internal
      tls: false
    - name: tls
      port: 9093
      type: internal
      tls: true
      authentication:
        type: tls

    config:
      # Replication settings
      offsets.topic.replication.factor: 3
      transaction.state.log.replication.factor: 3
      transaction.state.log.min.isr: 2
      default.replication.factor: 3
      min.insync.replicas: 2

      # Performance settings
      num.network.threads: 8
      num.io.threads: 16
      socket.send.buffer.bytes: 102400
      socket.receive.buffer.bytes: 102400
      socket.request.max.bytes: 104857600

      # Retention settings
      log.retention.hours: 168  # 7 days
      log.segment.bytes: 1073741824  # 1GB
      log.retention.check.interval.ms: 300000

      # Compression
      compression.type: snappy

      # Leader election
      unclean.leader.election.enable: false
      auto.create.topics.enable: false

    storage:
      type: jbod
      volumes:
      - id: 0
        type: persistent-claim
        size: 1Ti
        class: fast-ssd
        deleteClaim: false

    resources:
      requests:
        memory: 16Gi
        cpu: "4"
      limits:
        memory: 32Gi
        cpu: "8"

    jvmOptions:
      -Xms: 8192m
      -Xmx: 8192m
      gcLoggingEnabled: true

    metricsConfig:
      type: jmxPrometheusExporter
      valueFrom:
        configMapKeyRef:
          name: kafka-metrics
          key: kafka-metrics-config.yml

  zookeeper:
    replicas: 3

    storage:
      type: persistent-claim
      size: 100Gi
      class: fast-ssd
      deleteClaim: false

    resources:
      requests:
        memory: 4Gi
        cpu: "2"
      limits:
        memory: 8Gi
        cpu: "4"

    metricsConfig:
      type: jmxPrometheusExporter
      valueFrom:
        configMapKeyRef:
          name: kafka-metrics
          key: zookeeper-metrics-config.yml

  entityOperator:
    topicOperator:
      resources:
        requests:
          memory: 512Mi
          cpu: "200m"
        limits:
          memory: 1Gi
          cpu: "1"
    userOperator:
      resources:
        requests:
          memory: 512Mi
          cpu: "200m"
        limits:
          memory: 1Gi
          cpu: "1"
```

### 4.3.2 Topic Configuration and Partitioning

```yaml
# kafka/topics.yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaTopic
metadata:
  name: experiments
  namespace: llm-research-lab-prod
  labels:
    strimzi.io/cluster: llm-research-kafka
spec:
  partitions: 30
  replicas: 3
  config:
    retention.ms: 604800000  # 7 days
    segment.ms: 3600000      # 1 hour
    compression.type: snappy
    min.insync.replicas: 2
    cleanup.policy: delete
---
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaTopic
metadata:
  name: experiment-results
  namespace: llm-research-lab-prod
  labels:
    strimzi.io/cluster: llm-research-kafka
spec:
  partitions: 20
  replicas: 3
  config:
    retention.ms: 2592000000  # 30 days
    segment.ms: 3600000
    compression.type: snappy
    min.insync.replicas: 2
    cleanup.policy: delete
---
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaTopic
metadata:
  name: metrics-events
  namespace: llm-research-lab-prod
  labels:
    strimzi.io/cluster: llm-research-kafka
spec:
  partitions: 50
  replicas: 3
  config:
    retention.ms: 259200000  # 3 days
    segment.ms: 1800000      # 30 minutes
    compression.type: lz4
    min.insync.replicas: 2
    cleanup.policy: delete
---
# High-volume topic for telemetry
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaTopic
metadata:
  name: telemetry-stream
  namespace: llm-research-lab-prod
  labels:
    strimzi.io/cluster: llm-research-kafka
spec:
  partitions: 100
  replicas: 3
  config:
    retention.ms: 86400000   # 1 day
    segment.ms: 600000       # 10 minutes
    compression.type: lz4
    min.insync.replicas: 2
    cleanup.policy: delete
```

### 4.3.3 Consumer Group Management

```yaml
# kafka/consumer-groups.yaml
# Rust application configuration for consumer groups

# Application-side configuration example (config.toml)
[kafka.consumer]
group_id = "experiment-processor-v1"
enable_auto_commit = false
auto_offset_reset = "earliest"
session_timeout_ms = 30000
max_poll_interval_ms = 300000
max_poll_records = 500

[kafka.consumer.processing]
max_concurrency = 10
batch_size = 100
commit_interval_ms = 5000

# Kafka Connect for CDC and integration
---
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaConnect
metadata:
  name: llm-research-connect
  namespace: llm-research-lab-prod
  annotations:
    strimzi.io/use-connector-resources: "true"
spec:
  version: 3.6.0
  replicas: 3
  bootstrapServers: llm-research-kafka-kafka-bootstrap:9093
  tls:
    trustedCertificates:
    - secretName: llm-research-kafka-cluster-ca-cert
      certificate: ca.crt

  config:
    group.id: connect-cluster
    offset.storage.topic: connect-cluster-offsets
    config.storage.topic: connect-cluster-configs
    status.storage.topic: connect-cluster-status

    config.storage.replication.factor: 3
    offset.storage.replication.factor: 3
    status.storage.replication.factor: 3

  resources:
    requests:
      memory: 4Gi
      cpu: "2"
    limits:
      memory: 8Gi
      cpu: "4"
```

### 4.3.4 Replication and Durability Settings

```yaml
# kafka/kafka-producer-config.yaml
# Producer configuration for Rust application

[kafka.producer]
bootstrap_servers = "llm-research-kafka-kafka-bootstrap:9093"
acks = "all"  # Wait for all in-sync replicas
retries = 2147483647
max_in_flight_requests_per_connection = 5
enable_idempotence = true
compression_type = "snappy"
linger_ms = 10
batch_size = 32768

# Durability guarantees
[kafka.producer.durability]
min_insync_replicas = 2
request_timeout_ms = 30000
delivery_timeout_ms = 120000
```

#### Kafka MirrorMaker 2 for DR

```yaml
# kafka/mirror-maker-2.yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaMirrorMaker2
metadata:
  name: llm-kafka-mirror
  namespace: llm-research-lab-prod
spec:
  version: 3.6.0
  replicas: 3
  connectCluster: "target"

  clusters:
  - alias: "source"
    bootstrapServers: llm-research-kafka-kafka-bootstrap:9093
    tls:
      trustedCertificates:
      - secretName: source-cluster-ca-cert
        certificate: ca.crt

  - alias: "target"
    bootstrapServers: dr-kafka-bootstrap.dr-namespace:9093
    tls:
      trustedCertificates:
      - secretName: target-cluster-ca-cert
        certificate: ca.crt
    config:
      config.storage.replication.factor: 3
      offset.storage.replication.factor: 3
      status.storage.replication.factor: 3

  mirrors:
  - sourceCluster: "source"
    targetCluster: "target"
    sourceConnector:
      config:
        replication.factor: 3
        offset-syncs.topic.replication.factor: 3
        sync.topic.acls.enabled: "false"

    heartbeatConnector:
      config:
        heartbeats.topic.replication.factor: 3

    checkpointConnector:
      config:
        checkpoints.topic.replication.factor: 3
        sync.group.offsets.enabled: "true"

    topicsPattern: ".*"
    groupsPattern: ".*"
```

---

## 4.4 Caching Layer

### 4.4.1 Redis Cluster Configuration

Deploy Redis in cluster mode for high availability.

```yaml
# redis/redis-cluster.yaml
apiVersion: redis.redis.opstreelabs.in/v1beta1
kind: RedisCluster
metadata:
  name: llm-research-redis
  namespace: llm-research-lab-prod
spec:
  clusterSize: 6  # 3 masters + 3 replicas
  clusterVersion: v7.2

  kubernetesConfig:
    image: redis:7.2-alpine
    imagePullPolicy: IfNotPresent

    redisSecret:
      name: redis-secret
      key: password

    resources:
      requests:
        cpu: "1"
        memory: 4Gi
      limits:
        cpu: "2"
        memory: 8Gi

  storage:
    volumeClaimTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        storageClassName: fast-ssd
        resources:
          requests:
            storage: 100Gi

  redisExporter:
    enabled: true
    image: oliver006/redis_exporter:v1.55.0
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 512Mi

  redisConfig:
    # Memory management
    maxmemory: "6gb"
    maxmemory-policy: "allkeys-lru"

    # Persistence
    save: "900 1 300 10 60 10000"
    appendonly: "yes"
    appendfsync: "everysec"

    # Performance
    tcp-backlog: "511"
    timeout: "300"
    tcp-keepalive: "300"

    # Slow log
    slowlog-log-slower-than: "10000"
    slowlog-max-len: "128"

    # Client output buffer limits
    client-output-buffer-limit: "normal 0 0 0 slave 256mb 64mb 60 pubsub 32mb 8mb 60"
```

### 4.4.2 Redis Sentinel for Master Election

Alternative: Sentinel mode for simpler high availability.

```yaml
# redis/redis-sentinel.yaml
apiVersion: redis.redis.opstreelabs.in/v1beta1
kind: Redis
metadata:
  name: llm-research-redis-sentinel
  namespace: llm-research-lab-prod
spec:
  kubernetesConfig:
    image: redis:7.2-alpine
    imagePullPolicy: IfNotPresent

    redisSecret:
      name: redis-secret
      key: password

    resources:
      requests:
        cpu: "1"
        memory: 4Gi
      limits:
        cpu: "2"
        memory: 8Gi

  redisConfig:
    maxmemory: "6gb"
    maxmemory-policy: "allkeys-lru"
    appendonly: "yes"

  storage:
    volumeClaimTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        storageClassName: fast-ssd
        resources:
          requests:
            storage: 100Gi

  redisExporter:
    enabled: true
    image: oliver006/redis_exporter:v1.55.0
---
apiVersion: redis.redis.opstreelabs.in/v1beta1
kind: RedisSentinel
metadata:
  name: llm-research-sentinel
  namespace: llm-research-lab-prod
spec:
  clusterSize: 3

  kubernetesConfig:
    image: redis:7.2-alpine
    imagePullPolicy: IfNotPresent

    resources:
      requests:
        cpu: "200m"
        memory: 512Mi
      limits:
        cpu: "1"
        memory: 1Gi

  redisSentinelConfig:
    down-after-milliseconds: "5000"
    failover-timeout: "10000"
    parallel-syncs: "1"
    quorum: "2"

    monitor:
      master: llm-research-redis-sentinel
      namespace: llm-research-lab-prod
```

### 4.4.3 Cache Eviction Policies

Configure different eviction strategies per use case.

```toml
# Application configuration (config.toml)

[cache.sessions]
# Volatile LRU for session data
redis_db = 0
maxmemory_policy = "volatile-lru"
ttl_seconds = 3600

[cache.experiments]
# Allkeys LRU for experiment metadata
redis_db = 1
maxmemory_policy = "allkeys-lru"
ttl_seconds = 86400

[cache.metrics]
# Volatile TTL for time-series metrics
redis_db = 2
maxmemory_policy = "volatile-ttl"
ttl_seconds = 1800

[cache.rate_limiting]
# No eviction for rate limit counters
redis_db = 3
maxmemory_policy = "noeviction"
ttl_seconds = 60
```

### 4.4.4 Persistence Configuration

```yaml
# redis/redis-persistence-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: redis-persistence-config
  namespace: llm-research-lab-prod
data:
  redis.conf: |
    # RDB Snapshots
    save 900 1       # Save after 900s if at least 1 key changed
    save 300 10      # Save after 300s if at least 10 keys changed
    save 60 10000    # Save after 60s if at least 10000 keys changed

    rdbcompression yes
    rdbchecksum yes
    dbfilename dump.rdb
    dir /data

    # AOF Persistence
    appendonly yes
    appendfilename "appendonly.aof"
    appendfsync everysec  # Balance between performance and durability

    # AOF rewrite configuration
    auto-aof-rewrite-percentage 100
    auto-aof-rewrite-min-size 64mb
    aof-load-truncated yes
    aof-use-rdb-preamble yes  # Hybrid RDB+AOF for faster restarts
```

### 4.4.5 Memory Management

```yaml
# redis/redis-memory-policy.yaml
# Configured via RedisConfig in operator

# Monitoring ConfigMap for memory alerts
apiVersion: v1
kind: ConfigMap
metadata:
  name: redis-memory-alerts
  namespace: llm-research-lab-monitoring
data:
  alerts.yml: |
    groups:
    - name: redis_memory
      interval: 30s
      rules:
      - alert: RedisMemoryHigh
        expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Redis memory usage is above 85%"
          description: "Redis instance {{ $labels.instance }} is using {{ $value | humanizePercentage }} of available memory"

      - alert: RedisMemoryCritical
        expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.95
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Redis memory usage is critical"
          description: "Redis instance {{ $labels.instance }} is using {{ $value | humanizePercentage }} of available memory"

      - alert: RedisEvictionRateHigh
        expr: rate(redis_evicted_keys_total[5m]) > 100
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High eviction rate detected"
          description: "Redis is evicting {{ $value }} keys per second"
```

---

## 4.5 Load Balancing & Ingress

### 4.5.1 Ingress Controller Configuration

Deploy NGINX Ingress Controller with production settings.

```yaml
# ingress/nginx-ingress-controller.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: ingress-nginx
---
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: nginx-ingress
  namespace: ingress-nginx
spec:
  repo: https://kubernetes.github.io/ingress-nginx
  chart: ingress-nginx
  version: 4.8.0

  valuesContent: |-
    controller:
      replicaCount: 3

      resources:
        requests:
          cpu: "1"
          memory: 2Gi
        limits:
          cpu: "2"
          memory: 4Gi

      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - ingress-nginx
            topologyKey: kubernetes.io/hostname

      config:
        # Performance tuning
        worker-processes: "auto"
        worker-connections: "65536"
        max-worker-connections: "65536"

        # Timeouts
        proxy-connect-timeout: "10"
        proxy-send-timeout: "60"
        proxy-read-timeout: "60"
        client-body-timeout: "60"

        # Buffer sizes
        proxy-buffer-size: "8k"
        proxy-buffers-number: "4"
        client-body-buffer-size: "1m"

        # Compression
        use-gzip: "true"
        gzip-level: "5"
        gzip-types: "application/json text/plain text/css application/javascript"

        # Security
        ssl-protocols: "TLSv1.2 TLSv1.3"
        ssl-ciphers: "ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384"
        ssl-prefer-server-ciphers: "true"

        # Rate limiting
        limit-req-status-code: "429"
        limit-conn-status-code: "429"

        # Logging
        log-format-upstream: '$remote_addr - $remote_user [$time_local] "$request" $status $body_bytes_sent "$http_referer" "$http_user_agent" $request_length $request_time [$proxy_upstream_name] [$proxy_alternative_upstream_name] $upstream_addr $upstream_response_length $upstream_response_time $upstream_status $req_id'

        # Header size
        large-client-header-buffers: "4 16k"

      metrics:
        enabled: true
        serviceMonitor:
          enabled: true

      service:
        type: LoadBalancer
        annotations:
          service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
          service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
```

### 4.5.2 TLS Termination

Configure TLS certificates with cert-manager.

```yaml
# ingress/cert-manager.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: cert-manager
---
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: cert-manager
  namespace: cert-manager
spec:
  repo: https://charts.jetstack.io
  chart: cert-manager
  version: v1.13.0

  valuesContent: |-
    installCRDs: true

    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 512Mi
---
# ClusterIssuer for Let's Encrypt
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-prod
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    email: platform@company.com
    privateKeySecretRef:
      name: letsencrypt-prod-key
    solvers:
    - http01:
        ingress:
          class: nginx
---
# Certificate resource
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: llm-research-lab-tls
  namespace: llm-research-lab-prod
spec:
  secretName: llm-research-lab-tls
  issuerRef:
    name: letsencrypt-prod
    kind: ClusterIssuer
  dnsNames:
  - api.llm-research-lab.company.com
  - research.llm-research-lab.company.com
```

### 4.5.3 Rate Limiting

Implement rate limiting at ingress level.

```yaml
# ingress/api-ingress-with-rate-limit.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: llm-research-api
  namespace: llm-research-lab-prod
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"

    # Rate limiting - 100 requests per minute per IP
    nginx.ingress.kubernetes.io/limit-rps: "100"
    nginx.ingress.kubernetes.io/limit-burst-multiplier: "5"

    # Connection limiting
    nginx.ingress.kubernetes.io/limit-connections: "20"

    # Request body size
    nginx.ingress.kubernetes.io/proxy-body-size: "10m"

    # CORS
    nginx.ingress.kubernetes.io/enable-cors: "true"
    nginx.ingress.kubernetes.io/cors-allow-origin: "https://dashboard.company.com"
    nginx.ingress.kubernetes.io/cors-allow-methods: "GET, POST, PUT, DELETE, OPTIONS"
    nginx.ingress.kubernetes.io/cors-allow-credentials: "true"

    # Security headers
    nginx.ingress.kubernetes.io/configuration-snippet: |
      more_set_headers "X-Frame-Options: DENY";
      more_set_headers "X-Content-Type-Options: nosniff";
      more_set_headers "X-XSS-Protection: 1; mode=block";
      more_set_headers "Strict-Transport-Security: max-age=31536000; includeSubDomains";
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - api.llm-research-lab.company.com
    secretName: llm-research-lab-tls
  rules:
  - host: api.llm-research-lab.company.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: llm-research-api
            port:
              number: 8080
```

### 4.5.4 Request Routing Rules

Advanced routing with canary deployments and A/B testing.

```yaml
# ingress/canary-routing.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: llm-research-api-stable
  namespace: llm-research-lab-prod
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - api.llm-research-lab.company.com
    secretName: llm-research-lab-tls
  rules:
  - host: api.llm-research-lab.company.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: llm-research-api-stable
            port:
              number: 8080
---
# Canary ingress - 10% traffic to new version
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: llm-research-api-canary
  namespace: llm-research-lab-prod
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/canary: "true"
    nginx.ingress.kubernetes.io/canary-weight: "10"
    # Alternative: Header-based routing
    # nginx.ingress.kubernetes.io/canary-by-header: "X-Canary"
    # nginx.ingress.kubernetes.io/canary-by-header-value: "always"
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - api.llm-research-lab.company.com
    secretName: llm-research-lab-tls
  rules:
  - host: api.llm-research-lab.company.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: llm-research-api-canary
            port:
              number: 8080
```

### 4.5.5 Health Check Configuration

```yaml
# ingress/health-check-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: llm-research-health
  namespace: llm-research-lab-prod
  annotations:
    nginx.ingress.kubernetes.io/upstream-health-checks: "true"
    nginx.ingress.kubernetes.io/upstream-health-checks-interval: "10s"
    nginx.ingress.kubernetes.io/upstream-health-checks-timeout: "5s"
    nginx.ingress.kubernetes.io/upstream-health-checks-port: "8080"
    nginx.ingress.kubernetes.io/upstream-health-checks-path: "/health/ready"
    nginx.ingress.kubernetes.io/upstream-health-checks-mandatory: "true"
spec:
  ingressClassName: nginx
  rules:
  - host: api.llm-research-lab.company.com
    http:
      paths:
      - path: /health
        pathType: Prefix
        backend:
          service:
            name: llm-research-api
            port:
              number: 8080
```

---

## 4.6 Service Mesh (Optional)

### 4.6.1 Istio Configuration

Deploy Istio for advanced traffic management and security.

#### Istio Installation

```yaml
# service-mesh/istio-operator.yaml
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
metadata:
  name: llm-research-istio
  namespace: istio-system
spec:
  profile: production

  components:
    pilot:
      k8s:
        resources:
          requests:
            cpu: 500m
            memory: 2Gi
          limits:
            cpu: 2000m
            memory: 4Gi
        hpaSpec:
          minReplicas: 2
          maxReplicas: 5

    ingressGateways:
    - name: istio-ingressgateway
      enabled: true
      k8s:
        resources:
          requests:
            cpu: 1000m
            memory: 1Gi
          limits:
            cpu: 2000m
            memory: 2Gi
        hpaSpec:
          minReplicas: 3
          maxReplicas: 10
        service:
          type: LoadBalancer

    egressGateways:
    - name: istio-egressgateway
      enabled: true
      k8s:
        resources:
          requests:
            cpu: 500m
            memory: 512Mi
          limits:
            cpu: 1000m
            memory: 1Gi

  meshConfig:
    accessLogFile: /dev/stdout
    accessLogEncoding: JSON

    enableTracing: true
    defaultConfig:
      tracing:
        zipkin:
          address: jaeger-collector.observability:9411
        sampling: 1.0  # 100% sampling for production

    # Telemetry
    enablePrometheusMerge: true

  values:
    global:
      proxy:
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 2000m
            memory: 1Gi

        # Lifecycle configuration
        holdApplicationUntilProxyStarts: true

      # mTLS settings
      mtls:
        enabled: true
        auto: true
```

### 4.6.2 mTLS Between Services

Enforce mutual TLS for service-to-service communication.

```yaml
# service-mesh/peer-authentication.yaml
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: default
  namespace: llm-research-lab-prod
spec:
  mtls:
    mode: STRICT
---
# Authorization policy
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: allow-same-namespace
  namespace: llm-research-lab-prod
spec:
  action: ALLOW
  rules:
  - from:
    - source:
        namespaces: ["llm-research-lab-prod"]
---
# Allow specific service communication
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: api-to-database
  namespace: llm-research-lab-data
spec:
  selector:
    matchLabels:
      app: pgbouncer
  action: ALLOW
  rules:
  - from:
    - source:
        principals: ["cluster.local/ns/llm-research-lab-prod/sa/llm-api-sa"]
    to:
    - operation:
        ports: ["5432"]
```

### 4.6.3 Traffic Management Policies

#### Circuit Breaking

```yaml
# service-mesh/destination-rule-circuit-breaker.yaml
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: llm-api-circuit-breaker
  namespace: llm-research-lab-prod
spec:
  host: llm-research-api.llm-research-lab-prod.svc.cluster.local
  trafficPolicy:
    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 50
        http2MaxRequests: 100
        maxRequestsPerConnection: 2

    outlierDetection:
      consecutiveErrors: 5
      interval: 30s
      baseEjectionTime: 60s
      maxEjectionPercent: 50
      minHealthPercent: 30

    loadBalancer:
      simple: LEAST_REQUEST
```

#### Request Timeout and Retry

```yaml
# service-mesh/virtual-service-retry.yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: llm-api-resilience
  namespace: llm-research-lab-prod
spec:
  hosts:
  - llm-research-api.llm-research-lab-prod.svc.cluster.local
  http:
  - match:
    - uri:
        prefix: /api/v1/experiments
    timeout: 30s
    retries:
      attempts: 3
      perTryTimeout: 10s
      retryOn: 5xx,reset,connect-failure,refused-stream
    route:
    - destination:
        host: llm-research-api.llm-research-lab-prod.svc.cluster.local
        port:
          number: 8080
```

#### Traffic Splitting

```yaml
# service-mesh/virtual-service-traffic-split.yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: llm-api-traffic-split
  namespace: llm-research-lab-prod
spec:
  hosts:
  - api.llm-research-lab.company.com
  gateways:
  - llm-gateway
  http:
  - match:
    - headers:
        user-agent:
          regex: ".*mobile.*"
    route:
    - destination:
        host: llm-research-api-mobile
        subset: v2
      weight: 100

  - route:
    - destination:
        host: llm-research-api
        subset: v1
      weight: 90
    - destination:
        host: llm-research-api
        subset: v2
      weight: 10
---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: llm-api-subsets
  namespace: llm-research-lab-prod
spec:
  host: llm-research-api.llm-research-lab-prod.svc.cluster.local
  subsets:
  - name: v1
    labels:
      version: v1.0.0
  - name: v2
    labels:
      version: v2.0.0
```

#### Rate Limiting with Envoy Filter

```yaml
# service-mesh/envoy-rate-limit.yaml
apiVersion: networking.istio.io/v1alpha3
kind: EnvoyFilter
metadata:
  name: rate-limit-filter
  namespace: istio-system
spec:
  workloadSelector:
    labels:
      app: istio-ingressgateway
  configPatches:
  - applyTo: HTTP_FILTER
    match:
      context: GATEWAY
      listener:
        filterChain:
          filter:
            name: "envoy.filters.network.http_connection_manager"
            subFilter:
              name: "envoy.filters.http.router"
    patch:
      operation: INSERT_BEFORE
      value:
        name: envoy.filters.http.local_ratelimit
        typed_config:
          "@type": type.googleapis.com/udpa.type.v1.TypedStruct
          type_url: type.googleapis.com/envoy.extensions.filters.http.local_ratelimit.v3.LocalRateLimit
          value:
            stat_prefix: http_local_rate_limiter
            token_bucket:
              max_tokens: 1000
              tokens_per_fill: 1000
              fill_interval: 60s
            filter_enabled:
              runtime_key: local_rate_limit_enabled
              default_value:
                numerator: 100
                denominator: HUNDRED
            filter_enforced:
              runtime_key: local_rate_limit_enforced
              default_value:
                numerator: 100
                denominator: HUNDRED
```

### 4.6.4 Gateway Configuration

```yaml
# service-mesh/istio-gateway.yaml
apiVersion: networking.istio.io/v1beta1
kind: Gateway
metadata:
  name: llm-gateway
  namespace: llm-research-lab-prod
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 443
      name: https
      protocol: HTTPS
    tls:
      mode: SIMPLE
      credentialName: llm-research-lab-tls
    hosts:
    - api.llm-research-lab.company.com
    - research.llm-research-lab.company.com

  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - api.llm-research-lab.company.com
    - research.llm-research-lab.company.com
    tls:
      httpsRedirect: true
---
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: llm-gateway-routes
  namespace: llm-research-lab-prod
spec:
  hosts:
  - api.llm-research-lab.company.com
  gateways:
  - llm-gateway
  http:
  - match:
    - uri:
        prefix: /api/v1/
    route:
    - destination:
        host: llm-research-api.llm-research-lab-prod.svc.cluster.local
        port:
          number: 8080
```

### 4.6.5 Observability with Istio

```yaml
# service-mesh/telemetry.yaml
apiVersion: telemetry.istio.io/v1alpha1
kind: Telemetry
metadata:
  name: mesh-telemetry
  namespace: istio-system
spec:
  accessLogging:
  - providers:
    - name: envoy

  metrics:
  - providers:
    - name: prometheus
    dimensions:
      request_protocol: request.protocol
      response_code: response.code
      source_workload: source.workload.name
      destination_workload: destination.workload.name

  tracing:
  - providers:
    - name: jaeger
    randomSamplingPercentage: 100.0
    customTags:
      environment:
        literal:
          value: production
      version:
        environment:
          name: APP_VERSION
```

---

## 4.7 Helm Chart for Complete Deployment

### 4.7.1 Helm Values Production

```yaml
# helm/values-production.yaml
global:
  environment: production
  domain: llm-research-lab.company.com
  storageClass: fast-ssd

  registry:
    url: ghcr.io/company
    imagePullSecrets:
    - name: ghcr-pull-secret

api:
  replicaCount: 3
  image:
    repository: ghcr.io/company/llm-research-api
    tag: "v1.0.0"
    pullPolicy: IfNotPresent

  resources:
    requests:
      cpu: "2"
      memory: 4Gi
    limits:
      cpu: "4"
      memory: 8Gi

  autoscaling:
    enabled: true
    minReplicas: 3
    maxReplicas: 20
    targetCPU: 70
    targetMemory: 80

  service:
    type: ClusterIP
    port: 8080

  ingress:
    enabled: true
    className: nginx
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt-prod
      nginx.ingress.kubernetes.io/limit-rps: "100"
    hosts:
    - host: api.llm-research-lab.company.com
      paths:
      - path: /
        pathType: Prefix
    tls:
    - secretName: llm-api-tls
      hosts:
      - api.llm-research-lab.company.com

worker:
  replicaCount: 5
  image:
    repository: ghcr.io/company/llm-research-worker
    tag: "v1.0.0"

  resources:
    requests:
      cpu: "4"
      memory: 8Gi
    limits:
      cpu: "8"
      memory: 16Gi

  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 50

postgresql:
  enabled: true
  instances: 3
  storage:
    size: 500Gi
  backup:
    enabled: true
    schedule: "0 2 * * *"
    retentionDays: 30

clickhouse:
  enabled: true
  shards: 3
  replicas: 2
  storage:
    size: 1Ti

kafka:
  enabled: true
  replicas: 5
  zookeeper:
    replicas: 3
  storage:
    size: 1Ti

redis:
  enabled: true
  cluster:
    enabled: true
    nodes: 6
  storage:
    size: 100Gi

monitoring:
  prometheus:
    enabled: true
    retention: 30d
    storage: 500Gi

  grafana:
    enabled: true
    adminPassword: <from-secret>

  jaeger:
    enabled: true
    storage:
      type: elasticsearch

serviceMesh:
  istio:
    enabled: false  # Optional
```

---

## Document Metadata

| Field | Value |
|-------|-------|
| **Version** | 1.0.0 |
| **Status** | Complete |
| **SPARC Phase** | Phase 5 - Completion |
| **Section** | 4 - Production Infrastructure |
| **Created** | 2025-11-28 |
| **Target SLA** | 99.9% |
| **Tech Stack** | Kubernetes, PostgreSQL, ClickHouse, Kafka, Redis |

---

## Next Steps

1. **Review and Validate**: Infrastructure team reviews all configurations
2. **Provision Infrastructure**: Set up Kubernetes clusters and node pools
3. **Deploy Base Components**: Install operators (Strimzi, CloudNativePG, Redis Operator)
4. **Configure Networking**: Set up ingress controllers and service mesh
5. **Deploy Data Layer**: PostgreSQL, ClickHouse, and connection pooling
6. **Deploy Message Queue**: Kafka cluster with topics and consumer groups
7. **Deploy Caching**: Redis cluster/sentinel configuration
8. **Security Hardening**: Network policies, RBAC, secrets management
9. **Monitoring Setup**: Prometheus, Grafana, Jaeger deployment
10. **Load Testing**: Validate infrastructure under expected load
11. **Disaster Recovery**: Test backup/restore and failover procedures
12. **Documentation**: Create runbooks and operational procedures

---

*This document is part of the SPARC Phase 5 (Completion) specification for LLM-Research-Lab, providing production-ready infrastructure configurations for enterprise-grade deployment.*
---

## Operations & Monitoring


## Table of Contents

1. [Overview](#overview)
2. [5.1 Observability Stack](#51-observability-stack)
3. [5.2 Alerting Configuration](#52-alerting-configuration)
4. [5.3 SLO/SLI Definition](#53-slosli-definition)
5. [5.4 On-Call Procedures](#54-on-call-procedures)
6. [5.5 Capacity Management](#55-capacity-management)
7. [5.6 Security Operations](#56-security-operations)
8. [Appendix A: Configuration Files](#appendix-a-configuration-files)
9. [Appendix B: Dashboard Templates](#appendix-b-dashboard-templates)
10. [Appendix C: Runbook Index](#appendix-c-runbook-index)

---

## Overview

This section defines the complete operations and monitoring strategy for LLM-Research-Lab, ensuring enterprise-grade reliability, observability, and incident response capabilities. The specification targets a 99.9% availability SLA with comprehensive monitoring, alerting, and operational procedures.

### Design Philosophy

1. **Observability by Default**: Every component emits structured metrics, logs, and traces
2. **Actionable Alerts**: Every alert has a clear runbook and escalation path
3. **Proactive Detection**: SLO violations trigger automated responses before user impact
4. **Data-Driven Operations**: All operational decisions backed by metrics and trends
5. **Defense in Depth**: Multiple layers of monitoring with correlation across signals

### Technology Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| Metrics | Prometheus | Time-series metric collection and storage |
| Logging | Loki + Vector | Structured log aggregation and indexing |
| Tracing | OpenTelemetry + Jaeger | Distributed request tracing |
| Dashboards | Grafana | Unified visualization and alerting |
| Alerting | Alertmanager + PagerDuty | Alert routing and on-call management |
| APM | Grafana Tempo | Application performance monitoring |
| Profiling | pprof + pyroscope | Continuous profiling |

---

## 5.1 Observability Stack

### 5.1.1 Metrics Collection (Prometheus)

#### Core Metrics Architecture

```yaml
# prometheus.yml
global:
  scrape_interval: 15s
  scrape_timeout: 10s
  evaluation_interval: 15s
  external_labels:
    cluster: 'llm-research-lab-prod'
    environment: 'production'
    region: 'us-east-1'

# Alertmanager configuration
alerting:
  alertmanagers:
    - static_configs:
        - targets:
            - alertmanager:9093
      timeout: 10s
      api_version: v2

# Rule files for alerts and recording rules
rule_files:
  - /etc/prometheus/rules/*.yml
  - /etc/prometheus/alerts/*.yml

# Scrape configurations
scrape_configs:
  # LLM Research Lab API
  - job_name: 'llm-research-lab-api'
    scrape_interval: 10s
    static_configs:
      - targets: ['api:9090']
    relabel_configs:
      - source_labels: [__address__]
        target_label: instance
      - source_labels: [__metrics_path__]
        target_label: metrics_path

  # Experiment Runner
  - job_name: 'experiment-runner'
    scrape_interval: 15s
    kubernetes_sd_configs:
      - role: pod
        namespaces:
          names:
            - llm-research-lab
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_label_app]
        regex: experiment-runner
        action: keep
      - source_labels: [__meta_kubernetes_pod_name]
        target_label: pod
      - source_labels: [__meta_kubernetes_namespace]
        target_label: namespace

  # PostgreSQL Exporter
  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']
    metric_relabel_configs:
      - source_labels: [__name__]
        regex: 'pg_(database_size_bytes|stat_.*|locks_count)'
        action: keep

  # Redis Exporter
  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']

  # Node Exporter (System Metrics)
  - job_name: 'node'
    kubernetes_sd_configs:
      - role: node
    relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)

  # cAdvisor (Container Metrics)
  - job_name: 'cadvisor'
    kubernetes_sd_configs:
      - role: node
    relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - target_label: __address__
        replacement: kubernetes.default.svc:443
      - source_labels: [__meta_kubernetes_node_name]
        regex: (.+)
        target_label: __metrics_path__
        replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor

# Remote write for long-term storage
remote_write:
  - url: "https://prometheus-remote-storage.example.com/api/v1/write"
    queue_config:
      capacity: 10000
      max_shards: 50
      min_shards: 1
      max_samples_per_send: 5000
      batch_send_deadline: 5s
    write_relabel_configs:
      - source_labels: [__name__]
        regex: 'experiment_.*|slo_.*|api_request_.*'
        action: keep
```

#### Custom Application Metrics

**Rust Implementation (using `prometheus` crate)**

```rust
// src/metrics/mod.rs
use prometheus::{
    Counter, CounterVec, Gauge, GaugeVec, Histogram, HistogramVec,
    Registry, Opts, HistogramOpts,
};
use lazy_static::lazy_static;

lazy_static! {
    // HTTP Request Metrics
    pub static ref HTTP_REQUEST_DURATION: HistogramVec = HistogramVec::new(
        HistogramOpts::new(
            "api_request_duration_seconds",
            "HTTP request latency in seconds"
        )
        .buckets(vec![0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0]),
        &["method", "path", "status"]
    ).unwrap();

    pub static ref HTTP_REQUEST_TOTAL: CounterVec = CounterVec::new(
        Opts::new("api_request_total", "Total HTTP requests"),
        &["method", "path", "status"]
    ).unwrap();

    pub static ref HTTP_REQUEST_IN_FLIGHT: GaugeVec = GaugeVec::new(
        Opts::new("api_request_in_flight", "Currently processing HTTP requests"),
        &["method", "path"]
    ).unwrap();

    // Experiment Metrics
    pub static ref EXPERIMENT_TOTAL: CounterVec = CounterVec::new(
        Opts::new("experiment_total", "Total experiments executed"),
        &["status", "experiment_type"]
    ).unwrap();

    pub static ref EXPERIMENT_DURATION: HistogramVec = HistogramVec::new(
        HistogramOpts::new(
            "experiment_duration_seconds",
            "Experiment execution duration"
        )
        .buckets(vec![1.0, 5.0, 10.0, 30.0, 60.0, 300.0, 600.0, 1800.0, 3600.0]),
        &["experiment_type", "dataset_size"]
    ).unwrap();

    pub static ref EXPERIMENT_ACTIVE: Gauge = Gauge::new(
        "experiment_active_count",
        "Currently running experiments"
    ).unwrap();

    // Database Metrics
    pub static ref DB_QUERY_DURATION: HistogramVec = HistogramVec::new(
        HistogramOpts::new(
            "db_query_duration_seconds",
            "Database query duration"
        )
        .buckets(vec![0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]),
        &["query_type", "table"]
    ).unwrap();

    pub static ref DB_CONNECTION_POOL_SIZE: Gauge = Gauge::new(
        "db_connection_pool_size",
        "Database connection pool size"
    ).unwrap();

    pub static ref DB_CONNECTION_POOL_IDLE: Gauge = Gauge::new(
        "db_connection_pool_idle",
        "Idle database connections"
    ).unwrap();

    // Model Inference Metrics
    pub static ref MODEL_INFERENCE_DURATION: HistogramVec = HistogramVec::new(
        HistogramOpts::new(
            "model_inference_duration_seconds",
            "Model inference latency"
        )
        .buckets(vec![0.01, 0.05, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0]),
        &["model_name", "model_version", "provider"]
    ).unwrap();

    pub static ref MODEL_INFERENCE_TOKENS: HistogramVec = HistogramVec::new(
        HistogramOpts::new(
            "model_inference_tokens_total",
            "Total tokens processed per inference"
        )
        .buckets(vec![10.0, 50.0, 100.0, 500.0, 1000.0, 2000.0, 4000.0, 8000.0]),
        &["model_name", "token_type"]
    ).unwrap();

    pub static ref MODEL_INFERENCE_COST: HistogramVec = HistogramVec::new(
        HistogramOpts::new(
            "model_inference_cost_usd",
            "Cost per inference in USD"
        )
        .buckets(vec![0.0001, 0.001, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0]),
        &["model_name", "provider"]
    ).unwrap();

    // Dataset Metrics
    pub static ref DATASET_LOAD_DURATION: HistogramVec = HistogramVec::new(
        HistogramOpts::new(
            "dataset_load_duration_seconds",
            "Dataset loading duration"
        )
        .buckets(vec![0.1, 0.5, 1.0, 5.0, 10.0, 30.0, 60.0, 300.0]),
        &["dataset_name", "dataset_version", "format"]
    ).unwrap();

    pub static ref DATASET_SIZE_BYTES: GaugeVec = GaugeVec::new(
        Opts::new("dataset_size_bytes", "Dataset size in bytes"),
        &["dataset_name", "dataset_version"]
    ).unwrap();

    // Cache Metrics
    pub static ref CACHE_HIT_TOTAL: CounterVec = CounterVec::new(
        Opts::new("cache_hit_total", "Cache hits"),
        &["cache_name", "cache_type"]
    ).unwrap();

    pub static ref CACHE_MISS_TOTAL: CounterVec = CounterVec::new(
        Opts::new("cache_miss_total", "Cache misses"),
        &["cache_name", "cache_type"]
    ).unwrap();

    // Resource Metrics
    pub static ref MEMORY_USAGE_BYTES: GaugeVec = GaugeVec::new(
        Opts::new("memory_usage_bytes", "Memory usage by component"),
        &["component"]
    ).unwrap();

    pub static ref CPU_USAGE_SECONDS_TOTAL: CounterVec = CounterVec::new(
        Opts::new("cpu_usage_seconds_total", "CPU time by component"),
        &["component"]
    ).unwrap();

    // Error Metrics
    pub static ref ERROR_TOTAL: CounterVec = CounterVec::new(
        Opts::new("error_total", "Total errors"),
        &["error_type", "component", "severity"]
    ).unwrap();

    pub static ref PANIC_TOTAL: Counter = Counter::new(
        "panic_total",
        "Total panics (should always be 0)"
    ).unwrap();
}

// Metrics registration
pub fn register_metrics(registry: &Registry) -> Result<(), Box<dyn std::error::Error>> {
    registry.register(Box::new(HTTP_REQUEST_DURATION.clone()))?;
    registry.register(Box::new(HTTP_REQUEST_TOTAL.clone()))?;
    registry.register(Box::new(HTTP_REQUEST_IN_FLIGHT.clone()))?;
    registry.register(Box::new(EXPERIMENT_TOTAL.clone()))?;
    registry.register(Box::new(EXPERIMENT_DURATION.clone()))?;
    registry.register(Box::new(EXPERIMENT_ACTIVE.clone()))?;
    registry.register(Box::new(DB_QUERY_DURATION.clone()))?;
    registry.register(Box::new(DB_CONNECTION_POOL_SIZE.clone()))?;
    registry.register(Box::new(DB_CONNECTION_POOL_IDLE.clone()))?;
    registry.register(Box::new(MODEL_INFERENCE_DURATION.clone()))?;
    registry.register(Box::new(MODEL_INFERENCE_TOKENS.clone()))?;
    registry.register(Box::new(MODEL_INFERENCE_COST.clone()))?;
    registry.register(Box::new(DATASET_LOAD_DURATION.clone()))?;
    registry.register(Box::new(DATASET_SIZE_BYTES.clone()))?;
    registry.register(Box::new(CACHE_HIT_TOTAL.clone()))?;
    registry.register(Box::new(CACHE_MISS_TOTAL.clone()))?;
    registry.register(Box::new(MEMORY_USAGE_BYTES.clone()))?;
    registry.register(Box::new(CPU_USAGE_SECONDS_TOTAL.clone()))?;
    registry.register(Box::new(ERROR_TOTAL.clone()))?;
    registry.register(Box::new(PANIC_TOTAL.clone()))?;
    Ok(())
}

// Middleware for automatic HTTP metrics
pub async fn metrics_middleware(
    req: axum::extract::Request,
    next: axum::middleware::Next,
) -> axum::response::Response {
    let method = req.method().to_string();
    let path = req.uri().path().to_string();

    let in_flight = HTTP_REQUEST_IN_FLIGHT
        .with_label_values(&[&method, &path]);
    in_flight.inc();

    let timer = HTTP_REQUEST_DURATION
        .with_label_values(&[&method, &path, "pending"])
        .start_timer();

    let response = next.run(req).await;

    let status = response.status().as_u16().to_string();
    HTTP_REQUEST_TOTAL
        .with_label_values(&[&method, &path, &status])
        .inc();

    timer.observe_duration();
    HTTP_REQUEST_DURATION
        .with_label_values(&[&method, &path, &status])
        .observe(timer.stop_and_record());

    in_flight.dec();

    response
}
```

#### Recording Rules

```yaml
# /etc/prometheus/rules/recording_rules.yml
groups:
  - name: api_slo_recording_rules
    interval: 30s
    rules:
      # Request rate per second
      - record: api:request_rate:5m
        expr: rate(api_request_total[5m])

      # Error rate
      - record: api:error_rate:5m
        expr: |
          sum(rate(api_request_total{status=~"5.."}[5m]))
          /
          sum(rate(api_request_total[5m]))

      # Latency p99
      - record: api:latency:p99:5m
        expr: |
          histogram_quantile(0.99,
            sum(rate(api_request_duration_seconds_bucket[5m])) by (le)
          )

      # Latency p95
      - record: api:latency:p95:5m
        expr: |
          histogram_quantile(0.95,
            sum(rate(api_request_duration_seconds_bucket[5m])) by (le)
          )

      # Latency p50
      - record: api:latency:p50:5m
        expr: |
          histogram_quantile(0.50,
            sum(rate(api_request_duration_seconds_bucket[5m])) by (le)
          )

      # Availability (non-5xx responses)
      - record: api:availability:5m
        expr: |
          sum(rate(api_request_total{status!~"5.."}[5m]))
          /
          sum(rate(api_request_total[5m]))

  - name: experiment_slo_recording_rules
    interval: 1m
    rules:
      # Experiment success rate
      - record: experiment:success_rate:5m
        expr: |
          sum(rate(experiment_total{status="success"}[5m]))
          /
          sum(rate(experiment_total[5m]))

      # Experiment duration p99
      - record: experiment:duration:p99:5m
        expr: |
          histogram_quantile(0.99,
            sum(rate(experiment_duration_seconds_bucket[5m])) by (le)
          )

      # Active experiment count
      - record: experiment:active:current
        expr: experiment_active_count

  - name: database_recording_rules
    interval: 30s
    rules:
      # Database query rate
      - record: db:query_rate:5m
        expr: sum(rate(db_query_duration_seconds_count[5m])) by (query_type)

      # Database query latency p99
      - record: db:query_latency:p99:5m
        expr: |
          histogram_quantile(0.99,
            sum(rate(db_query_duration_seconds_bucket[5m])) by (le, query_type)
          )

      # Connection pool utilization
      - record: db:connection_pool:utilization
        expr: |
          (db_connection_pool_size - db_connection_pool_idle)
          /
          db_connection_pool_size

  - name: model_inference_recording_rules
    interval: 30s
    rules:
      # Model inference rate
      - record: model:inference_rate:5m
        expr: sum(rate(model_inference_duration_seconds_count[5m])) by (model_name, provider)

      # Model inference latency p99
      - record: model:inference_latency:p99:5m
        expr: |
          histogram_quantile(0.99,
            sum(rate(model_inference_duration_seconds_bucket[5m])) by (le, model_name)
          )

      # Total token usage rate
      - record: model:token_usage_rate:5m
        expr: sum(rate(model_inference_tokens_total[5m])) by (model_name, token_type)

      # Cost per request
      - record: model:cost_per_request:5m
        expr: |
          sum(rate(model_inference_cost_usd_sum[5m])) by (model_name)
          /
          sum(rate(model_inference_cost_usd_count[5m])) by (model_name)

  - name: resource_recording_rules
    interval: 30s
    rules:
      # CPU utilization percentage
      - record: node:cpu_utilization:percent
        expr: |
          100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)

      # Memory utilization percentage
      - record: node:memory_utilization:percent
        expr: |
          100 * (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes))

      # Disk I/O utilization
      - record: node:disk_io_utilization:percent
        expr: |
          rate(node_disk_io_time_seconds_total[5m]) * 100
```

### 5.1.2 Logging Stack (Structured Logs)

#### Log Format Specification

```rust
// src/logging/mod.rs
use tracing::{info, warn, error, debug, trace};
use tracing_subscriber::{
    layer::SubscriberExt,
    util::SubscriberInitExt,
    fmt,
    EnvFilter,
};
use serde::{Serialize, Deserialize};

#[derive(Debug, Serialize, Deserialize)]
pub struct LogContext {
    pub request_id: String,
    pub experiment_id: Option<String>,
    pub user_id: Option<String>,
    pub session_id: Option<String>,
    pub trace_id: Option<String>,
    pub span_id: Option<String>,
}

pub fn init_logging() {
    let fmt_layer = fmt::layer()
        .json()
        .with_target(true)
        .with_level(true)
        .with_thread_ids(true)
        .with_thread_names(true)
        .with_file(true)
        .with_line_number(true);

    let filter_layer = EnvFilter::try_from_default_env()
        .or_else(|_| EnvFilter::try_new("info"))
        .unwrap();

    tracing_subscriber::registry()
        .with(filter_layer)
        .with(fmt_layer)
        .init();
}

// Example structured log entries
pub fn log_experiment_start(ctx: &LogContext, experiment_type: &str, dataset_name: &str) {
    info!(
        request_id = %ctx.request_id,
        experiment_id = ?ctx.experiment_id,
        user_id = ?ctx.user_id,
        experiment_type = experiment_type,
        dataset_name = dataset_name,
        event = "experiment.start",
        "Starting experiment"
    );
}

pub fn log_experiment_complete(
    ctx: &LogContext,
    experiment_type: &str,
    duration_seconds: f64,
    status: &str,
    result_count: usize,
) {
    info!(
        request_id = %ctx.request_id,
        experiment_id = ?ctx.experiment_id,
        user_id = ?ctx.user_id,
        experiment_type = experiment_type,
        duration_seconds = duration_seconds,
        status = status,
        result_count = result_count,
        event = "experiment.complete",
        "Experiment completed"
    );
}

pub fn log_database_error(ctx: &LogContext, query_type: &str, error: &str) {
    error!(
        request_id = %ctx.request_id,
        query_type = query_type,
        error = error,
        event = "database.error",
        "Database query failed"
    );
}

pub fn log_api_request(
    ctx: &LogContext,
    method: &str,
    path: &str,
    status: u16,
    duration_ms: f64,
    user_agent: Option<&str>,
) {
    info!(
        request_id = %ctx.request_id,
        trace_id = ?ctx.trace_id,
        method = method,
        path = path,
        status = status,
        duration_ms = duration_ms,
        user_agent = ?user_agent,
        event = "api.request",
        "API request completed"
    );
}
```

#### Vector Configuration (Log Aggregation)

```toml
# /etc/vector/vector.toml
[sources.kubernetes_logs]
type = "kubernetes_logs"
namespace_annotation_fields.namespace = "namespace"
pod_annotation_fields.pod = "pod_name"

[sources.syslog]
type = "syslog"
address = "0.0.0.0:514"
mode = "tcp"

[transforms.parse_json_logs]
type = "remap"
inputs = ["kubernetes_logs"]
source = '''
  . = parse_json!(.message)
  .kubernetes = .kubernetes
  .timestamp = to_timestamp!(.timestamp)
'''

[transforms.add_metadata]
type = "remap"
inputs = ["parse_json_logs"]
source = '''
  .environment = "production"
  .cluster = "llm-research-lab-prod"
  .service = .kubernetes.pod_labels.app
  .version = .kubernetes.pod_labels.version
'''

[transforms.filter_errors]
type = "filter"
inputs = ["add_metadata"]
condition = '.level == "error" || .level == "warn" || .level == "critical"'

[transforms.sample_debug_logs]
type = "sample"
inputs = ["add_metadata"]
rate = 10  # Keep 1 in 10 debug logs
condition = '.level == "debug"'

[sinks.loki]
type = "loki"
inputs = ["add_metadata", "sample_debug_logs"]
endpoint = "http://loki:3100"
encoding.codec = "json"
labels.environment = "{{ environment }}"
labels.cluster = "{{ cluster }}"
labels.service = "{{ service }}"
labels.level = "{{ level }}"
labels.namespace = "{{ kubernetes.namespace }}"

[sinks.error_logs_s3]
type = "aws_s3"
inputs = ["filter_errors"]
bucket = "llm-research-lab-error-logs"
compression = "gzip"
encoding.codec = "json"
key_prefix = "errors/%Y/%m/%d/"
region = "us-east-1"

[sinks.prometheus_metrics_from_logs]
type = "prometheus_exporter"
inputs = ["add_metadata"]
address = "0.0.0.0:9598"

[[sinks.prometheus_metrics_from_logs.metrics]]
type = "counter"
field = "level"
name = "log_lines_total"
labels.level = "{{ level }}"
labels.service = "{{ service }}"
```

#### Loki Configuration

```yaml
# loki-config.yaml
auth_enabled: false

server:
  http_listen_port: 3100
  grpc_listen_port: 9096

common:
  path_prefix: /loki
  storage:
    filesystem:
      chunks_directory: /loki/chunks
      rules_directory: /loki/rules
  replication_factor: 1
  ring:
    kvstore:
      store: inmemory

schema_config:
  configs:
    - from: 2024-01-01
      store: boltdb-shipper
      object_store: s3
      schema: v11
      index:
        prefix: loki_index_
        period: 24h

storage_config:
  boltdb_shipper:
    active_index_directory: /loki/boltdb-shipper-active
    cache_location: /loki/boltdb-shipper-cache
    cache_ttl: 24h
    shared_store: s3
  aws:
    s3: s3://us-east-1/llm-research-lab-loki
    s3forcepathstyle: true

compactor:
  working_directory: /loki/compactor
  shared_store: s3
  compaction_interval: 10m

limits_config:
  enforce_metric_name: false
  reject_old_samples: true
  reject_old_samples_max_age: 168h
  ingestion_rate_mb: 10
  ingestion_burst_size_mb: 20
  max_query_length: 721h  # 30 days

chunk_store_config:
  max_look_back_period: 720h  # 30 days

table_manager:
  retention_deletes_enabled: true
  retention_period: 720h  # 30 days

query_range:
  align_queries_with_step: true
  cache_results: true
  max_retries: 5
  results_cache:
    cache:
      enable_fifocache: true
      fifocache:
        max_size_bytes: 1GB
        validity: 24h
```

### 5.1.3 Distributed Tracing (OpenTelemetry + Jaeger)

#### OpenTelemetry Configuration

```rust
// src/tracing/mod.rs
use opentelemetry::{
    global,
    sdk::{
        trace::{self, RandomIdGenerator, Sampler},
        Resource,
    },
    KeyValue,
};
use opentelemetry_otlp::WithExportConfig;
use tracing_subscriber::{layer::SubscriberExt, Registry};

pub fn init_tracing(service_name: &str) -> Result<(), Box<dyn std::error::Error>> {
    global::set_text_map_propagator(opentelemetry_jaeger::Propagator::new());

    let tracer = opentelemetry_otlp::new_pipeline()
        .tracing()
        .with_exporter(
            opentelemetry_otlp::new_exporter()
                .tonic()
                .with_endpoint("http://jaeger-collector:4317"),
        )
        .with_trace_config(
            trace::config()
                .with_sampler(Sampler::ParentBased(Box::new(Sampler::TraceIdRatioBased(
                    0.1, // Sample 10% of traces
                ))))
                .with_id_generator(RandomIdGenerator::default())
                .with_resource(Resource::new(vec![
                    KeyValue::new("service.name", service_name.to_string()),
                    KeyValue::new("service.version", env!("CARGO_PKG_VERSION")),
                    KeyValue::new("deployment.environment", "production"),
                ])),
        )
        .install_batch(opentelemetry::runtime::Tokio)?;

    let telemetry = tracing_opentelemetry::layer().with_tracer(tracer);
    let subscriber = Registry::default().with(telemetry);
    tracing::subscriber::set_global_default(subscriber)?;

    Ok(())
}

// Example traced function
#[tracing::instrument(
    name = "execute_experiment",
    skip(ctx, dataset),
    fields(
        experiment_id = %ctx.experiment_id,
        experiment_type = %experiment_type,
        dataset_size = dataset.len(),
    )
)]
pub async fn execute_experiment(
    ctx: &ExperimentContext,
    experiment_type: &str,
    dataset: &Dataset,
) -> Result<ExperimentResult, Error> {
    let span = tracing::span!(tracing::Level::INFO, "load_dataset");
    let _guard = span.enter();

    // Nested span for dataset loading
    let loaded_data = load_dataset_with_tracing(dataset).await?;
    drop(_guard);

    // Nested span for model inference
    let span = tracing::span!(
        tracing::Level::INFO,
        "model_inference",
        model_name = %ctx.model_name
    );
    let _guard = span.enter();
    let results = run_inference(ctx, &loaded_data).await?;
    drop(_guard);

    // Nested span for result aggregation
    let span = tracing::span!(tracing::Level::INFO, "aggregate_results");
    let _guard = span.enter();
    let aggregated = aggregate_results(results).await?;

    Ok(aggregated)
}

#[tracing::instrument(
    name = "http_request",
    skip(req, body),
    fields(
        http.method = %req.method(),
        http.url = %req.uri(),
        http.status_code = tracing::field::Empty,
    )
)]
pub async fn trace_http_request(
    req: axum::extract::Request,
    body: String,
) -> Result<axum::response::Response, Error> {
    let current_span = tracing::Span::current();

    // Execute request
    let response = handle_request(req, body).await?;

    // Record response status in span
    current_span.record("http.status_code", response.status().as_u16());

    Ok(response)
}
```

#### Jaeger Deployment Configuration

```yaml
# jaeger-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jaeger
  namespace: llm-research-lab
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jaeger
  template:
    metadata:
      labels:
        app: jaeger
    spec:
      containers:
      - name: jaeger
        image: jaegertracing/all-in-one:1.50
        env:
        - name: COLLECTOR_ZIPKIN_HOST_PORT
          value: ":9411"
        - name: COLLECTOR_OTLP_ENABLED
          value: "true"
        - name: SPAN_STORAGE_TYPE
          value: "elasticsearch"
        - name: ES_SERVER_URLS
          value: "http://elasticsearch:9200"
        - name: ES_TAGS_AS_FIELDS_ALL
          value: "true"
        - name: QUERY_BASE_PATH
          value: "/jaeger"
        ports:
        - containerPort: 5775
          protocol: UDP
        - containerPort: 6831
          protocol: UDP
        - containerPort: 6832
          protocol: UDP
        - containerPort: 5778
          protocol: TCP
        - containerPort: 16686
          protocol: TCP
        - containerPort: 14268
          protocol: TCP
        - containerPort: 14250
          protocol: TCP
        - containerPort: 9411
          protocol: TCP
        - containerPort: 4317
          protocol: TCP
        - containerPort: 4318
          protocol: TCP
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
---
apiVersion: v1
kind: Service
metadata:
  name: jaeger-collector
  namespace: llm-research-lab
spec:
  selector:
    app: jaeger
  ports:
  - name: grpc-otlp
    port: 4317
    protocol: TCP
    targetPort: 4317
  - name: http-otlp
    port: 4318
    protocol: TCP
    targetPort: 4318
  - name: jaeger-collector
    port: 14268
    protocol: TCP
    targetPort: 14268
  - name: zipkin
    port: 9411
    protocol: TCP
    targetPort: 9411
---
apiVersion: v1
kind: Service
metadata:
  name: jaeger-query
  namespace: llm-research-lab
spec:
  selector:
    app: jaeger
  ports:
  - name: query
    port: 16686
    protocol: TCP
    targetPort: 16686
  type: ClusterIP
```

### 5.1.4 Dashboard Requirements (Grafana)

#### Grafana Data Sources Configuration

```yaml
# grafana-datasources.yaml
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    jsonData:
      timeInterval: 15s
      queryTimeout: 60s
      httpMethod: POST
    editable: false

  - name: Loki
    type: loki
    access: proxy
    url: http://loki:3100
    jsonData:
      maxLines: 1000
      derivedFields:
        - datasourceUid: jaeger
          matcherRegex: "trace_id=(\\w+)"
          name: TraceID
          url: '$${__value.raw}'
    editable: false

  - name: Jaeger
    type: jaeger
    access: proxy
    url: http://jaeger-query:16686
    jsonData:
      tracesToLogs:
        datasourceUid: loki
        tags: ['request_id', 'experiment_id']
        mappedTags: [{ key: 'service.name', value: 'service' }]
        mapTagNamesEnabled: true
        spanStartTimeShift: '-1h'
        spanEndTimeShift: '1h'
    editable: false

  - name: Tempo
    type: tempo
    access: proxy
    url: http://tempo:3200
    jsonData:
      tracesToLogs:
        datasourceUid: loki
        tags: ['request_id']
      serviceMap:
        datasourceUid: prometheus
    editable: false

  - name: PostgreSQL
    type: postgres
    url: postgres:5432
    database: llm_research_lab
    user: grafana_reader
    secureJsonData:
      password: ${GRAFANA_DB_PASSWORD}
    jsonData:
      sslmode: require
      maxOpenConns: 10
      maxIdleConns: 5
      connMaxLifetime: 14400
    editable: false
```

#### Core Dashboard Definitions

See [Appendix B](#appendix-b-dashboard-templates) for complete JSON dashboard definitions.

**Required Dashboards:**

1. **Service Overview Dashboard**
   - Request rate, error rate, latency (RED metrics)
   - Active experiments, experiment success rate
   - Resource utilization (CPU, memory, disk)
   - Database connection pool status

2. **SLO Dashboard**
   - Availability SLO (99.9% target)
   - Latency SLO (p99 < 100ms)
   - Error budget consumption
   - SLO compliance trends

3. **Experiment Execution Dashboard**
   - Active experiments timeline
   - Experiment duration distribution
   - Success/failure breakdown by type
   - Dataset loading performance

4. **Model Inference Dashboard**
   - Inference latency by model/provider
   - Token usage and cost tracking
   - Model provider API errors
   - Cache hit rates

5. **Database Performance Dashboard**
   - Query latency by type
   - Connection pool utilization
   - Slow query log
   - Lock contention metrics

6. **Infrastructure Dashboard**
   - Kubernetes pod status
   - Node resource utilization
   - Persistent volume capacity
   - Network I/O and errors

---

## 5.2 Alerting Configuration

### 5.2.1 Alert Severity Levels

| Priority | Response Time | Scope | Example |
|----------|---------------|-------|---------|
| **P1 - Critical** | Immediate (< 5 min) | Service down, data loss risk | API unavailable, database corruption |
| **P2 - High** | < 15 minutes | Degraded service, SLO violation | p99 latency > 500ms, error rate > 5% |
| **P3 - Medium** | < 1 hour | Potential future impact | Disk at 75%, elevated error rate |
| **P4 - Low** | < 4 hours | Informational, no immediate impact | Certificate expiring in 30 days |

### 5.2.2 Alert Definitions

```yaml
# /etc/prometheus/alerts/slo_alerts.yml
groups:
  - name: slo_availability
    interval: 30s
    rules:
      - alert: AvailabilitySLOViolation
        expr: api:availability:5m < 0.999
        for: 5m
        labels:
          severity: P1
          component: api
          slo: availability
        annotations:
          summary: "Availability SLO violation ({{ $value | humanizePercentage }})"
          description: |
            API availability has dropped below 99.9% SLO.
            Current availability: {{ $value | humanizePercentage }}
            Error rate: {{ query "api:error_rate:5m" | first | value | humanizePercentage }}

            Runbook: https://runbooks.example.com/availability-slo-violation
          dashboard: https://grafana.example.com/d/slo-dashboard

      - alert: AvailabilitySLOWarning
        expr: api:availability:5m < 0.9995
        for: 10m
        labels:
          severity: P2
          component: api
          slo: availability
        annotations:
          summary: "Availability approaching SLO limit ({{ $value | humanizePercentage }})"
          description: |
            API availability is close to the 99.9% SLO threshold.
            Current availability: {{ $value | humanizePercentage }}
            Error budget remaining: {{ query "slo:error_budget:remaining:percent" | first | value }}%

            Runbook: https://runbooks.example.com/availability-slo-warning

  - name: slo_latency
    interval: 30s
    rules:
      - alert: LatencySLOViolation
        expr: api:latency:p99:5m > 0.1
        for: 5m
        labels:
          severity: P1
          component: api
          slo: latency
        annotations:
          summary: "p99 latency SLO violation ({{ $value | humanizeDuration }})"
          description: |
            API p99 latency has exceeded 100ms SLO.
            Current p99 latency: {{ $value | humanizeDuration }}
            p95 latency: {{ query "api:latency:p95:5m" | first | value | humanizeDuration }}
            p50 latency: {{ query "api:latency:p50:5m" | first | value | humanizeDuration }}

            Runbook: https://runbooks.example.com/latency-slo-violation
          dashboard: https://grafana.example.com/d/latency-dashboard

      - alert: LatencySLOWarning
        expr: api:latency:p99:5m > 0.075
        for: 10m
        labels:
          severity: P2
          component: api
          slo: latency
        annotations:
          summary: "p99 latency approaching SLO ({{ $value | humanizeDuration }})"
          description: |
            API p99 latency is approaching the 100ms SLO threshold.
            Current p99 latency: {{ $value | humanizeDuration }}

            Runbook: https://runbooks.example.com/latency-slo-warning

  - name: error_budget
    interval: 1h
    rules:
      - alert: ErrorBudgetExhausted
        expr: slo:error_budget:remaining:percent < 0
        for: 0m
        labels:
          severity: P1
          component: slo
        annotations:
          summary: "Error budget completely exhausted"
          description: |
            The 30-day error budget has been completely consumed.
            All non-critical deployments must be halted.

            Runbook: https://runbooks.example.com/error-budget-exhausted

      - alert: ErrorBudgetCritical
        expr: slo:error_budget:remaining:percent < 10
        for: 0m
        labels:
          severity: P2
          component: slo
        annotations:
          summary: "Error budget critical ({{ $value }}% remaining)"
          description: |
            Less than 10% of error budget remains for this 30-day period.
            Review ongoing incidents and consider deployment freeze.

            Runbook: https://runbooks.example.com/error-budget-critical

      - alert: ErrorBudgetWarning
        expr: slo:error_budget:remaining:percent < 25
        for: 0m
        labels:
          severity: P3
          component: slo
        annotations:
          summary: "Error budget warning ({{ $value }}% remaining)"
          description: |
            Less than 25% of error budget remains.
            Exercise caution with deployments.

  - name: infrastructure_alerts
    interval: 30s
    rules:
      - alert: ServiceDown
        expr: up{job=~"llm-research-lab.*"} == 0
        for: 1m
        labels:
          severity: P1
          component: infrastructure
        annotations:
          summary: "Service {{ $labels.job }} is down"
          description: |
            The {{ $labels.job }} service on {{ $labels.instance }} is not responding.

            Runbook: https://runbooks.example.com/service-down

      - alert: HighCPUUsage
        expr: node:cpu_utilization:percent > 80
        for: 10m
        labels:
          severity: P2
          component: infrastructure
        annotations:
          summary: "High CPU usage on {{ $labels.instance }} ({{ $value }}%)"
          description: |
            CPU utilization has been above 80% for 10 minutes.
            Instance: {{ $labels.instance }}
            Current usage: {{ $value }}%

            Runbook: https://runbooks.example.com/high-cpu-usage

      - alert: HighMemoryUsage
        expr: node:memory_utilization:percent > 85
        for: 10m
        labels:
          severity: P2
          component: infrastructure
        annotations:
          summary: "High memory usage on {{ $labels.instance }} ({{ $value }}%)"
          description: |
            Memory utilization has been above 85% for 10 minutes.
            Instance: {{ $labels.instance }}
            Current usage: {{ $value }}%

            Runbook: https://runbooks.example.com/high-memory-usage

      - alert: DiskSpaceLow
        expr: |
          (node_filesystem_avail_bytes{fstype!~"tmpfs|fuse.lxcfs"}
          / node_filesystem_size_bytes{fstype!~"tmpfs|fuse.lxcfs"}) * 100 < 15
        for: 5m
        labels:
          severity: P2
          component: infrastructure
        annotations:
          summary: "Disk space low on {{ $labels.instance }} ({{ $value }}% free)"
          description: |
            Less than 15% disk space remaining.
            Instance: {{ $labels.instance }}
            Mountpoint: {{ $labels.mountpoint }}

            Runbook: https://runbooks.example.com/disk-space-low

  - name: database_alerts
    interval: 30s
    rules:
      - alert: DatabaseConnectionPoolExhausted
        expr: db:connection_pool:utilization > 0.9
        for: 5m
        labels:
          severity: P1
          component: database
        annotations:
          summary: "Database connection pool nearly exhausted ({{ $value | humanizePercentage }})"
          description: |
            Database connection pool utilization is above 90%.
            This may cause request timeouts and service degradation.

            Runbook: https://runbooks.example.com/db-connection-pool-exhausted

      - alert: SlowDatabaseQueries
        expr: db:query_latency:p99:5m{query_type="select"} > 1.0
        for: 10m
        labels:
          severity: P2
          component: database
        annotations:
          summary: "Slow database queries detected (p99: {{ $value | humanizeDuration }})"
          description: |
            Database query latency p99 is above 1 second.
            Query type: {{ $labels.query_type }}

            Runbook: https://runbooks.example.com/slow-database-queries

      - alert: DatabaseReplicationLag
        expr: pg_replication_lag_seconds > 30
        for: 5m
        labels:
          severity: P2
          component: database
        annotations:
          summary: "Database replication lag ({{ $value }}s)"
          description: |
            Replication lag has exceeded 30 seconds.
            This may impact read replica consistency.

            Runbook: https://runbooks.example.com/db-replication-lag

  - name: experiment_alerts
    interval: 1m
    rules:
      - alert: HighExperimentFailureRate
        expr: (1 - experiment:success_rate:5m) > 0.1
        for: 10m
        labels:
          severity: P2
          component: experiment-runner
        annotations:
          summary: "High experiment failure rate ({{ $value | humanizePercentage }})"
          description: |
            More than 10% of experiments are failing.
            This may indicate issues with model APIs, datasets, or infrastructure.

            Runbook: https://runbooks.example.com/high-experiment-failure-rate

      - alert: ExperimentQueueBacklog
        expr: experiment_queue_depth > 100
        for: 15m
        labels:
          severity: P3
          component: experiment-runner
        annotations:
          summary: "Large experiment queue backlog ({{ $value }} experiments)"
          description: |
            More than 100 experiments are queued.
            Consider scaling up experiment runner workers.

            Runbook: https://runbooks.example.com/experiment-queue-backlog

  - name: model_inference_alerts
    interval: 30s
    rules:
      - alert: ModelAPIHighLatency
        expr: model:inference_latency:p99:5m > 10.0
        for: 5m
        labels:
          severity: P2
          component: model-api
        annotations:
          summary: "Model API high latency ({{ $labels.model_name }}: {{ $value | humanizeDuration }})"
          description: |
            Model inference latency p99 is above 10 seconds.
            Model: {{ $labels.model_name }}
            Provider: {{ $labels.provider }}

            Runbook: https://runbooks.example.com/model-api-high-latency

      - alert: ModelAPIErrors
        expr: |
          sum(rate(model_inference_errors_total[5m])) by (model_name, provider)
          /
          sum(rate(model_inference_duration_seconds_count[5m])) by (model_name, provider)
          > 0.05
        for: 5m
        labels:
          severity: P2
          component: model-api
        annotations:
          summary: "High model API error rate ({{ $labels.model_name }}: {{ $value | humanizePercentage }})"
          description: |
            More than 5% of model API requests are failing.
            Model: {{ $labels.model_name }}
            Provider: {{ $labels.provider }}

            Runbook: https://runbooks.example.com/model-api-errors

  - name: security_alerts
    interval: 1m
    rules:
      - alert: UnauthorizedAccessAttempt
        expr: increase(api_request_total{status="401"}[5m]) > 50
        for: 0m
        labels:
          severity: P2
          component: security
        annotations:
          summary: "Multiple unauthorized access attempts detected"
          description: |
            More than 50 unauthorized (401) requests in 5 minutes.
            This may indicate a brute force attack.

            Runbook: https://runbooks.example.com/unauthorized-access-attempts

      - alert: SuspiciousDataAccess
        expr: increase(dataset_access_denied_total[10m]) > 10
        for: 0m
        labels:
          severity: P2
          component: security
        annotations:
          summary: "Multiple data access denials detected"
          description: |
            More than 10 dataset access denials in 10 minutes.
            Review access logs for potential security breach.

            Runbook: https://runbooks.example.com/suspicious-data-access
```

### 5.2.3 Alert Routing and Escalation

```yaml
# alertmanager.yml
global:
  resolve_timeout: 5m
  slack_api_url: ${SLACK_WEBHOOK_URL}
  pagerduty_url: https://events.pagerduty.com/v2/enqueue

route:
  receiver: 'default-receiver'
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 5m
  repeat_interval: 4h

  routes:
    # P1 Critical - Immediate PagerDuty + Slack
    - match:
        severity: P1
      receiver: 'pagerduty-critical'
      group_wait: 0s
      group_interval: 1m
      repeat_interval: 5m
      continue: true

    - match:
        severity: P1
      receiver: 'slack-critical'
      group_wait: 0s
      group_interval: 1m
      repeat_interval: 30m

    # P2 High - PagerDuty during business hours, Slack always
    - match:
        severity: P2
      receiver: 'pagerduty-high'
      group_wait: 5s
      group_interval: 5m
      repeat_interval: 30m
      continue: true
      time_intervals:
        - business_hours

    - match:
        severity: P2
      receiver: 'slack-high'
      group_wait: 5s
      group_interval: 5m
      repeat_interval: 1h

    # P3 Medium - Slack only
    - match:
        severity: P3
      receiver: 'slack-medium'
      group_wait: 30s
      group_interval: 10m
      repeat_interval: 4h

    # P4 Low - Email digest
    - match:
        severity: P4
      receiver: 'email-low'
      group_wait: 5m
      group_interval: 1h
      repeat_interval: 24h

    # Security alerts - dedicated channel
    - match:
        component: security
      receiver: 'security-team'
      group_wait: 0s
      repeat_interval: 15m
      continue: true

# Time intervals
time_intervals:
  - name: business_hours
    time_intervals:
      - times:
          - start_time: '09:00'
            end_time: '17:00'
        weekdays: ['monday:friday']
        location: 'America/New_York'

# Receivers
receivers:
  - name: 'default-receiver'
    slack_configs:
      - channel: '#llm-research-lab-alerts'
        title: 'Default Alert'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

  - name: 'pagerduty-critical'
    pagerduty_configs:
      - service_key: ${PAGERDUTY_SERVICE_KEY_CRITICAL}
        severity: critical
        description: '{{ .CommonAnnotations.summary }}'
        details:
          alert_count: '{{ len .Alerts }}'
          firing_alerts: '{{ range .Alerts.Firing }}{{ .Labels.alertname }} {{ end }}'
          runbook: '{{ .CommonAnnotations.runbook }}'
          dashboard: '{{ .CommonAnnotations.dashboard }}'
        client: 'Prometheus Alertmanager'
        client_url: 'https://alertmanager.example.com'

  - name: 'pagerduty-high'
    pagerduty_configs:
      - service_key: ${PAGERDUTY_SERVICE_KEY_HIGH}
        severity: error
        description: '{{ .CommonAnnotations.summary }}'

  - name: 'slack-critical'
    slack_configs:
      - channel: '#llm-research-lab-critical'
        title: ':rotating_light: P1 CRITICAL ALERT'
        text: |
          *Summary:* {{ .CommonAnnotations.summary }}
          *Description:* {{ .CommonAnnotations.description }}
          *Runbook:* {{ .CommonAnnotations.runbook }}
          *Dashboard:* {{ .CommonAnnotations.dashboard }}
          *Firing Alerts:* {{ len .Alerts.Firing }}
        color: danger
        send_resolved: true

  - name: 'slack-high'
    slack_configs:
      - channel: '#llm-research-lab-alerts'
        title: ':warning: P2 High Priority Alert'
        text: '{{ .CommonAnnotations.description }}'
        color: warning
        send_resolved: true

  - name: 'slack-medium'
    slack_configs:
      - channel: '#llm-research-lab-alerts'
        title: ':information_source: P3 Medium Priority Alert'
        text: '{{ .CommonAnnotations.summary }}'
        color: good
        send_resolved: true

  - name: 'email-low'
    email_configs:
      - to: 'llm-research-lab-team@example.com'
        from: 'alertmanager@example.com'
        smarthost: 'smtp.example.com:587'
        auth_username: 'alertmanager@example.com'
        auth_password: ${SMTP_PASSWORD}
        headers:
          Subject: 'LLM Research Lab - P4 Alert Digest'

  - name: 'security-team'
    slack_configs:
      - channel: '#security-alerts'
        title: ':lock: Security Alert'
        text: '{{ .CommonAnnotations.description }}'
        color: danger
    email_configs:
      - to: 'security-team@example.com'
        from: 'alertmanager@example.com'
        smarthost: 'smtp.example.com:587'

# Inhibition rules
inhibit_rules:
  # Inhibit warning alerts if critical alert is firing
  - source_match:
      severity: P1
    target_match:
      severity: P2
    equal: ['alertname', 'cluster', 'service']

  - source_match:
      severity: P1
    target_match:
      severity: P3
    equal: ['alertname', 'cluster', 'service']

  # Inhibit individual component alerts if service is down
  - source_match:
      alertname: ServiceDown
    target_match_re:
      alertname: '(HighCPU|HighMemory|SlowQueries).*'
    equal: ['instance']
```

### 5.2.4 Runbook Structure

Each alert must link to a runbook with the following structure:

```markdown
# Runbook: [Alert Name]

## Alert Details
- **Severity**: P1/P2/P3/P4
- **Component**: [Component Name]
- **SLO Impact**: Yes/No
- **Typical Duration**: [Expected time to resolve]

## Symptoms
- What users/systems are experiencing
- Observable symptoms without logging into systems

## Diagnosis
1. Check [Dashboard Link]
2. Run diagnostic queries:
   ```promql
   [Prometheus queries for diagnosis]
   ```
3. Check logs:
   ```
   kubectl logs -n llm-research-lab [pod-name]
   ```

## Mitigation Steps
1. **Immediate actions** (< 5 min)
   - Step 1
   - Step 2
2. **Short-term fixes** (< 1 hour)
   - Step 1
   - Step 2
3. **Long-term resolution**
   - Root cause analysis
   - Permanent fixes

## Escalation
- **Primary On-Call**: [PagerDuty escalation policy]
- **Secondary**: [Team lead contact]
- **Manager Escalation Threshold**: [Condition requiring manager involvement]

## Related Information
- Related alerts
- Dependencies
- Recent changes
- Known issues
```

### 5.2.5 Alert Fatigue Prevention

**Strategies:**

1. **Alert Tuning**
   - Review alert effectiveness monthly
   - Disable or adjust alerts with > 50% false positive rate
   - Require "for" duration on all alerts to avoid flapping

2. **Alert Grouping**
   - Group related alerts (e.g., all database alerts during database outage)
   - Use inhibition rules to suppress redundant alerts

3. **Actionability Requirement**
   - Every alert must have a clear action item
   - Remove "informational" alerts from PagerDuty

4. **Alert Review Process**
   - Quarterly alert effectiveness review
   - Track mean time to acknowledge (MTTA) and mean time to resolve (MTTR)
   - Disable alerts with MTTA > 30 minutes (indicates unclear action)

5. **Graduated Response**
   - Use multiple severity levels
   - Escalate from warning → critical only if condition worsens

---

## 5.3 SLO/SLI Definition

### 5.3.1 Service Level Indicators (SLIs)

| SLI | Measurement | Target | Data Source |
|-----|-------------|--------|-------------|
| **Availability** | Ratio of successful requests (non-5xx) to total requests | 99.9% | `api_request_total` metric |
| **Latency (p99)** | 99th percentile request latency | < 100ms | `api_request_duration_seconds` histogram |
| **Latency (p95)** | 95th percentile request latency | < 50ms | `api_request_duration_seconds` histogram |
| **Latency (p50)** | 50th percentile request latency | < 25ms | `api_request_duration_seconds` histogram |
| **Experiment Success Rate** | Ratio of successful experiments to total | > 95% | `experiment_total` metric |
| **Data Freshness** | Age of most recent dataset update | < 24 hours | Custom metric |

### 5.3.2 Service Level Objectives (SLOs)

#### Availability SLO

**Target**: 99.9% availability over 30-day rolling window

**Error Budget**: 0.1% = 43.2 minutes of downtime per 30 days

```yaml
# Recording rule for availability SLO
- record: slo:availability:30d
  expr: |
    sum(rate(api_request_total{status!~"5.."}[30d]))
    /
    sum(rate(api_request_total[30d]))

- record: slo:availability:error_budget:consumed
  expr: 1 - slo:availability:30d

- record: slo:availability:error_budget:remaining:percent
  expr: |
    (0.001 - (1 - slo:availability:30d)) / 0.001 * 100
```

#### Latency SLO

**Target**: 99% of requests complete in < 100ms

**Error Budget**: 1% of requests may exceed 100ms

```yaml
# Recording rule for latency SLO
- record: slo:latency:requests_under_100ms:30d
  expr: |
    sum(rate(api_request_duration_seconds_bucket{le="0.1"}[30d]))
    /
    sum(rate(api_request_duration_seconds_count[30d]))

- record: slo:latency:error_budget:consumed
  expr: 1 - slo:latency:requests_under_100ms:30d

- record: slo:latency:error_budget:remaining:percent
  expr: |
    (0.01 - (1 - slo:latency:requests_under_100ms:30d)) / 0.01 * 100
```

### 5.3.3 Error Budget Tracking

```rust
// src/slo/error_budget.rs
use chrono::{DateTime, Duration, Utc};

#[derive(Debug, Clone)]
pub struct ErrorBudget {
    pub window_duration: Duration,
    pub target_availability: f64,
    pub start_time: DateTime<Utc>,
    pub end_time: DateTime<Utc>,
}

impl ErrorBudget {
    pub fn new_30_day() -> Self {
        let now = Utc::now();
        Self {
            window_duration: Duration::days(30),
            target_availability: 0.999,
            start_time: now - Duration::days(30),
            end_time: now,
        }
    }

    pub fn allowed_downtime_seconds(&self) -> f64 {
        let total_seconds = self.window_duration.num_seconds() as f64;
        total_seconds * (1.0 - self.target_availability)
    }

    pub fn consumed_downtime_seconds(&self, actual_availability: f64) -> f64 {
        let total_seconds = self.window_duration.num_seconds() as f64;
        total_seconds * (1.0 - actual_availability)
    }

    pub fn remaining_downtime_seconds(&self, actual_availability: f64) -> f64 {
        self.allowed_downtime_seconds() - self.consumed_downtime_seconds(actual_availability)
    }

    pub fn remaining_percent(&self, actual_availability: f64) -> f64 {
        let remaining = self.remaining_downtime_seconds(actual_availability);
        let allowed = self.allowed_downtime_seconds();
        (remaining / allowed) * 100.0
    }

    pub fn burn_rate(&self, actual_availability: f64, time_window: Duration) -> f64 {
        let consumed = self.consumed_downtime_seconds(actual_availability);
        let time_elapsed = (self.end_time - self.start_time).num_seconds() as f64;
        let window_seconds = time_window.num_seconds() as f64;

        (consumed / time_elapsed) * (self.window_duration.num_seconds() as f64 / window_seconds)
    }

    pub fn is_exhausted(&self, actual_availability: f64) -> bool {
        self.remaining_downtime_seconds(actual_availability) <= 0.0
    }

    pub fn is_critical(&self, actual_availability: f64) -> bool {
        self.remaining_percent(actual_availability) < 10.0
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_error_budget_calculations() {
        let budget = ErrorBudget::new_30_day();

        // 30 days = 2,592,000 seconds
        // 0.1% = 2,592 seconds = 43.2 minutes
        assert_eq!(budget.allowed_downtime_seconds(), 2592.0);

        // If availability is 99.95%, we've used half the budget
        let consumed = budget.consumed_downtime_seconds(0.9995);
        assert_eq!(consumed, 1296.0);

        // Remaining should be 50%
        let remaining_pct = budget.remaining_percent(0.9995);
        assert!((remaining_pct - 50.0).abs() < 0.1);
    }

    #[test]
    fn test_error_budget_exhaustion() {
        let budget = ErrorBudget::new_30_day();

        // If availability is below 99.9%, budget is exhausted
        assert!(budget.is_exhausted(0.998));
        assert!(!budget.is_exhausted(0.9995));
    }
}
```

### 5.3.4 SLO Dashboard Requirements

**Dashboard Panels Required:**

1. **Current SLO Compliance**
   - Gauge showing current 30-day availability
   - Threshold markers at 99.9% (SLO) and 99.95% (warning)
   - Color coding: green (> 99.95%), yellow (99.9-99.95%), red (< 99.9%)

2. **Error Budget Burn Rate**
   - Time series showing daily error budget consumption
   - Projection line showing estimated time to budget exhaustion
   - Alerts when burn rate exceeds 2x expected

3. **Error Budget Remaining**
   - Bar chart showing percentage of error budget remaining
   - Historical comparison to previous 30-day windows
   - Annotations for incidents and deployments

4. **Latency Distribution**
   - Heatmap of request latency over time
   - Percentile lines (p50, p95, p99)
   - SLO threshold marker at 100ms

5. **SLO Violation Events**
   - Table of SLO violation incidents
   - Duration, impact, root cause
   - Links to incident reports

---

## 5.4 On-Call Procedures

### 5.4.1 On-Call Rotation Setup

**Rotation Schedule:**
- **Primary On-Call**: 7-day rotation, Monday-Monday handoff
- **Secondary On-Call**: Shadows primary, escalation path
- **Manager Escalation**: Available for P1 incidents > 2 hours

**Schedule Management:**
- Use PagerDuty for rotation management
- Minimum 2 weeks notice for rotation assignments
- Swap requests must be confirmed 48 hours in advance
- Time zone considerations for global team (follow-the-sun)

**On-Call Responsibilities:**
1. Respond to P1 alerts within 5 minutes
2. Respond to P2 alerts within 15 minutes
3. Acknowledge all alerts in PagerDuty
4. Update incident status in Slack
5. Create incident reports for P1/P2 incidents
6. Participate in post-incident reviews

### 5.4.2 Escalation Matrix

| Severity | Initial Response | Escalation Time | Escalation Path |
|----------|------------------|-----------------|-----------------|
| **P1** | Primary On-Call | 15 minutes | Secondary → Manager → Director |
| **P2** | Primary On-Call | 30 minutes | Secondary → Manager |
| **P3** | Primary On-Call | 2 hours | Secondary |
| **P4** | Business hours | 8 hours | Team lead |

**Escalation Contacts:**

```yaml
# escalation-policy.yaml
escalation_policies:
  - name: llm-research-lab-primary
    num_loops: 3
    on_call_handoff_notifications: restrict
    escalation_rules:
      - escalation_delay_in_minutes: 0
        targets:
          - type: schedule_reference
            id: primary-on-call-schedule

      - escalation_delay_in_minutes: 15
        targets:
          - type: schedule_reference
            id: secondary-on-call-schedule

      - escalation_delay_in_minutes: 30
        targets:
          - type: user_reference
            id: engineering-manager

      - escalation_delay_in_minutes: 60
        targets:
          - type: user_reference
            id: director-of-engineering
```

### 5.4.3 Incident Response Playbook

#### Phase 1: Detection and Triage (0-5 minutes)

1. **Acknowledge Alert**
   - Click acknowledge in PagerDuty to stop escalation
   - Post initial acknowledgment in #llm-research-lab-incidents Slack channel

2. **Initial Assessment**
   - Check alert dashboard for context
   - Determine severity (confirm P1/P2 classification)
   - Identify affected components and user impact

3. **Incident Declaration**
   - For P1: Declare major incident, start incident bridge
   - For P2: Create incident thread in Slack
   - Assign incident commander role (usually primary on-call)

#### Phase 2: Investigation (5-30 minutes)

1. **Gather Context**
   ```bash
   # Check service health
   kubectl get pods -n llm-research-lab

   # Check recent deployments
   kubectl rollout history deployment/llm-research-lab-api -n llm-research-lab

   # Check logs for errors
   kubectl logs -n llm-research-lab deployment/llm-research-lab-api --tail=100

   # Check Grafana dashboards
   # Link: https://grafana.example.com/d/slo-dashboard
   ```

2. **Identify Root Cause**
   - Review recent changes (deployments, config updates)
   - Check dependency health (database, model APIs, external services)
   - Correlate metrics, logs, and traces

3. **Document Findings**
   - Update incident thread with findings
   - Create incident document (Google Doc/Confluence)
   - Include timeline of events

#### Phase 3: Mitigation (30 minutes - 2 hours)

1. **Implement Fix**
   - Apply hotfix, rollback, or scaling adjustment
   - For rollback:
     ```bash
     kubectl rollout undo deployment/llm-research-lab-api -n llm-research-lab
     ```
   - For scaling:
     ```bash
     kubectl scale deployment/llm-research-lab-api --replicas=10 -n llm-research-lab
     ```

2. **Verify Resolution**
   - Monitor SLO metrics for recovery
   - Check error rates return to baseline
   - Verify user-facing functionality

3. **Communicate Status**
   - Update incident thread with resolution
   - Post status to #llm-research-lab channel
   - Update status page if customer-facing

#### Phase 4: Recovery and Validation (2-4 hours)

1. **Monitor for Regression**
   - Watch dashboards for 30-60 minutes post-fix
   - Ensure no new alerts triggered
   - Verify SLO compliance

2. **Close Incident**
   - Resolve PagerDuty incident
   - Update incident document with resolution
   - Thank participants in Slack

3. **Schedule Post-Incident Review**
   - For P1: Within 24 hours
   - For P2: Within 48 hours
   - Invite all participants and stakeholders

### 5.4.4 Communication Templates

#### Incident Declaration (P1)

```
:rotating_light: **INCIDENT DECLARED - P1 CRITICAL** :rotating_light:

**Incident**: [Brief description]
**Severity**: P1 Critical
**Impact**: [User-facing impact]
**Incident Commander**: @[primary-on-call]
**Status**: Investigating

**Incident Doc**: [Link]
**Dashboard**: [Link]
**Bridge**: [Conference link]

Updates every 15 minutes or when status changes.
```

#### Status Update

```
**INCIDENT UPDATE** - [Timestamp]

**Status**: [Investigating | Identified | Monitoring | Resolved]
**Summary**: [What we know so far]
**Current Actions**: [What we're doing]
**Next Update**: [Time]

**Incident Doc**: [Link]
```

#### Resolution

```
:white_check_mark: **INCIDENT RESOLVED** :white_check_mark:

**Incident**: [Brief description]
**Duration**: [Start time - End time]
**Root Cause**: [Brief summary]
**Resolution**: [What fixed it]

**Impact Summary**:
- Error budget consumed: X%
- Requests affected: Y
- Users impacted: Z

**Post-Incident Review**: [Scheduled time]
**Incident Report**: [Link when available]

Thanks to @[participants] for quick response!
```

### 5.4.5 Post-Incident Review Process

#### Review Agenda (60 minutes)

1. **Timeline Review** (10 min)
   - Incident timeline from detection to resolution
   - Key decision points and actions taken

2. **Root Cause Analysis** (15 min)
   - What happened and why
   - Contributing factors
   - Five Whys technique

3. **Response Effectiveness** (15 min)
   - What went well
   - What could be improved
   - Runbook effectiveness

4. **Action Items** (15 min)
   - Preventive measures
   - Detection improvements
   - Response process improvements
   - Assign owners and due dates

5. **Lessons Learned** (5 min)
   - Key takeaways
   - Documentation updates needed

#### Post-Incident Report Template

```markdown
# Post-Incident Report: [Incident Name]

## Incident Summary
- **Date**: [Date]
- **Duration**: [Start - End]
- **Severity**: P1/P2
- **Incident Commander**: [Name]
- **Participants**: [Names]

## Impact
- **Users Affected**: [Count]
- **Error Budget Consumed**: [Percentage]
- **Revenue Impact**: [If applicable]
- **SLO Violation**: Yes/No

## Timeline
| Time | Event |
|------|-------|
| HH:MM | Alert triggered |
| HH:MM | Incident acknowledged |
| HH:MM | Root cause identified |
| HH:MM | Mitigation applied |
| HH:MM | Service recovered |
| HH:MM | Incident resolved |

## Root Cause
[Detailed explanation of what caused the incident]

### Five Whys Analysis
1. Why did [symptom] occur? [Answer]
2. Why did [answer1] happen? [Answer]
3. Why did [answer2] happen? [Answer]
4. Why did [answer3] happen? [Answer]
5. Why did [answer4] happen? [Root cause]

## Resolution
[What was done to resolve the incident]

## What Went Well
- [Item]
- [Item]

## What Could Be Improved
- [Item]
- [Item]

## Action Items
| Action | Owner | Due Date | Status |
|--------|-------|----------|--------|
| [Preventive measure] | [Name] | [Date] | Open |
| [Detection improvement] | [Name] | [Date] | Open |
| [Process improvement] | [Name] | [Date] | Open |

## Lessons Learned
- [Lesson]
- [Lesson]

---
**Report Author**: [Name]
**Review Date**: [Date]
**Attendees**: [Names]
```

---

## 5.5 Capacity Management

### 5.5.1 Resource Utilization Thresholds

| Resource | Warning Threshold | Critical Threshold | Action |
|----------|-------------------|-------------------|--------|
| **CPU** | 70% sustained | 85% sustained | Scale horizontally |
| **Memory** | 75% | 90% | Scale vertically or horizontally |
| **Disk** | 75% full | 85% full | Provision additional storage |
| **Network** | 70% bandwidth | 85% bandwidth | Upgrade network tier |
| **Database Connections** | 80% pool | 90% pool | Increase pool size |
| **Queue Depth** | 100 items | 500 items | Scale workers |

**Monitoring Configuration:**

```yaml
# Capacity threshold alerts
- alert: CPUCapacityWarning
  expr: node:cpu_utilization:percent > 70
  for: 30m
  labels:
    severity: P3
    component: capacity
  annotations:
    summary: "CPU capacity warning ({{ $value }}%)"
    description: |
      CPU utilization has been above 70% for 30 minutes.
      Consider scaling if trend continues.

      Runbook: https://runbooks.example.com/cpu-capacity

- alert: MemoryCapacityWarning
  expr: node:memory_utilization:percent > 75
  for: 30m
  labels:
    severity: P3
    component: capacity
  annotations:
    summary: "Memory capacity warning ({{ $value }}%)"
    description: |
      Memory utilization has been above 75% for 30 minutes.
      Consider scaling if trend continues.

- alert: DiskCapacityCritical
  expr: |
    (node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100 < 15
  for: 10m
  labels:
    severity: P2
    component: capacity
  annotations:
    summary: "Disk capacity critical ({{ $value }}% free)"
    description: |
      Less than 15% disk space remaining.
      Immediate action required to prevent service disruption.
```

### 5.5.2 Scaling Triggers

#### Horizontal Pod Autoscaling (HPA)

```yaml
# hpa-api.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: llm-research-lab-api
  namespace: llm-research-lab
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: llm-research-lab-api
  minReplicas: 3
  maxReplicas: 20
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 75
    - type: Pods
      pods:
        metric:
          name: api_request_in_flight
        target:
          type: AverageValue
          averageValue: "100"
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        - type: Percent
          value: 50
          periodSeconds: 60
        - type: Pods
          value: 2
          periodSeconds: 60
      selectPolicy: Max
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          value: 10
          periodSeconds: 60
        - type: Pods
          value: 1
          periodSeconds: 60
      selectPolicy: Min
```

#### Cluster Autoscaling

```yaml
# cluster-autoscaler-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-autoscaler-config
  namespace: kube-system
data:
  autoscaler-config: |
    {
      "scaleDownUtilizationThreshold": 0.5,
      "scaleDownGpuUtilizationThreshold": 0.5,
      "scaleDownUnneededTime": "10m",
      "scaleDownUnreadyTime": "20m",
      "maxNodeProvisionTime": "15m",
      "maxTotalUnreadyPercentage": 45,
      "okTotalUnreadyCount": 3
    }
```

### 5.5.3 Capacity Forecasting

```python
# capacity_forecast.py
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from datetime import datetime, timedelta

class CapacityForecaster:
    def __init__(self, prometheus_url):
        self.prom_url = prometheus_url
        self.models = {}

    def fetch_metric_history(self, metric_name, days=90):
        """Fetch historical metric data from Prometheus"""
        query = f"{metric_name}[{days}d]"
        # Implementation using Prometheus HTTP API
        pass

    def train_forecast_model(self, metric_data):
        """Train linear regression model on historical data"""
        df = pd.DataFrame(metric_data)
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        df['days_since_start'] = (df['timestamp'] - df['timestamp'].min()).dt.days

        X = df[['days_since_start']].values
        y = df['value'].values

        model = LinearRegression()
        model.fit(X, y)

        return model

    def forecast(self, metric_name, days_ahead=30):
        """Forecast metric value for N days ahead"""
        historical_data = self.fetch_metric_history(metric_name)
        model = self.train_forecast_model(historical_data)

        last_day = historical_data['days_since_start'].max()
        future_days = np.array([[last_day + i] for i in range(1, days_ahead + 1)])

        forecast_values = model.predict(future_days)

        return {
            'metric': metric_name,
            'forecast': forecast_values,
            'confidence_interval': self.calculate_confidence_interval(model, future_days),
            'threshold_breach_date': self.find_threshold_breach(forecast_values, threshold=0.85)
        }

    def calculate_confidence_interval(self, model, X, confidence=0.95):
        """Calculate confidence intervals for forecast"""
        # Implementation using prediction intervals
        pass

    def find_threshold_breach(self, forecast, threshold):
        """Find date when forecast exceeds threshold"""
        breach_index = np.where(forecast > threshold)[0]
        if len(breach_index) > 0:
            return breach_index[0]
        return None

# Usage
forecaster = CapacityForecaster("http://prometheus:9090")
cpu_forecast = forecaster.forecast("node:cpu_utilization:percent", days_ahead=60)
memory_forecast = forecaster.forecast("node:memory_utilization:percent", days_ahead=60)

# Generate capacity report
if cpu_forecast['threshold_breach_date']:
    print(f"WARNING: CPU capacity expected to breach 85% in {cpu_forecast['threshold_breach_date']} days")
```

### 5.5.4 Cost Monitoring and Alerting

```yaml
# cost-monitoring.yml
groups:
  - name: cost_alerts
    interval: 1h
    rules:
      - alert: DailySpendExceeded
        expr: sum(increase(model_inference_cost_usd_sum[24h])) > 1000
        for: 0m
        labels:
          severity: P2
          component: cost
        annotations:
          summary: "Daily model inference cost exceeded $1000"
          description: |
            Model inference costs have exceeded $1000 in the last 24 hours.
            Current spend: ${{ $value | humanize }}

            Breakdown by provider:
            {{ range query "sum(increase(model_inference_cost_usd_sum[24h])) by (provider)" }}
              {{ .Labels.provider }}: ${{ .Value | humanize }}
            {{ end }}

      - alert: UnexpectedCostIncrease
        expr: |
          sum(rate(model_inference_cost_usd_sum[1h]))
          >
          sum(rate(model_inference_cost_usd_sum[1h] offset 24h)) * 1.5
        for: 2h
        labels:
          severity: P3
          component: cost
        annotations:
          summary: "Unexpected 50% cost increase detected"
          description: |
            Model inference costs are 50% higher than same time yesterday.
            This may indicate inefficient usage or pricing changes.

      - alert: MonthlyBudgetProjection
        expr: |
          (sum(increase(model_inference_cost_usd_sum[30d])) / 30) * 30 > 25000
        for: 0m
        labels:
          severity: P3
          component: cost
        annotations:
          summary: "Monthly budget projection exceeds $25,000"
          description: |
            Current spending trajectory projects monthly cost above budget.
            Projected monthly cost: ${{ $value | humanize }}
```

**Cost Optimization Strategies:**

1. **Caching**: Reduce redundant model inferences
   - Target: 40% cache hit rate for repeated queries
   - Monitor: `cache_hit_total / (cache_hit_total + cache_miss_total)`

2. **Model Selection**: Route to cost-effective models
   - Use cheaper models for simple tasks
   - Reserve expensive models for complex evaluations

3. **Batch Processing**: Group inferences to reduce API overhead
   - Batch size: 10-50 inferences per API call
   - Trade-off: Slight latency increase for cost reduction

4. **Resource Right-Sizing**: Optimize pod resource requests
   ```bash
   # Analyze actual usage vs requests
   kubectl top pods -n llm-research-lab

   # Adjust resource requests based on actual usage + 20% headroom
   ```

---

## 5.6 Security Operations

### 5.6.1 Security Event Monitoring

```yaml
# security-monitoring.yml
groups:
  - name: security_events
    interval: 1m
    rules:
      - alert: MultipleFailedLogins
        expr: increase(authentication_failures_total[5m]) > 10
        for: 0m
        labels:
          severity: P2
          component: security
          category: authentication
        annotations:
          summary: "Multiple failed login attempts detected"
          description: |
            More than 10 failed authentication attempts in 5 minutes.
            User: {{ $labels.user }}
            IP: {{ $labels.source_ip }}

            This may indicate a brute force attack.

            Runbook: https://runbooks.example.com/failed-logins

      - alert: PrivilegeEscalationAttempt
        expr: increase(rbac_authorization_failures_total[10m]) > 5
        for: 0m
        labels:
          severity: P1
          component: security
          category: authorization
        annotations:
          summary: "Potential privilege escalation attempt"
          description: |
            Multiple authorization failures detected.
            User: {{ $labels.user }}
            Resource: {{ $labels.resource }}

            Immediate investigation required.

      - alert: UnusualDataAccess
        expr: |
          sum(rate(dataset_access_total[5m])) by (user)
          >
          sum(rate(dataset_access_total[5m] offset 24h)) by (user) * 3
        for: 10m
        labels:
          severity: P2
          component: security
          category: data_access
        annotations:
          summary: "Unusual data access pattern detected"
          description: |
            User {{ $labels.user }} is accessing data at 3x normal rate.
            This may indicate data exfiltration.

      - alert: SuspiciousAPIActivity
        expr: |
          sum(rate(api_request_total{user_agent=~".*bot.*|.*crawler.*|.*scanner.*"}[5m]))
          > 10
        for: 5m
        labels:
          severity: P3
          component: security
          category: api_abuse
        annotations:
          summary: "Suspicious API activity from automated tools"
          description: |
            High volume of requests from bot-like user agents.
            User agent: {{ $labels.user_agent }}
            IP: {{ $labels.source_ip }}
```

### 5.6.2 Intrusion Detection

**Falco Configuration** (Kubernetes Runtime Security)

```yaml
# falco-rules.yaml
- rule: Unauthorized Process in Container
  desc: Detect execution of unauthorized processes in LLM Research Lab containers
  condition: >
    container.name startswith "llm-research-lab" and
    proc.name not in (allowed_processes) and
    not proc.pname in (allowed_processes)
  output: >
    Unauthorized process started in container
    (user=%user.name command=%proc.cmdline container=%container.name image=%container.image.repository)
  priority: WARNING
  tags: [container, process, security]

- rule: Sensitive File Access
  desc: Detect access to sensitive files containing credentials or secrets
  condition: >
    container.name startswith "llm-research-lab" and
    (fd.name glob "/etc/secrets/*" or
     fd.name glob "/app/.env" or
     fd.name glob "/root/.ssh/*") and
    evt.type in (open, openat)
  output: >
    Sensitive file accessed
    (user=%user.name file=%fd.name container=%container.name command=%proc.cmdline)
  priority: CRITICAL
  tags: [file, secrets, security]

- rule: Unexpected Network Connection
  desc: Detect outbound connections to unexpected destinations
  condition: >
    container.name startswith "llm-research-lab" and
    evt.type=connect and
    fd.sip not in (allowed_egress_ips) and
    fd.sport != 443 and fd.sport != 80
  output: >
    Unexpected network connection
    (user=%user.name destination=%fd.sip:%fd.sport container=%container.name command=%proc.cmdline)
  priority: WARNING
  tags: [network, security]

- rule: Shell Spawned in Container
  desc: Detect shell execution inside container (possible compromise)
  condition: >
    container.name startswith "llm-research-lab" and
    proc.name in (sh, bash, zsh, fish) and
    proc.pname exists
  output: >
    Shell spawned in container
    (user=%user.name shell=%proc.name parent=%proc.pname container=%container.name)
  priority: WARNING
  tags: [shell, container, security]
```

### 5.6.3 Vulnerability Scanning Schedule

| Scan Type | Frequency | Tool | Severity Threshold | Action |
|-----------|-----------|------|-------------------|--------|
| **Container Image** | Every build + daily | Trivy | Critical: Block, High: Alert | Fail CI for Critical |
| **Dependency** | Every build + weekly | cargo-audit | Critical: Block, High: Alert | Fail CI for Critical |
| **SAST** | Every commit | Semgrep | Critical: Block, High: Alert | Block merge for Critical |
| **DAST** | Weekly | OWASP ZAP | Critical: Alert, High: Alert | Create ticket |
| **Infrastructure** | Daily | Prowler (AWS) | Critical: Alert, High: Alert | Remediate within SLA |
| **Secrets** | Every commit | gitleaks | Any: Block | Fail CI for any secret |

**Trivy Configuration**

```yaml
# trivy-scan.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: trivy-scan
  namespace: llm-research-lab
spec:
  schedule: "0 2 * * *"  # Daily at 2 AM
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: trivy
            image: aquasec/trivy:latest
            args:
            - image
            - --severity
            - CRITICAL,HIGH
            - --exit-code
            - "1"
            - --format
            - json
            - --output
            - /reports/trivy-report.json
            - llm-research-lab-api:latest
            volumeMounts:
            - name: reports
              mountPath: /reports
          volumes:
          - name: reports
            persistentVolumeClaim:
              claimName: security-reports
          restartPolicy: OnFailure
```

**Vulnerability Remediation SLAs**

| Severity | Time to Remediation | Approval Required |
|----------|-------------------|-------------------|
| **Critical** | 24 hours | Security team |
| **High** | 7 days | Engineering lead |
| **Medium** | 30 days | Team lead |
| **Low** | 90 days | Next sprint |

### 5.6.4 Access Audit Logging

```rust
// src/audit/mod.rs
use serde::{Serialize, Deserialize};
use tracing::info;

#[derive(Debug, Serialize, Deserialize)]
pub struct AuditLog {
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub user_id: String,
    pub action: AuditAction,
    pub resource_type: String,
    pub resource_id: String,
    pub source_ip: String,
    pub user_agent: Option<String>,
    pub outcome: AuditOutcome,
    pub metadata: serde_json::Value,
}

#[derive(Debug, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum AuditAction {
    Create,
    Read,
    Update,
    Delete,
    Execute,
    Login,
    Logout,
    AccessDenied,
}

#[derive(Debug, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum AuditOutcome {
    Success,
    Failure,
    PartialSuccess,
}

impl AuditLog {
    pub fn log_data_access(
        user_id: &str,
        dataset_id: &str,
        source_ip: &str,
        outcome: AuditOutcome,
    ) {
        let audit_log = AuditLog {
            timestamp: chrono::Utc::now(),
            user_id: user_id.to_string(),
            action: AuditAction::Read,
            resource_type: "dataset".to_string(),
            resource_id: dataset_id.to_string(),
            source_ip: source_ip.to_string(),
            user_agent: None,
            outcome,
            metadata: serde_json::json!({
                "event_type": "data_access",
            }),
        };

        info!(
            audit_log = ?audit_log,
            event = "audit.data_access",
            "Data access audit log"
        );

        // Also write to dedicated audit log storage
        write_to_audit_store(&audit_log);
    }

    pub fn log_experiment_execution(
        user_id: &str,
        experiment_id: &str,
        outcome: AuditOutcome,
        metadata: serde_json::Value,
    ) {
        let audit_log = AuditLog {
            timestamp: chrono::Utc::now(),
            user_id: user_id.to_string(),
            action: AuditAction::Execute,
            resource_type: "experiment".to_string(),
            resource_id: experiment_id.to_string(),
            source_ip: "internal".to_string(),
            user_agent: None,
            outcome,
            metadata,
        };

        info!(
            audit_log = ?audit_log,
            event = "audit.experiment_execution",
            "Experiment execution audit log"
        );

        write_to_audit_store(&audit_log);
    }

    pub fn log_authorization_failure(
        user_id: &str,
        resource_type: &str,
        resource_id: &str,
        action: AuditAction,
        source_ip: &str,
    ) {
        let audit_log = AuditLog {
            timestamp: chrono::Utc::now(),
            user_id: user_id.to_string(),
            action,
            resource_type: resource_type.to_string(),
            resource_id: resource_id.to_string(),
            source_ip: source_ip.to_string(),
            user_agent: None,
            outcome: AuditOutcome::Failure,
            metadata: serde_json::json!({
                "reason": "insufficient_permissions",
            }),
        };

        info!(
            audit_log = ?audit_log,
            event = "audit.authorization_failure",
            "Authorization failure audit log"
        );

        write_to_audit_store(&audit_log);
    }
}

fn write_to_audit_store(audit_log: &AuditLog) {
    // Write to immutable audit log storage (e.g., S3 with WORM)
    // Implementation depends on chosen audit storage backend
}
```

**Audit Log Retention:**
- **Operational logs**: 30 days in hot storage, 1 year in cold storage
- **Audit logs**: 7 years in compliance-grade storage (WORM)
- **Security event logs**: 2 years in hot storage
- **Access logs**: 90 days in hot storage, 2 years in cold storage

### 5.6.5 Compliance Monitoring

**SOC2 Compliance Checks**

```yaml
# compliance-checks.yml
groups:
  - name: soc2_compliance
    interval: 1h
    rules:
      - alert: EncryptionAtRestDisabled
        expr: |
          kube_persistentvolume_info{encrypted="false"} > 0
        for: 0m
        labels:
          severity: P1
          component: compliance
          framework: soc2
          control: CC6.1
        annotations:
          summary: "Encryption at rest disabled for persistent volume"
          description: |
            Persistent volume is not encrypted, violating SOC2 CC6.1.
            Volume: {{ $labels.persistentvolume }}

      - alert: UnauthorizedAccessAttempts
        expr: |
          increase(rbac_authorization_failures_total[24h]) > 100
        for: 0m
        labels:
          severity: P2
          component: compliance
          framework: soc2
          control: CC6.2
        annotations:
          summary: "Excessive unauthorized access attempts"
          description: |
            More than 100 authorization failures in 24 hours.
            This may indicate inadequate access controls (SOC2 CC6.2).

      - alert: AuditLogGap
        expr: |
          absent(audit_log_entries_total) or
          increase(audit_log_entries_total[1h]) == 0
        for: 1h
        labels:
          severity: P1
          component: compliance
          framework: soc2
          control: CC7.2
        annotations:
          summary: "Audit logging gap detected"
          description: |
            No audit logs generated in the last hour.
            This violates SOC2 CC7.2 monitoring requirements.

      - alert: BackupFailure
        expr: |
          time() - backup_last_success_timestamp > 86400
        for: 0m
        labels:
          severity: P1
          component: compliance
          framework: soc2
          control: CC7.2
        annotations:
          summary: "Backup has not completed in 24 hours"
          description: |
            Last successful backup: {{ $value | humanizeDuration }} ago.
            This violates SOC2 CC7.2 backup requirements.
```

**GDPR Compliance Monitoring**

```rust
// src/compliance/gdpr.rs
use chrono::{DateTime, Utc, Duration};

pub struct GDPRComplianceMonitor {
    retention_policies: HashMap<String, Duration>,
}

impl GDPRComplianceMonitor {
    pub fn check_data_retention(&self) -> Vec<ComplianceViolation> {
        // Check if any personal data exceeds retention period (GDPR Art. 5)
        let violations = vec![];

        for (data_type, max_retention) in &self.retention_policies {
            let old_records = self.find_records_older_than(data_type, *max_retention);
            if !old_records.is_empty() {
                violations.push(ComplianceViolation {
                    regulation: "GDPR".to_string(),
                    article: "Article 5".to_string(),
                    description: format!(
                        "{} records of type '{}' exceed retention period",
                        old_records.len(),
                        data_type
                    ),
                    severity: "High".to_string(),
                    remediation: "Delete or anonymize old records".to_string(),
                });
            }
        }

        violations
    }

    pub fn verify_consent_tracking(&self) -> Vec<ComplianceViolation> {
        // Verify all personal data processing has documented consent (GDPR Art. 6)
        // Implementation checks consent records in database
        vec![]
    }

    pub fn check_data_subject_rights(&self) -> Vec<ComplianceViolation> {
        // Monitor response times to data subject requests (GDPR Art. 15-17)
        // - Right to access: 30 days
        // - Right to erasure: 30 days
        // - Right to rectification: 30 days
        vec![]
    }

    pub fn audit_data_breach_notification(&self) -> Vec<ComplianceViolation> {
        // Verify breach notification timelines (GDPR Art. 33-34)
        // - Authority notification: 72 hours
        // - Individual notification: Without undue delay
        vec![]
    }
}

#[derive(Debug)]
pub struct ComplianceViolation {
    pub regulation: String,
    pub article: String,
    pub description: String,
    pub severity: String,
    pub remediation: String,
}
```

---

## Appendix A: Configuration Files

### A.1 Complete Prometheus Configuration

```yaml
# Full prometheus.yml with all scrape configs
# See Section 5.1.1 for details
```

### A.2 Complete Alertmanager Configuration

```yaml
# Full alertmanager.yml with all receivers
# See Section 5.2.3 for details
```

### A.3 Vector Configuration

```toml
# Full vector.toml for log aggregation
# See Section 5.1.2 for details
```

### A.4 Grafana Provisioning

```yaml
# grafana-dashboards.yaml
apiVersion: 1

providers:
  - name: 'default'
    orgId: 1
    folder: 'LLM Research Lab'
    type: file
    disableDeletion: false
    updateIntervalSeconds: 10
    allowUiUpdates: true
    options:
      path: /var/lib/grafana/dashboards
```

---

## Appendix B: Dashboard Templates

### B.1 Service Overview Dashboard

```json
{
  "dashboard": {
    "title": "LLM Research Lab - Service Overview",
    "tags": ["llm-research-lab", "overview"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(rate(api_request_total[5m]))",
            "legendFormat": "Total Requests/sec"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0}
      },
      {
        "id": 2,
        "title": "Error Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(rate(api_request_total{status=~\"5..\"}[5m])) / sum(rate(api_request_total[5m]))",
            "legendFormat": "Error Rate"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0},
        "alert": {
          "conditions": [
            {
              "evaluator": {"params": [0.01], "type": "gt"},
              "query": {"params": ["A", "5m", "now"]},
              "type": "query"
            }
          ],
          "executionErrorState": "alerting",
          "frequency": "60s",
          "handler": 1,
          "name": "Error Rate Alert",
          "noDataState": "no_data",
          "notifications": []
        }
      },
      {
        "id": 3,
        "title": "Request Latency (p99)",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.99, sum(rate(api_request_duration_seconds_bucket[5m])) by (le))",
            "legendFormat": "p99 Latency"
          },
          {
            "expr": "histogram_quantile(0.95, sum(rate(api_request_duration_seconds_bucket[5m])) by (le))",
            "legendFormat": "p95 Latency"
          },
          {
            "expr": "histogram_quantile(0.50, sum(rate(api_request_duration_seconds_bucket[5m])) by (le))",
            "legendFormat": "p50 Latency"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8},
        "yaxes": [
          {"format": "s", "logBase": 1, "show": true},
          {"format": "short", "logBase": 1, "show": false}
        ]
      },
      {
        "id": 4,
        "title": "Active Experiments",
        "type": "stat",
        "targets": [
          {
            "expr": "experiment_active_count",
            "legendFormat": "Active Experiments"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 8}
      }
    ]
  }
}
```

### B.2 SLO Dashboard

```json
{
  "dashboard": {
    "title": "LLM Research Lab - SLO Compliance",
    "tags": ["llm-research-lab", "slo"],
    "panels": [
      {
        "id": 1,
        "title": "Availability SLO (30-day)",
        "type": "gauge",
        "targets": [
          {
            "expr": "slo:availability:30d * 100",
            "legendFormat": "Availability %"
          }
        ],
        "options": {
          "showThresholdLabels": false,
          "showThresholdMarkers": true
        },
        "fieldConfig": {
          "defaults": {
            "thresholds": {
              "mode": "absolute",
              "steps": [
                {"value": 0, "color": "red"},
                {"value": 99.9, "color": "yellow"},
                {"value": 99.95, "color": "green"}
              ]
            },
            "max": 100,
            "min": 99
          }
        }
      },
      {
        "id": 2,
        "title": "Error Budget Remaining",
        "type": "bargauge",
        "targets": [
          {
            "expr": "slo:availability:error_budget:remaining:percent",
            "legendFormat": "Availability Error Budget"
          },
          {
            "expr": "slo:latency:error_budget:remaining:percent",
            "legendFormat": "Latency Error Budget"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "thresholds": {
              "mode": "absolute",
              "steps": [
                {"value": 0, "color": "red"},
                {"value": 10, "color": "yellow"},
                {"value": 25, "color": "green"}
              ]
            },
            "max": 100,
            "min": 0
          }
        }
      },
      {
        "id": 3,
        "title": "SLO Compliance Trend",
        "type": "graph",
        "targets": [
          {
            "expr": "slo:availability:30d * 100",
            "legendFormat": "Availability %"
          },
          {
            "expr": "99.9",
            "legendFormat": "SLO Target"
          }
        ],
        "gridPos": {"h": 8, "w": 24, "x": 0, "y": 16}
      }
    ]
  }
}
```

---

## Appendix C: Runbook Index

| Alert | Severity | Runbook Link |
|-------|----------|--------------|
| AvailabilitySLOViolation | P1 | [Link](https://runbooks.example.com/availability-slo-violation) |
| LatencySLOViolation | P1 | [Link](https://runbooks.example.com/latency-slo-violation) |
| ServiceDown | P1 | [Link](https://runbooks.example.com/service-down) |
| DatabaseConnectionPoolExhausted | P1 | [Link](https://runbooks.example.com/db-connection-pool-exhausted) |
| HighExperimentFailureRate | P2 | [Link](https://runbooks.example.com/high-experiment-failure-rate) |
| ModelAPIHighLatency | P2 | [Link](https://runbooks.example.com/model-api-high-latency) |
| HighCPUUsage | P2 | [Link](https://runbooks.example.com/high-cpu-usage) |
| DiskSpaceLow | P2 | [Link](https://runbooks.example.com/disk-space-low) |
| UnauthorizedAccessAttempt | P2 | [Link](https://runbooks.example.com/unauthorized-access-attempts) |

---

## Document Metadata

| Field | Value |
|-------|-------|
| **Version** | 1.0.0 |
| **Status** | Complete |
| **SPARC Phase** | Phase 5 - Completion |
| **Section** | 5 - Operations & Monitoring |
| **Created** | 2025-11-28 |
| **Target SLA** | 99.9% Availability |
| **Technology Stack** | Rust, Axum, Tokio, Prometheus, Grafana, OpenTelemetry |

---

*This document is part of the SPARC Phase 5 (Completion) specification for LLM-Research-Lab. It provides enterprise-grade operations and monitoring requirements for production deployment.*
---

## Disaster Recovery & Business Continuity


## Table of Contents

- [Overview](#overview)
- [6.1 Backup Strategy](#61-backup-strategy)
- [6.2 Recovery Procedures](#62-recovery-procedures)
- [6.3 High Availability Architecture](#63-high-availability-architecture)
- [6.4 Disaster Recovery Plan](#64-disaster-recovery-plan)
- [6.5 Business Continuity](#65-business-continuity)
- [Appendix A: Recovery Scripts](#appendix-a-recovery-scripts)
- [Appendix B: Runbook Templates](#appendix-b-runbook-templates)
- [Appendix C: DR Testing Checklist](#appendix-c-dr-testing-checklist)

---

## Overview

This section establishes comprehensive disaster recovery and business continuity capabilities for LLM-Research-Lab, ensuring data protection, rapid recovery, and service resilience across all infrastructure components.

### Recovery Objectives

| Metric | Target | Measurement |
|--------|--------|-------------|
| RTO (Recovery Time Objective) | < 4 hours | Time to restore service functionality |
| RPO (Recovery Point Objective) | < 1 hour | Maximum acceptable data loss |
| Availability SLA | 99.9% | Annual uptime target (8.76h downtime/year) |
| MTTR (Mean Time to Recovery) | < 2 hours | Average incident resolution time |
| Backup Verification | 100% | All backups tested monthly |

### Disaster Categories

1. **Infrastructure Failure**: Single AZ outage, network partition
2. **Data Corruption**: Database corruption, accidental deletion
3. **Security Incident**: Ransomware, data breach, unauthorized access
4. **Application Failure**: Software bugs, deployment issues
5. **Regional Outage**: Multi-AZ failure, cloud provider incident

---

## 6.1 Backup Strategy

### 6.1.1 PostgreSQL Backup Configuration

#### Continuous Archiving (WAL-E/WAL-G)

```yaml
# k8s/postgres-backup-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres-backup-config
  namespace: llm-research-lab-data
data:
  backup-config.yaml: |
    # WAL-G Configuration
    aws:
      region: us-east-1
      s3_bucket: llm-research-lab-postgres-backups
      s3_prefix: wal-archive
      sse: AES256

    postgres:
      host: postgres-primary.llm-research-lab-data.svc.cluster.local
      port: 5432
      database: llm_research_lab
      user: backup_user

    # Backup Schedule
    backup_schedule:
      full_backup: "0 2 * * *"  # Daily at 2 AM UTC
      incremental_backup: "0 */4 * * *"  # Every 4 hours
      wal_archival: continuous

    # Retention Policy
    retention:
      full_backups: 30  # Keep 30 daily backups
      incremental_backups: 7  # Keep 7 days of incrementals
      wal_segments: 7  # Keep 7 days of WAL
      weekly_backups: 12  # Keep 12 weekly backups (90 days)
      monthly_backups: 12  # Keep 12 monthly backups (1 year)

    # Compression & Encryption
    compression: lz4
    encryption:
      enabled: true
      kms_key_id: arn:aws:kms:us-east-1:ACCOUNT:key/backup-encryption

    # Performance Tuning
    upload_concurrency: 4
    download_concurrency: 4
    upload_disk_concurrency: 2
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgres-full-backup
  namespace: llm-research-lab-data
spec:
  schedule: "0 2 * * *"
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 7
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      backoffLimit: 2
      template:
        metadata:
          labels:
            app: postgres-backup
            type: full
        spec:
          restartPolicy: OnFailure
          serviceAccountName: postgres-backup-sa
          containers:
          - name: wal-g-backup
            image: wal-g/wal-g:latest
            imagePullPolicy: IfNotPresent
            command:
            - /bin/bash
            - -c
            - |
              set -e
              echo "Starting PostgreSQL full backup at $(date)"

              # Pre-backup validation
              pg_isready -h ${PGHOST} -p ${PGPORT} -U ${PGUSER}

              # Create backup with metadata
              BACKUP_NAME="base_$(date +%Y%m%d_%H%M%S)"
              wal-g backup-push /var/lib/postgresql/data \
                --backup-name ${BACKUP_NAME} \
                --full-backup-on-error \
                --delta-from-name LATEST

              # Verify backup
              wal-g backup-list | grep ${BACKUP_NAME}

              # Send metrics
              curl -X POST http://prometheus-pushgateway:9091/metrics/job/postgres_backup \
                -d "postgres_backup_success{type=\"full\"} 1"
              curl -X POST http://prometheus-pushgateway:9091/metrics/job/postgres_backup \
                -d "postgres_backup_duration_seconds{type=\"full\"} $SECONDS"

              echo "Backup completed successfully"
            env:
            - name: PGHOST
              value: postgres-primary.llm-research-lab-data.svc.cluster.local
            - name: PGPORT
              value: "5432"
            - name: PGUSER
              valueFrom:
                secretKeyRef:
                  name: postgres-backup-credentials
                  key: username
            - name: PGPASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres-backup-credentials
                  key: password
            - name: WALG_S3_PREFIX
              value: s3://llm-research-lab-postgres-backups/wal-archive
            - name: AWS_REGION
              value: us-east-1
            - name: WALG_COMPRESSION_METHOD
              value: lz4
            - name: WALG_DELTA_MAX_STEPS
              value: "7"
            volumeMounts:
            - name: backup-config
              mountPath: /etc/wal-g
              readOnly: true
            resources:
              requests:
                cpu: 500m
                memory: 1Gi
              limits:
                cpu: 2
                memory: 4Gi
          volumes:
          - name: backup-config
            configMap:
              name: postgres-backup-config
```

#### PostgreSQL Logical Backups

```yaml
# k8s/postgres-logical-backup.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgres-logical-backup
  namespace: llm-research-lab-data
spec:
  schedule: "30 3 * * *"  # Daily at 3:30 AM UTC
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: OnFailure
          containers:
          - name: pg-dump
            image: postgres:15-alpine
            command:
            - /bin/sh
            - -c
            - |
              set -e
              BACKUP_FILE="llm_research_lab_$(date +%Y%m%d_%H%M%S).dump"

              # Create custom format dump (supports parallel restore)
              pg_dump -h ${PGHOST} \
                      -U ${PGUSER} \
                      -d llm_research_lab \
                      -F c \
                      -Z 6 \
                      -f /tmp/${BACKUP_FILE} \
                      --verbose \
                      --no-owner \
                      --no-acl

              # Upload to S3
              aws s3 cp /tmp/${BACKUP_FILE} \
                s3://llm-research-lab-postgres-backups/logical/${BACKUP_FILE} \
                --storage-class INTELLIGENT_TIERING \
                --server-side-encryption AES256

              # Create metadata
              cat > /tmp/metadata.json <<EOF
              {
                "backup_type": "logical",
                "database": "llm_research_lab",
                "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
                "size_bytes": $(stat -f%z /tmp/${BACKUP_FILE}),
                "pg_version": "$(psql -h ${PGHOST} -U ${PGUSER} -t -c 'SELECT version()')"
              }
              EOF

              aws s3 cp /tmp/metadata.json \
                s3://llm-research-lab-postgres-backups/logical/${BACKUP_FILE}.metadata.json

              echo "Logical backup completed: ${BACKUP_FILE}"
            env:
            - name: PGHOST
              value: postgres-primary.llm-research-lab-data.svc.cluster.local
            - name: PGUSER
              valueFrom:
                secretKeyRef:
                  name: postgres-backup-credentials
                  key: username
            - name: PGPASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres-backup-credentials
                  key: password
            resources:
              requests:
                cpu: 1
                memory: 2Gi
              limits:
                cpu: 2
                memory: 4Gi
```

### 6.1.2 ClickHouse Backup Configuration

```yaml
# k8s/clickhouse-backup-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: clickhouse-backup-config
  namespace: llm-research-lab-data
data:
  config.yaml: |
    general:
      remote_storage: s3
      disable_progress_bar: false
      backups_to_keep_local: 3
      backups_to_keep_remote: 30
      log_level: info
      allow_empty_backups: false

    clickhouse:
      username: backup_user
      host: clickhouse.llm-research-lab-data.svc.cluster.local
      port: 9000
      data_path: /var/lib/clickhouse
      skip_tables:
        - system.*
        - INFORMATION_SCHEMA.*
      timeout: 5m

    s3:
      access_key: ${AWS_ACCESS_KEY_ID}
      secret_key: ${AWS_SECRET_ACCESS_KEY}
      bucket: llm-research-lab-clickhouse-backups
      region: us-east-1
      acl: private
      endpoint: ""
      force_path_style: false
      path: backups
      disable_ssl: false
      compression_level: 9
      compression_format: gzip
      sse: AES256
      disable_cert_verification: false
      storage_class: STANDARD
      concurrency: 4
      part_size: 104857600  # 100MB
      max_parts_count: 10000
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: clickhouse-backup
  namespace: llm-research-lab-data
spec:
  schedule: "0 4 * * *"  # Daily at 4 AM UTC
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: OnFailure
          containers:
          - name: clickhouse-backup
            image: altinity/clickhouse-backup:latest
            command:
            - /bin/bash
            - -c
            - |
              set -e
              BACKUP_NAME="auto_$(date +%Y%m%d_%H%M%S)"

              echo "Creating ClickHouse backup: ${BACKUP_NAME}"
              clickhouse-backup create ${BACKUP_NAME}

              echo "Uploading backup to S3"
              clickhouse-backup upload ${BACKUP_NAME}

              echo "Cleaning up old local backups"
              clickhouse-backup delete local --keep-last=3

              echo "Cleaning up old remote backups"
              clickhouse-backup delete remote --keep-last=30

              echo "Backup completed successfully"
            env:
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: clickhouse-backup-credentials
                  key: aws_access_key_id
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: clickhouse-backup-credentials
                  key: aws_secret_access_key
            volumeMounts:
            - name: backup-config
              mountPath: /etc/clickhouse-backup
            - name: clickhouse-data
              mountPath: /var/lib/clickhouse
            resources:
              requests:
                cpu: 1
                memory: 2Gi
              limits:
                cpu: 2
                memory: 4Gi
          volumes:
          - name: backup-config
            configMap:
              name: clickhouse-backup-config
          - name: clickhouse-data
            persistentVolumeClaim:
              claimName: clickhouse-data
```

### 6.1.3 Kafka Backup Strategy

```yaml
# k8s/kafka-backup-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kafka-backup-config
  namespace: llm-research-lab-data
data:
  backup.sh: |
    #!/bin/bash
    set -e

    BACKUP_DATE=$(date +%Y%m%d_%H%M%S)
    BACKUP_DIR="/tmp/kafka-backup-${BACKUP_DATE}"
    S3_BUCKET="s3://llm-research-lab-kafka-backups"

    mkdir -p ${BACKUP_DIR}

    # Export topic configurations
    echo "Exporting topic configurations..."
    kafka-topics.sh --bootstrap-server kafka:9092 \
      --describe --topics-with-overrides > ${BACKUP_DIR}/topic-configs.txt

    # Export consumer group offsets
    echo "Exporting consumer group offsets..."
    kafka-consumer-groups.sh --bootstrap-server kafka:9092 \
      --all-groups --describe > ${BACKUP_DIR}/consumer-offsets.txt

    # Export ACLs
    echo "Exporting ACLs..."
    kafka-acls.sh --bootstrap-server kafka:9092 \
      --list > ${BACKUP_DIR}/acls.txt

    # Create metadata snapshot
    cat > ${BACKUP_DIR}/metadata.json <<EOF
    {
      "backup_timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
      "kafka_version": "$(kafka-broker-api-versions.sh --bootstrap-server kafka:9092 | head -1)",
      "cluster_id": "$(kafka-cluster.sh cluster-id --bootstrap-server kafka:9092)"
    }
    EOF

    # Compress and upload
    tar -czf /tmp/kafka-metadata-${BACKUP_DATE}.tar.gz -C /tmp kafka-backup-${BACKUP_DATE}
    aws s3 cp /tmp/kafka-metadata-${BACKUP_DATE}.tar.gz \
      ${S3_BUCKET}/metadata/kafka-metadata-${BACKUP_DATE}.tar.gz \
      --storage-class STANDARD_IA

    # Cleanup
    rm -rf ${BACKUP_DIR} /tmp/kafka-metadata-${BACKUP_DATE}.tar.gz

    echo "Kafka metadata backup completed"
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: kafka-metadata-backup
  namespace: llm-research-lab-data
spec:
  schedule: "0 */6 * * *"  # Every 6 hours
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: OnFailure
          containers:
          - name: kafka-backup
            image: confluentinc/cp-kafka:7.5.0
            command: ["/bin/bash", "/scripts/backup.sh"]
            volumeMounts:
            - name: backup-script
              mountPath: /scripts
            resources:
              requests:
                cpu: 200m
                memory: 512Mi
          volumes:
          - name: backup-script
            configMap:
              name: kafka-backup-config
              defaultMode: 0755
```

### 6.1.4 Redis Backup Configuration

```yaml
# k8s/redis-backup-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: redis-backup-config
  namespace: llm-research-lab-data
data:
  backup.sh: |
    #!/bin/bash
    set -e

    BACKUP_DATE=$(date +%Y%m%d_%H%M%S)
    S3_BUCKET="s3://llm-research-lab-redis-backups"

    # Trigger BGSAVE
    echo "Triggering Redis BGSAVE..."
    redis-cli -h redis-master BGSAVE

    # Wait for save to complete
    while [ $(redis-cli -h redis-master LASTSAVE) -eq $(redis-cli -h redis-master LASTSAVE) ]; do
      echo "Waiting for BGSAVE to complete..."
      sleep 5
    done

    # Copy RDB file
    RDB_FILE="/data/dump.rdb"
    BACKUP_FILE="redis-dump-${BACKUP_DATE}.rdb"

    cp ${RDB_FILE} /tmp/${BACKUP_FILE}

    # Upload to S3
    aws s3 cp /tmp/${BACKUP_FILE} \
      ${S3_BUCKET}/snapshots/${BACKUP_FILE} \
      --storage-class STANDARD_IA \
      --server-side-encryption AES256

    # Create metadata
    cat > /tmp/metadata.json <<EOF
    {
      "backup_timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
      "redis_version": "$(redis-cli -h redis-master INFO server | grep redis_version)",
      "used_memory": "$(redis-cli -h redis-master INFO memory | grep used_memory_human)",
      "db_keys": "$(redis-cli -h redis-master DBSIZE)"
    }
    EOF

    aws s3 cp /tmp/metadata.json \
      ${S3_BUCKET}/snapshots/${BACKUP_FILE}.metadata.json

    # Cleanup
    rm -f /tmp/${BACKUP_FILE} /tmp/metadata.json

    echo "Redis backup completed: ${BACKUP_FILE}"
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: redis-backup
  namespace: llm-research-lab-data
spec:
  schedule: "0 */12 * * *"  # Every 12 hours
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: OnFailure
          containers:
          - name: redis-backup
            image: redis:7-alpine
            command: ["/bin/sh", "/scripts/backup.sh"]
            volumeMounts:
            - name: backup-script
              mountPath: /scripts
            - name: redis-data
              mountPath: /data
            env:
            - name: AWS_REGION
              value: us-east-1
            resources:
              requests:
                cpu: 200m
                memory: 512Mi
          volumes:
          - name: backup-script
            configMap:
              name: redis-backup-config
              defaultMode: 0755
          - name: redis-data
            persistentVolumeClaim:
              claimName: redis-data
```

### 6.1.5 Backup Verification Testing

```yaml
# k8s/backup-verification-job.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: backup-verification
  namespace: llm-research-lab-data
spec:
  schedule: "0 6 * * 0"  # Weekly on Sunday at 6 AM UTC
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: backup-verification
        spec:
          restartPolicy: OnFailure
          containers:
          - name: verify-backups
            image: amazon/aws-cli:latest
            command:
            - /bin/bash
            - -c
            - |
              set -e

              echo "=== Backup Verification Report ==="
              echo "Date: $(date -u +%Y-%m-%dT%H:%M:%SZ)"
              echo ""

              # Verify PostgreSQL backups
              echo "PostgreSQL Backups:"
              LATEST_PG_BACKUP=$(aws s3 ls s3://llm-research-lab-postgres-backups/wal-archive/ \
                --recursive | sort | tail -1 | awk '{print $4}')
              BACKUP_AGE=$(($(date +%s) - $(date -d "$(aws s3 ls s3://llm-research-lab-postgres-backups/wal-archive/${LATEST_PG_BACKUP} | awk '{print $1" "$2}')" +%s)))

              if [ ${BACKUP_AGE} -lt 86400 ]; then
                echo "  ✓ Latest backup: ${LATEST_PG_BACKUP} (${BACKUP_AGE}s old)"
              else
                echo "  ✗ Latest backup is older than 24h: ${BACKUP_AGE}s"
                exit 1
              fi

              # Verify ClickHouse backups
              echo "ClickHouse Backups:"
              LATEST_CH_BACKUP=$(aws s3 ls s3://llm-research-lab-clickhouse-backups/backups/ | sort | tail -1 | awk '{print $2}')
              echo "  ✓ Latest backup: ${LATEST_CH_BACKUP}"

              # Verify Kafka metadata backups
              echo "Kafka Metadata Backups:"
              LATEST_KAFKA_BACKUP=$(aws s3 ls s3://llm-research-lab-kafka-backups/metadata/ | sort | tail -1 | awk '{print $4}')
              echo "  ✓ Latest backup: ${LATEST_KAFKA_BACKUP}"

              # Verify Redis backups
              echo "Redis Backups:"
              LATEST_REDIS_BACKUP=$(aws s3 ls s3://llm-research-lab-redis-backups/snapshots/ | grep .rdb | sort | tail -1 | awk '{print $4}')
              echo "  ✓ Latest backup: ${LATEST_REDIS_BACKUP}"

              # Send success metric
              curl -X POST http://prometheus-pushgateway:9091/metrics/job/backup_verification \
                -d "backup_verification_success 1"

              echo ""
              echo "=== Verification Complete ==="
            resources:
              requests:
                cpu: 100m
                memory: 256Mi
```

### 6.1.6 Retention Policy Management

```yaml
# k8s/backup-retention-cleanup.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: backup-retention-cleanup
  namespace: llm-research-lab-data
spec:
  schedule: "0 5 * * 0"  # Weekly on Sunday at 5 AM UTC
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: OnFailure
          containers:
          - name: cleanup
            image: amazon/aws-cli:latest
            command:
            - /bin/bash
            - -c
            - |
              set -e

              # Function to delete old backups
              cleanup_backups() {
                local bucket=$1
                local prefix=$2
                local days=$3

                echo "Cleaning up backups older than ${days} days in ${bucket}/${prefix}"

                aws s3 ls s3://${bucket}/${prefix} --recursive | \
                  while read -r line; do
                    createDate=$(echo $line | awk '{print $1" "$2}')
                    createDate=$(date -d "$createDate" +%s)
                    olderThan=$(date -d "-${days} days" +%s)

                    if [[ $createDate -lt $olderThan ]]; then
                      fileName=$(echo $line | awk '{print $4}')
                      echo "Deleting: ${fileName}"
                      aws s3 rm s3://${bucket}/${fileName}
                    fi
                  done
              }

              # PostgreSQL: Keep daily for 30 days, weekly for 90 days, monthly for 1 year
              echo "=== PostgreSQL Backup Retention ==="
              # Daily backups older than 30 days
              cleanup_backups "llm-research-lab-postgres-backups" "wal-archive/base_" 30
              # Logical backups older than 90 days
              cleanup_backups "llm-research-lab-postgres-backups" "logical/" 90

              # ClickHouse: Keep 30 daily backups
              echo "=== ClickHouse Backup Retention ==="
              cleanup_backups "llm-research-lab-clickhouse-backups" "backups/" 30

              # Kafka: Keep metadata for 90 days
              echo "=== Kafka Backup Retention ==="
              cleanup_backups "llm-research-lab-kafka-backups" "metadata/" 90

              # Redis: Keep snapshots for 30 days
              echo "=== Redis Backup Retention ==="
              cleanup_backups "llm-research-lab-redis-backups" "snapshots/" 30

              echo "Retention cleanup completed"
            resources:
              requests:
                cpu: 100m
                memory: 256Mi
```

---

## 6.2 Recovery Procedures

### 6.2.1 PostgreSQL Point-in-Time Recovery (PITR)

```bash
#!/bin/bash
# scripts/postgres-pitr-recovery.sh
# PostgreSQL Point-in-Time Recovery Script

set -e

RECOVERY_TARGET_TIME=${1:-"latest"}
RECOVERY_TARGET_NAME=${2:-""}
S3_BACKUP_BUCKET="s3://llm-research-lab-postgres-backups/wal-archive"
PGDATA="/var/lib/postgresql/data"
RECOVERY_CONF="${PGDATA}/postgresql.auto.conf"

echo "=== PostgreSQL PITR Recovery ==="
echo "Recovery Target Time: ${RECOVERY_TARGET_TIME}"
echo "Recovery Target Name: ${RECOVERY_TARGET_NAME}"
echo ""

# Step 1: Stop PostgreSQL if running
echo "[1/7] Stopping PostgreSQL..."
pg_ctl stop -D ${PGDATA} -m fast || true
sleep 5

# Step 2: Backup current data directory
echo "[2/7] Backing up current data directory..."
BACKUP_DIR="/var/lib/postgresql/data-backup-$(date +%Y%m%d_%H%M%S)"
mv ${PGDATA} ${BACKUP_DIR}
echo "Existing data backed up to: ${BACKUP_DIR}"

# Step 3: List available base backups
echo "[3/7] Available base backups:"
wal-g backup-list

# Step 4: Restore base backup
if [ "${RECOVERY_TARGET_TIME}" == "latest" ]; then
  BACKUP_NAME="LATEST"
else
  # Find backup closest to target time
  BACKUP_NAME=$(wal-g backup-list --detail --json | \
    jq -r --arg target "${RECOVERY_TARGET_TIME}" \
    '.[] | select(.time <= $target) | .backup_name' | \
    tail -1)
fi

echo "[4/7] Restoring base backup: ${BACKUP_NAME}"
mkdir -p ${PGDATA}
wal-g backup-fetch ${PGDATA} ${BACKUP_NAME}

# Step 5: Configure recovery settings
echo "[5/7] Configuring recovery settings..."
cat > ${PGDATA}/recovery.signal <<EOF
# Recovery configuration
EOF

cat >> ${RECOVERY_CONF} <<EOF
# Recovery Settings
restore_command = 'wal-g wal-fetch %f %p'
recovery_target_action = 'promote'
EOF

if [ "${RECOVERY_TARGET_TIME}" != "latest" ]; then
  echo "recovery_target_time = '${RECOVERY_TARGET_TIME}'" >> ${RECOVERY_CONF}
fi

if [ -n "${RECOVERY_TARGET_NAME}" ]; then
  echo "recovery_target_name = '${RECOVERY_TARGET_NAME}'" >> ${RECOVERY_CONF}
fi

# Step 6: Set permissions
echo "[6/7] Setting permissions..."
chown -R postgres:postgres ${PGDATA}
chmod 700 ${PGDATA}

# Step 7: Start PostgreSQL in recovery mode
echo "[7/7] Starting PostgreSQL in recovery mode..."
pg_ctl start -D ${PGDATA} -l ${PGDATA}/recovery.log

# Wait for recovery to complete
echo "Waiting for recovery to complete..."
until pg_isready -h localhost -p 5432; do
  echo "PostgreSQL is recovering..."
  sleep 5
done

echo ""
echo "=== Recovery Complete ==="
echo "PostgreSQL is now running and has been promoted."
echo "Recovery log: ${PGDATA}/recovery.log"
echo ""

# Verify recovery
echo "=== Post-Recovery Verification ==="
psql -U postgres -c "SELECT pg_is_in_recovery();"
psql -U postgres -c "SELECT pg_last_wal_receive_lsn();"
psql -U postgres -c "SELECT COUNT(*) FROM experiments;"

echo ""
echo "Recovery completed successfully!"
```

### 6.2.2 ClickHouse Full System Restore

```bash
#!/bin/bash
# scripts/clickhouse-full-restore.sh
# ClickHouse Full System Restore Script

set -e

BACKUP_NAME=${1:-""}
RESTORE_SCHEMA=${2:-"all"}
S3_BUCKET="s3://llm-research-lab-clickhouse-backups/backups"

if [ -z "${BACKUP_NAME}" ]; then
  echo "Usage: $0 <backup_name> [schema_name|all]"
  echo ""
  echo "Available backups:"
  clickhouse-backup list remote
  exit 1
fi

echo "=== ClickHouse Full Restore ==="
echo "Backup Name: ${BACKUP_NAME}"
echo "Restore Schema: ${RESTORE_SCHEMA}"
echo ""

# Step 1: Download backup from S3
echo "[1/5] Downloading backup from S3..."
clickhouse-backup download ${BACKUP_NAME}

# Step 2: List backup contents
echo "[2/5] Backup contents:"
clickhouse-backup list local | grep ${BACKUP_NAME}

# Step 3: Stop ClickHouse writes (optional, creates consistency)
echo "[3/5] Setting ClickHouse to read-only mode..."
clickhouse-client --query "SYSTEM STOP MERGES"
clickhouse-client --query "SET readonly = 1"

# Step 4: Restore backup
echo "[4/5] Restoring backup..."
if [ "${RESTORE_SCHEMA}" == "all" ]; then
  clickhouse-backup restore --schema --data ${BACKUP_NAME}
else
  clickhouse-backup restore --schema --data --table ${RESTORE_SCHEMA}.* ${BACKUP_NAME}
fi

# Step 5: Resume normal operations
echo "[5/5] Resuming normal operations..."
clickhouse-client --query "SET readonly = 0"
clickhouse-client --query "SYSTEM START MERGES"

echo ""
echo "=== Restore Complete ==="

# Verification
echo "=== Post-Restore Verification ==="
clickhouse-client --query "SELECT database, name, total_rows FROM system.tables WHERE database NOT IN ('system', 'INFORMATION_SCHEMA')"

echo ""
echo "Restore completed successfully!"
```

### 6.2.3 Data Integrity Verification

```bash
#!/bin/bash
# scripts/verify-data-integrity.sh
# Post-Recovery Data Integrity Verification

set -e

echo "=== Data Integrity Verification ==="
echo "Timestamp: $(date -u +%Y-%m-%dT%H:%M:%SZ)"
echo ""

ERRORS=0

# PostgreSQL Integrity Checks
echo "[PostgreSQL Checks]"
echo "Running pg_checksums..."
if pg_checksums -D /var/lib/postgresql/data --check; then
  echo "  ✓ Checksums valid"
else
  echo "  ✗ Checksum validation failed"
  ((ERRORS++))
fi

echo "Running VACUUM ANALYZE..."
psql -U postgres -d llm_research_lab -c "VACUUM ANALYZE;" || ((ERRORS++))

echo "Checking table counts..."
EXPERIMENT_COUNT=$(psql -U postgres -d llm_research_lab -t -c "SELECT COUNT(*) FROM experiments;")
echo "  Experiments: ${EXPERIMENT_COUNT}"

echo "Checking referential integrity..."
psql -U postgres -d llm_research_lab -c "
  SELECT conrelid::regclass AS table_name, conname AS constraint_name
  FROM pg_constraint
  WHERE contype = 'f'
  ORDER BY conrelid::regclass::text, conname;
" || ((ERRORS++))

# ClickHouse Integrity Checks
echo ""
echo "[ClickHouse Checks]"
echo "Checking table integrity..."
clickhouse-client --query "CHECK TABLE experiments.runs" || ((ERRORS++))

echo "Running OPTIMIZE..."
clickhouse-client --query "OPTIMIZE TABLE experiments.runs FINAL" || ((ERRORS++))

echo "Verifying row counts..."
RUN_COUNT=$(clickhouse-client --query "SELECT COUNT(*) FROM experiments.runs")
echo "  Experiment Runs: ${RUN_COUNT}"

# Kafka Checks
echo ""
echo "[Kafka Checks]"
echo "Checking topic health..."
kafka-topics.sh --bootstrap-server kafka:9092 --describe | head -20

echo "Verifying consumer groups..."
kafka-consumer-groups.sh --bootstrap-server kafka:9092 --list

# Redis Checks
echo ""
echo "[Redis Checks]"
echo "Running Redis DBSIZE..."
REDIS_KEYS=$(redis-cli -h redis-master DBSIZE)
echo "  Total Keys: ${REDIS_KEYS}"

echo "Checking Redis memory..."
redis-cli -h redis-master INFO memory | grep used_memory_human

echo "Running Redis consistency check..."
redis-cli -h redis-master --rdb /tmp/redis-check.rdb
redis-check-rdb /tmp/redis-check.rdb || ((ERRORS++))
rm -f /tmp/redis-check.rdb

# Summary
echo ""
echo "=== Verification Summary ==="
if [ ${ERRORS} -eq 0 ]; then
  echo "✓ All integrity checks passed"
  exit 0
else
  echo "✗ ${ERRORS} integrity check(s) failed"
  exit 1
fi
```

---

## 6.3 High Availability Architecture

### 6.3.1 Multi-Zone Deployment Configuration

```yaml
# k8s/multi-az-topology.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: topology-spread-constraints
  namespace: llm-research-lab-prod
data:
  constraints.yaml: |
    # Topology Spread Constraints for HA
    topologySpreadConstraints:
    - maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
      labelSelector:
        matchLabels:
          app: llm-research-lab-api
    - maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
      labelSelector:
        matchLabels:
          app: llm-research-lab-api
---
# PostgreSQL Primary-Replica Configuration
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres-primary
  namespace: llm-research-lab-data
spec:
  replicas: 1
  serviceName: postgres-primary
  selector:
    matchLabels:
      app: postgres
      role: primary
  template:
    metadata:
      labels:
        app: postgres
        role: primary
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: topology.kubernetes.io/zone
                operator: In
                values:
                - us-east-1a
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - postgres
              - key: role
                operator: In
                values:
                - replica
            topologyKey: kubernetes.io/hostname
      containers:
      - name: postgres
        image: postgres:15-alpine
        ports:
        - containerPort: 5432
          name: postgres
        env:
        - name: POSTGRES_DB
          value: llm_research_lab
        - name: POSTGRES_USER
          valueFrom:
            secretKeyRef:
              name: postgres-credentials
              key: username
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-credentials
              key: password
        - name: POSTGRES_REPLICATION_USER
          value: replicator
        - name: POSTGRES_REPLICATION_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-replication
              key: password
        volumeMounts:
        - name: postgres-data
          mountPath: /var/lib/postgresql/data
        - name: postgres-config
          mountPath: /etc/postgresql
        resources:
          requests:
            cpu: 2
            memory: 4Gi
          limits:
            cpu: 4
            memory: 8Gi
        livenessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - pg_isready -U postgres
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - pg_isready -U postgres && psql -U postgres -c "SELECT 1"
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
      volumes:
      - name: postgres-config
        configMap:
          name: postgres-primary-config
  volumeClaimTemplates:
  - metadata:
      name: postgres-data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 100Gi
---
# PostgreSQL Read Replicas (Multi-AZ)
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres-replica
  namespace: llm-research-lab-data
spec:
  replicas: 2
  serviceName: postgres-replica
  selector:
    matchLabels:
      app: postgres
      role: replica
  template:
    metadata:
      labels:
        app: postgres
        role: replica
    spec:
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: postgres
            role: replica
      containers:
      - name: postgres
        image: postgres:15-alpine
        ports:
        - containerPort: 5432
          name: postgres
        env:
        - name: POSTGRES_PRIMARY_HOST
          value: postgres-primary.llm-research-lab-data.svc.cluster.local
        - name: POSTGRES_REPLICATION_USER
          value: replicator
        - name: POSTGRES_REPLICATION_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-replication
              key: password
        command:
        - /bin/bash
        - -c
        - |
          set -e
          # Wait for primary to be ready
          until pg_isready -h ${POSTGRES_PRIMARY_HOST}; do
            echo "Waiting for primary..."
            sleep 2
          done

          # Create base backup from primary
          rm -rf /var/lib/postgresql/data/*
          pg_basebackup -h ${POSTGRES_PRIMARY_HOST} \
            -U ${POSTGRES_REPLICATION_USER} \
            -D /var/lib/postgresql/data \
            -Fp -Xs -P -R

          # Start PostgreSQL in replica mode
          postgres
        volumeMounts:
        - name: postgres-data
          mountPath: /var/lib/postgresql/data
        resources:
          requests:
            cpu: 1
            memory: 2Gi
          limits:
            cpu: 2
            memory: 4Gi
  volumeClaimTemplates:
  - metadata:
      name: postgres-data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 100Gi
```

### 6.3.2 Failover Automation

```yaml
# k8s/postgres-failover-automation.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres-failover-script
  namespace: llm-research-lab-data
data:
  failover.sh: |
    #!/bin/bash
    set -e

    PRIMARY_HOST="postgres-primary.llm-research-lab-data.svc.cluster.local"
    REPLICA_SELECTOR="app=postgres,role=replica"
    NAMESPACE="llm-research-lab-data"

    echo "=== PostgreSQL Failover Automation ==="
    echo "Timestamp: $(date -u +%Y-%m-%dT%H:%M:%SZ)"
    echo ""

    # Check primary health
    if pg_isready -h ${PRIMARY_HOST} -t 5; then
      echo "Primary is healthy. No failover needed."
      exit 0
    fi

    echo "⚠️  Primary is unhealthy. Initiating failover..."

    # Find healthiest replica
    BEST_REPLICA=""
    BEST_LSN=0

    for pod in $(kubectl get pods -n ${NAMESPACE} -l ${REPLICA_SELECTOR} -o name); do
      POD_NAME=${pod#pod/}
      echo "Checking replica: ${POD_NAME}"

      # Get LSN (log sequence number)
      LSN=$(kubectl exec -n ${NAMESPACE} ${POD_NAME} -- \
        psql -U postgres -t -c "SELECT pg_last_wal_receive_lsn();" | tr -d ' ')

      echo "  LSN: ${LSN}"

      if [ "${LSN}" \> "${BEST_LSN}" ]; then
        BEST_LSN=${LSN}
        BEST_REPLICA=${POD_NAME}
      fi
    done

    if [ -z "${BEST_REPLICA}" ]; then
      echo "❌ No healthy replica found. Manual intervention required."
      exit 1
    fi

    echo "Selected replica for promotion: ${BEST_REPLICA}"
    echo "Promoting replica to primary..."

    # Promote replica
    kubectl exec -n ${NAMESPACE} ${BEST_REPLICA} -- \
      pg_ctl promote -D /var/lib/postgresql/data

    # Wait for promotion
    sleep 5

    # Verify new primary
    NEW_PRIMARY_IP=$(kubectl get pod -n ${NAMESPACE} ${BEST_REPLICA} -o jsonpath='{.status.podIP}')

    if pg_isready -h ${NEW_PRIMARY_IP}; then
      echo "✓ Failover successful. New primary: ${BEST_REPLICA}"

      # Update service to point to new primary
      kubectl patch service postgres-primary -n ${NAMESPACE} -p \
        "{\"spec\":{\"selector\":{\"statefulset.kubernetes.io/pod-name\":\"${BEST_REPLICA}\"}}}"

      # Send alert
      curl -X POST http://alertmanager:9093/api/v1/alerts -d "[{
        \"labels\": {
          \"alertname\": \"PostgreSQLFailover\",
          \"severity\": \"critical\",
          \"component\": \"postgres\"
        },
        \"annotations\": {
          \"summary\": \"PostgreSQL failover completed\",
          \"description\": \"Promoted ${BEST_REPLICA} to primary\"
        }
      }]"

      exit 0
    else
      echo "❌ Failover failed. New primary is not responding."
      exit 1
    fi
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgres-failover-monitor
  namespace: llm-research-lab-data
spec:
  schedule: "*/1 * * * *"  # Every minute
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: postgres-operator
          restartPolicy: Never
          containers:
          - name: failover-monitor
            image: postgres:15-alpine
            command: ["/bin/bash", "/scripts/failover.sh"]
            volumeMounts:
            - name: failover-script
              mountPath: /scripts
          volumes:
          - name: failover-script
            configMap:
              name: postgres-failover-script
              defaultMode: 0755
```

### 6.3.3 Health Check Configuration

```yaml
# k8s/health-checks.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: health-check-config
  namespace: llm-research-lab-prod
data:
  # PostgreSQL Health Check
  postgres-health.sh: |
    #!/bin/bash
    set -e

    # Connection check
    pg_isready -h ${PGHOST} -p ${PGPORT} -U ${PGUSER} || exit 1

    # Replication lag check (for replicas)
    if [ "${ROLE}" == "replica" ]; then
      LAG=$(psql -U ${PGUSER} -t -c "SELECT EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp()));" | tr -d ' ')
      if (( $(echo "$LAG > 60" | bc -l) )); then
        echo "Replication lag too high: ${LAG}s"
        exit 1
      fi
    fi

    # Connection pool check
    CONNECTIONS=$(psql -U ${PGUSER} -t -c "SELECT count(*) FROM pg_stat_activity;" | tr -d ' ')
    MAX_CONNECTIONS=$(psql -U ${PGUSER} -t -c "SHOW max_connections;" | tr -d ' ')
    USAGE=$(echo "scale=2; ${CONNECTIONS}/${MAX_CONNECTIONS}*100" | bc)

    if (( $(echo "$USAGE > 90" | bc -l) )); then
      echo "Connection pool near capacity: ${USAGE}%"
      exit 1
    fi

    exit 0

  # ClickHouse Health Check
  clickhouse-health.sh: |
    #!/bin/bash
    set -e

    # Basic connectivity
    clickhouse-client --query "SELECT 1" || exit 1

    # Check replicas (if applicable)
    REPLICA_STATUS=$(clickhouse-client --query "SELECT count() FROM system.replicas WHERE is_session_expired = 1")
    if [ "${REPLICA_STATUS}" -gt 0 ]; then
      echo "Found ${REPLICA_STATUS} expired replica sessions"
      exit 1
    fi

    # Check merge queue
    MERGE_QUEUE=$(clickhouse-client --query "SELECT count() FROM system.merges")
    if [ "${MERGE_QUEUE}" -gt 100 ]; then
      echo "Merge queue too large: ${MERGE_QUEUE}"
      exit 1
    fi

    exit 0

  # Kafka Health Check
  kafka-health.sh: |
    #!/bin/bash
    set -e

    # Broker connectivity
    kafka-broker-api-versions.sh --bootstrap-server ${KAFKA_BOOTSTRAP} || exit 1

    # Check under-replicated partitions
    URP=$(kafka-topics.sh --bootstrap-server ${KAFKA_BOOTSTRAP} \
      --describe --under-replicated-partitions | wc -l)

    if [ "${URP}" -gt 0 ]; then
      echo "Found ${URP} under-replicated partitions"
      exit 1
    fi

    exit 0

  # Redis Health Check
  redis-health.sh: |
    #!/bin/bash
    set -e

    # Ping check
    redis-cli -h ${REDIS_HOST} PING | grep -q PONG || exit 1

    # Memory usage check
    MEMORY_USAGE=$(redis-cli -h ${REDIS_HOST} INFO memory | \
      grep used_memory_rss_human | cut -d: -f2 | tr -d '\r')

    echo "Memory usage: ${MEMORY_USAGE}"

    # Replication check (for replicas)
    if [ "${ROLE}" == "replica" ]; then
      MASTER_LINK=$(redis-cli -h ${REDIS_HOST} INFO replication | \
        grep master_link_status | cut -d: -f2 | tr -d '\r')

      if [ "${MASTER_LINK}" != "up" ]; then
        echo "Master link down"
        exit 1
      fi
    fi

    exit 0
---
# Liveness and Readiness Probe Examples
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-research-lab-api
  namespace: llm-research-lab-prod
spec:
  template:
    spec:
      containers:
      - name: api
        image: llm-research-lab-api:latest
        ports:
        - containerPort: 8000
          name: http
        livenessProbe:
          httpGet:
            path: /health/live
            port: 8000
            httpHeaders:
            - name: X-Health-Check
              value: liveness
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /health/ready
            port: 8000
            httpHeaders:
            - name: X-Health-Check
              value: readiness
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 2
        startupProbe:
          httpGet:
            path: /health/startup
            port: 8000
          initialDelaySeconds: 0
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 30  # 30 * 5s = 2.5 min startup time
```

---

## 6.4 Disaster Recovery Plan

### 6.4.1 DR Scenarios and Triggers

```yaml
# disaster-recovery-scenarios.yaml
scenarios:
  - id: DR-001
    name: Single AZ Failure
    trigger:
      - Multiple node failures in single AZ
      - AZ network partition
      - AWS AZ status page indicates outage
    severity: high
    rto: 1 hour
    rpo: 15 minutes
    response:
      - Automatic pod rescheduling to healthy AZs
      - Verify replica promotion for databases
      - Monitor cross-AZ traffic increase
      - Validate application functionality

  - id: DR-002
    name: Database Primary Failure
    trigger:
      - PostgreSQL primary unresponsive for 60s
      - Failed health checks (3 consecutive)
      - Replication lag exceeds 5 minutes
    severity: critical
    rto: 15 minutes
    rpo: 30 seconds
    response:
      - Execute automatic failover to replica
      - Update DNS/service endpoints
      - Verify data consistency
      - Rebuild failed primary as new replica

  - id: DR-003
    name: Data Corruption
    trigger:
      - Checksum validation failures
      - Application reports data inconsistencies
      - Failed integrity checks
    severity: critical
    rto: 4 hours
    rpo: 1 hour
    response:
      - Identify corruption scope
      - Restore from last known good backup
      - Run PITR to minimize data loss
      - Perform full integrity verification

  - id: DR-004
    name: Ransomware/Security Breach
    trigger:
      - Security alert from IDS/IPS
      - Unexplained file encryption
      - Unauthorized database access
    severity: critical
    rto: 8 hours
    rpo: 4 hours
    response:
      - Isolate affected systems immediately
      - Preserve forensic evidence
      - Restore from immutable backups
      - Rotate all credentials
      - Conduct security audit

  - id: DR-005
    name: Regional Outage
    trigger:
      - AWS region status page indicates major outage
      - Cannot reach services in multiple AZs
      - Loss of connectivity to control plane
    severity: critical
    rto: 4 hours
    rpo: 1 hour
    response:
      - Activate DR region
      - Restore latest backups to DR region
      - Update DNS to point to DR region
      - Notify stakeholders
      - Monitor DR region performance
```

### 6.4.2 Recovery Runbook Template

```markdown
# Disaster Recovery Runbook: [Scenario Name]

**Scenario ID**: [DR-XXX]
**RTO**: [X hours]
**RPO**: [X minutes/hours]
**Last Updated**: [Date]
**Owner**: [Team/Individual]

---

## 1. Detection & Alert

### Symptoms
- [List observable symptoms]
- [Include metric thresholds]
- [Log patterns]

### Alert Channels
- PagerDuty: [incident-type]
- Slack: #incidents
- Email: oncall@company.com

### Initial Assessment Checklist
- [ ] Confirm incident severity
- [ ] Identify affected components
- [ ] Estimate blast radius
- [ ] Page on-call engineer
- [ ] Create incident ticket

---

## 2. Immediate Response (First 15 Minutes)

### Stop the Bleeding
```bash
# Emergency commands to stabilize system
# Example: Stop write traffic, enable read-only mode
```

### Communication
- [ ] Post in #incidents channel
- [ ] Notify stakeholders
- [ ] Update status page

### Data Preservation
- [ ] Capture logs
- [ ] Take system snapshots
- [ ] Document timeline

---

## 3. Recovery Procedure

### Step 1: [Action Name]
**Duration**: [X minutes]
**Owner**: [Role]

```bash
# Exact commands to execute
```

**Validation**:
- [ ] [Check 1]
- [ ] [Check 2]

### Step 2: [Action Name]
**Duration**: [X minutes]
**Owner**: [Role]

```bash
# Exact commands to execute
```

**Validation**:
- [ ] [Check 1]
- [ ] [Check 2]

[Continue for all steps...]

---

## 4. Verification

### Functional Tests
```bash
# Smoke tests to verify service functionality
./scripts/smoke-tests.sh
```

### Data Integrity
```bash
# Data verification commands
./scripts/verify-data-integrity.sh
```

### Performance Baseline
- [ ] API latency < 100ms p99
- [ ] Database connections healthy
- [ ] Message queue no backlog
- [ ] Cache hit rate > 80%

---

## 5. Post-Recovery

### Monitoring
- [ ] Enable enhanced monitoring (30 minutes)
- [ ] Watch for anomalies
- [ ] Monitor error rates

### Cleanup
- [ ] Remove temporary fixes
- [ ] Clean up failed pods/containers
- [ ] Archive logs

### Communication
- [ ] Send all-clear notification
- [ ] Update status page
- [ ] Schedule post-mortem

---

## 6. Rollback Procedure

If recovery fails:

```bash
# Rollback commands
```

---

## 7. Contacts

| Role | Name | Contact |
|------|------|---------|
| Incident Commander | [Name] | [Phone/Slack] |
| Database DRI | [Name] | [Phone/Slack] |
| Infrastructure Lead | [Name] | [Phone/Slack] |
| Engineering Manager | [Name] | [Phone/Slack] |

---

## 8. Related Resources

- Architecture Diagram: [Link]
- Monitoring Dashboard: [Link]
- Previous Incidents: [Links]
```

### 6.4.3 DR Testing Schedule

```yaml
# k8s/dr-testing-schedule.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: dr-testing-schedule
  namespace: llm-research-lab-prod
data:
  schedule.yaml: |
    # Quarterly DR Testing Schedule

    # Q1 - January
    - month: January
      week: 3
      test_type: Database Failover
      scope: PostgreSQL Primary Failover
      duration: 2 hours
      participants:
        - Database Team
        - Platform Team
      objectives:
        - Verify automatic failover within RTO
        - Validate replica promotion
        - Test monitoring and alerting
        - Document actual recovery time

    # Q1 - February
    - month: February
      week: 3
      test_type: Backup Restore
      scope: Full PostgreSQL PITR
      duration: 4 hours
      participants:
        - Database Team
        - Application Team
      objectives:
        - Restore from 24-hour-old backup
        - Verify data integrity
        - Test recovery procedures
        - Validate RPO compliance

    # Q1 - March
    - month: March
      week: 3
      test_type: AZ Failure Simulation
      scope: Drain single availability zone
      duration: 3 hours
      participants:
        - Platform Team
        - SRE Team
      objectives:
        - Simulate AZ outage
        - Verify pod rescheduling
        - Test cross-AZ failover
        - Monitor performance impact

    # Q2 - April
    - month: April
      week: 3
      test_type: ClickHouse Recovery
      scope: Full ClickHouse restore from backup
      duration: 3 hours
      participants:
        - Data Platform Team
        - Analytics Team
      objectives:
        - Restore ClickHouse from S3 backup
        - Verify table schemas and data
        - Test query performance post-restore
        - Validate backup integrity

    # Q2 - May
    - month: May
      week: 3
      test_type: Redis Failover
      scope: Redis master failover
      duration: 1 hour
      participants:
        - Cache Team
        - Application Team
      objectives:
        - Trigger Redis Sentinel failover
        - Verify cache hit rates post-failover
        - Test application resilience
        - Document failover timing

    # Q2 - June
    - month: June
      week: 3
      test_type: Full System DR
      scope: Complete regional failover
      duration: 8 hours
      participants:
        - All Engineering Teams
        - Management
      objectives:
        - Simulate complete region failure
        - Activate DR region
        - Restore all services
        - Full end-to-end testing
        - Update DR playbooks

    # Q3 - July
    - month: July
      week: 3
      test_type: Kafka Recovery
      scope: Kafka cluster rebuild
      duration: 3 hours
      participants:
        - Streaming Team
        - Platform Team
      objectives:
        - Restore Kafka metadata
        - Verify topic configurations
        - Test consumer group offsets
        - Validate message delivery

    # Q3 - August
    - month: August
      week: 3
      test_type: Security Incident Response
      scope: Simulated ransomware attack
      duration: 4 hours
      participants:
        - Security Team
        - Incident Response Team
      objectives:
        - Test isolation procedures
        - Restore from immutable backups
        - Validate credential rotation
        - Review forensic processes

    # Q3 - September
    - month: September
      week: 3
      test_type: Data Corruption Recovery
      scope: Restore from corrupted state
      duration: 4 hours
      participants:
        - Database Team
        - Application Team
      objectives:
        - Simulate data corruption
        - Execute PITR to pre-corruption state
        - Run integrity verification
        - Test detection mechanisms

    # Q4 - October
    - month: October
      week: 3
      test_type: Network Partition
      scope: Simulate network split-brain
      duration: 2 hours
      participants:
        - Network Team
        - Platform Team
      objectives:
        - Simulate network partition
        - Test split-brain prevention
        - Verify quorum behavior
        - Validate fencing mechanisms

    # Q4 - November
    - month: November
      week: 3
      test_type: Capacity Failure
      scope: Storage exhaustion recovery
      duration: 2 hours
      participants:
        - Storage Team
        - Platform Team
      objectives:
        - Simulate disk full condition
        - Test automatic cleanup
        - Verify volume expansion
        - Test monitoring alerts

    # Q4 - December
    - month: December
      week: 2  # Earlier in month due to holidays
      test_type: Year-End Full DR Drill
      scope: Complete DR validation
      duration: 8 hours
      participants:
        - All Teams
        - Executive Stakeholders
      objectives:
        - Full regional failover
        - Restore all data stores
        - End-to-end application testing
        - Update all DR documentation
        - Year-end compliance review
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: dr-test-reminder
  namespace: llm-research-lab-prod
spec:
  schedule: "0 9 1 * *"  # First day of each month at 9 AM
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: Never
          containers:
          - name: reminder
            image: curlimages/curl:latest
            command:
            - /bin/sh
            - -c
            - |
              MONTH=$(date +%B)

              curl -X POST https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK \
                -H 'Content-Type: application/json' \
                -d "{
                  \"text\": \"📋 *DR Testing Reminder*\",
                  \"blocks\": [
                    {
                      \"type\": \"section\",
                      \"text\": {
                        \"type\": \"mrkdwn\",
                        \"text\": \"DR testing for *${MONTH}* is scheduled for week 3. Please review the DR testing schedule and prepare.\"
                      }
                    },
                    {
                      \"type\": \"section\",
                      \"text\": {
                        \"type\": \"mrkdwn\",
                        \"text\": \"📘 <https://docs.company.com/dr-testing|DR Testing Guide>\"
                      }
                    }
                  ]
                }"
```

---

## 6.5 Business Continuity

### 6.5.1 Service Degradation Procedures

```yaml
# service-degradation-procedures.yaml
degradation_modes:
  # Level 1: Minor Degradation
  - level: 1
    name: Read-Only Mode
    triggers:
      - Database primary failure (during failover)
      - High write latency (>500ms p99)
      - Storage approaching capacity (>90%)
    actions:
      enable_read_only: true
      disable_features:
        - experiment_creation
        - parameter_updates
        - user_registration
      keep_enabled:
        - experiment_viewing
        - result_querying
        - dashboard_access
    implementation: |
      # Enable read-only mode
      kubectl patch configmap app-config -n llm-research-lab-prod \
        -p '{"data":{"READ_ONLY_MODE":"true"}}'

      # Restart pods to pick up new config
      kubectl rollout restart deployment llm-research-lab-api \
        -n llm-research-lab-prod

      # Update status page
      curl -X POST https://status.company.com/api/incidents \
        -H "Authorization: Bearer $TOKEN" \
        -d '{"status": "investigating", "message": "Write operations temporarily disabled"}'

  # Level 2: Moderate Degradation
  - level: 2
    name: Essential Services Only
    triggers:
      - Multiple component failures
      - Severe performance degradation
      - Approaching resource limits
    actions:
      disable_features:
        - experiment_creation
        - batch_processing
        - report_generation
        - background_jobs
        - non-critical_apis
      keep_enabled:
        - authentication
        - experiment_viewing
        - critical_dashboards
    implementation: |
      # Scale down non-essential services
      kubectl scale deployment experiment-runner --replicas=0 -n llm-research-lab-prod
      kubectl scale deployment report-generator --replicas=0 -n llm-research-lab-prod
      kubectl scale deployment background-worker --replicas=0 -n llm-research-lab-prod

      # Enable circuit breakers
      kubectl apply -f - <<EOF
      apiVersion: networking.istio.io/v1beta1
      kind: DestinationRule
      metadata:
        name: circuit-breaker
        namespace: llm-research-lab-prod
      spec:
        host: llm-research-lab-api
        trafficPolicy:
          outlierDetection:
            consecutiveErrors: 5
            interval: 30s
            baseEjectionTime: 30s
      EOF

  # Level 3: Severe Degradation
  - level: 3
    name: Emergency Maintenance Mode
    triggers:
      - Critical security incident
      - Data corruption detected
      - Cascading failures
    actions:
      disable_all_writes: true
      enable_maintenance_page: true
      preserve_read_access: limited
    implementation: |
      # Enable maintenance mode
      kubectl apply -f - <<EOF
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: maintenance-mode
        namespace: llm-research-lab-prod
      data:
        enabled: "true"
        message: "System under emergency maintenance. Expected recovery: [TIME]"
      EOF

      # Deploy maintenance page
      kubectl apply -f k8s/maintenance-page.yaml

      # Notify all users
      ./scripts/send-maintenance-notification.sh
```

### 6.5.2 Recovery Priority Matrix

```yaml
# recovery-priority-matrix.yaml
recovery_priorities:
  # Priority 1: Critical (RTO < 30 minutes)
  - priority: P1
    description: Core authentication and data access
    components:
      - name: Authentication Service
        rto: 15 minutes
        rpo: 5 minutes
        dependencies:
          - PostgreSQL Primary
          - Redis Cache
        recovery_steps:
          - Verify database connectivity
          - Restore Redis cache
          - Validate OAuth flows
          - Test login functionality

      - name: PostgreSQL Primary
        rto: 15 minutes
        rpo: 30 seconds
        dependencies: []
        recovery_steps:
          - Execute automatic failover
          - Promote replica to primary
          - Update service endpoints
          - Verify replication

      - name: API Gateway
        rto: 10 minutes
        rpo: 0 (stateless)
        dependencies:
          - Load Balancer
          - Authentication Service
        recovery_steps:
          - Verify pod health
          - Scale up if needed
          - Test routing rules
          - Validate SSL certificates

  # Priority 2: High (RTO < 1 hour)
  - priority: P2
    description: Core experiment functionality
    components:
      - name: Experiment API
        rto: 30 minutes
        rpo: 5 minutes
        dependencies:
          - PostgreSQL
          - Kafka
          - Redis
        recovery_steps:
          - Restore database connection
          - Verify Kafka connectivity
          - Test experiment CRUD
          - Validate result storage

      - name: ClickHouse Analytics
        rto: 1 hour
        rpo: 15 minutes
        dependencies:
          - Storage volumes
        recovery_steps:
          - Restore from backup if needed
          - Verify table integrity
          - Test query performance
          - Rebuild materialized views

      - name: Message Queue (Kafka)
        rto: 45 minutes
        rpo: 10 minutes
        dependencies:
          - ZooKeeper
        recovery_steps:
          - Restore broker configuration
          - Verify topic health
          - Check consumer groups
          - Test message flow

  # Priority 3: Medium (RTO < 4 hours)
  - priority: P3
    description: Analytics and reporting
    components:
      - name: Report Generator
        rto: 2 hours
        rpo: 1 hour
        dependencies:
          - ClickHouse
          - PostgreSQL
        recovery_steps:
          - Restore service deployment
          - Verify data access
          - Test report generation
          - Validate export functionality

      - name: Batch Processing
        rto: 4 hours
        rpo: 2 hours
        dependencies:
          - Kafka
          - ClickHouse
        recovery_steps:
          - Restore worker pods
          - Verify queue connectivity
          - Resume processing
          - Monitor backlog

  # Priority 4: Low (RTO < 24 hours)
  - priority: P4
    description: Nice-to-have features
    components:
      - name: Email Notifications
        rto: 8 hours
        rpo: 24 hours
        dependencies:
          - SMTP Service
        recovery_steps:
          - Restore service
          - Test email delivery
          - Process queued emails

      - name: Background Jobs
        rto: 24 hours
        rpo: 24 hours
        dependencies:
          - Redis
          - PostgreSQL
        recovery_steps:
          - Restore job scheduler
          - Verify job definitions
          - Resume processing
```

### 6.5.3 Communication Templates

```markdown
# Communication Templates

## Initial Incident Notification

**Subject**: [SEVERITY] Incident: [Brief Description]

Team,

We are currently experiencing [brief description of the issue].

**Impact**: [User-facing impact]
**Affected Services**: [List of services]
**Current Status**: [Investigating/Identified/Monitoring/Resolved]
**Estimated Resolution**: [Time or "Unknown - will update in X minutes"]

We will provide updates every [frequency].

Status Page: https://status.company.com/incidents/[ID]
Incident Channel: #incident-[ID]

---

## Update Notification

**Subject**: UPDATE - [SEVERITY] Incident: [Brief Description]

Team,

**Status Update** - [Timestamp]

**Progress**: [What we've done]
**Current State**: [Where we are now]
**Next Steps**: [What we're doing next]
**Expected Resolution**: [Updated ETA]

Previous updates: [Link to status page]

---

## Resolution Notification

**Subject**: RESOLVED - [SEVERITY] Incident: [Brief Description]

Team,

The incident affecting [services] has been resolved as of [timestamp].

**Root Cause**: [Brief explanation]
**Resolution**: [What was done to fix it]
**Preventive Measures**: [What we're doing to prevent recurrence]

**Timeline**:
- [Timestamp]: Issue detected
- [Timestamp]: Response initiated
- [Timestamp]: Fix implemented
- [Timestamp]: Service restored
- [Timestamp]: Verification complete

**Post-Mortem**: We will conduct a blameless post-mortem and share findings by [date].

Thank you for your patience.

---

## External/Customer Communication

**Subject**: Service Disruption Notice - [Date]

Dear Valued Customers,

We experienced a service disruption on [date] from [start time] to [end time].

**What Happened**: [Non-technical explanation]
**Impact**: [What users experienced]
**Resolution**: [What we did to fix it]
**Prevention**: [Steps we're taking to prevent this]

We sincerely apologize for any inconvenience this may have caused. If you have any questions or concerns, please contact our support team.

Best regards,
[Company] Engineering Team
```

---

## Appendix A: Recovery Scripts

### A.1 Emergency Database Recovery Script

```bash
#!/bin/bash
# scripts/emergency-db-recovery.sh
# Emergency PostgreSQL Recovery - Run under supervision

set -e

usage() {
  cat <<EOF
Emergency PostgreSQL Recovery Script

Usage: $0 [OPTIONS]

Options:
  -m, --mode          Recovery mode: failover|restore|rebuild
  -t, --target-time   Target time for PITR (ISO 8601 format)
  -b, --backup-name   Specific backup name to restore
  -f, --force         Skip confirmations (USE WITH CAUTION)
  -h, --help          Show this help message

Examples:
  # Automatic failover to replica
  $0 --mode failover

  # Point-in-time recovery
  $0 --mode restore --target-time "2024-01-15 14:30:00+00"

  # Restore specific backup
  $0 --mode restore --backup-name "base_20240115_143000"

EOF
  exit 1
}

# Parse arguments
MODE=""
TARGET_TIME=""
BACKUP_NAME=""
FORCE=false

while [[ $# -gt 0 ]]; do
  case $1 in
    -m|--mode)
      MODE="$2"
      shift 2
      ;;
    -t|--target-time)
      TARGET_TIME="$2"
      shift 2
      ;;
    -b|--backup-name)
      BACKUP_NAME="$2"
      shift 2
      ;;
    -f|--force)
      FORCE=true
      shift
      ;;
    -h|--help)
      usage
      ;;
    *)
      echo "Unknown option: $1"
      usage
      ;;
  esac
done

# Validate mode
if [[ ! "$MODE" =~ ^(failover|restore|rebuild)$ ]]; then
  echo "Error: Invalid mode. Must be failover, restore, or rebuild"
  usage
fi

# Confirmation
if [ "$FORCE" != true ]; then
  echo "⚠️  WARNING: This script will perform emergency database recovery."
  echo "Mode: $MODE"
  echo "Target Time: ${TARGET_TIME:-latest}"
  echo "Backup Name: ${BACKUP_NAME:-auto-select}"
  echo ""
  read -p "Are you sure you want to continue? (type 'YES' to confirm): " confirm

  if [ "$confirm" != "YES" ]; then
    echo "Aborted."
    exit 0
  fi
fi

# Logging
LOG_FILE="/var/log/postgres-recovery-$(date +%Y%m%d_%H%M%S).log"
exec 1> >(tee -a "$LOG_FILE")
exec 2>&1

echo "=== Emergency PostgreSQL Recovery ==="
echo "Start Time: $(date -u +%Y-%m-%dT%H:%M:%SZ)"
echo "Mode: $MODE"
echo "Log File: $LOG_FILE"
echo ""

# Mode-specific recovery
case $MODE in
  failover)
    echo "Initiating automatic failover..."
    /scripts/postgres-failover.sh
    ;;

  restore)
    echo "Initiating point-in-time recovery..."
    if [ -n "$BACKUP_NAME" ]; then
      /scripts/postgres-pitr-recovery.sh "$BACKUP_NAME"
    elif [ -n "$TARGET_TIME" ]; then
      /scripts/postgres-pitr-recovery.sh "$TARGET_TIME"
    else
      /scripts/postgres-pitr-recovery.sh "latest"
    fi
    ;;

  rebuild)
    echo "Rebuilding database from latest backup..."
    /scripts/postgres-rebuild.sh
    ;;
esac

# Post-recovery verification
echo ""
echo "=== Post-Recovery Verification ==="
/scripts/verify-data-integrity.sh

# Notify team
echo ""
echo "=== Sending Notifications ==="
curl -X POST https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK \
  -H 'Content-Type: application/json' \
  -d "{
    \"text\": \"🚨 Emergency database recovery completed\",
    \"blocks\": [
      {
        \"type\": \"section\",
        \"text\": {
          \"type\": \"mrkdwn\",
          \"text\": \"*Mode*: $MODE\n*Status*: Success\n*Duration*: $SECONDS seconds\n*Log*: \`$LOG_FILE\`\"
        }
      }
    ]
  }"

echo ""
echo "=== Recovery Complete ==="
echo "End Time: $(date -u +%Y-%m-%dT%H:%M:%SZ)"
echo "Duration: $SECONDS seconds"
echo "Log File: $LOG_FILE"
```

### A.2 Full System Health Check Script

```bash
#!/bin/bash
# scripts/full-system-health-check.sh
# Comprehensive system health verification

set -e

RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

ERRORS=0
WARNINGS=0

check_component() {
  local component=$1
  local check_cmd=$2

  echo -n "Checking ${component}... "

  if eval "$check_cmd" &>/dev/null; then
    echo -e "${GREEN}✓${NC}"
  else
    echo -e "${RED}✗${NC}"
    ((ERRORS++))
  fi
}

check_metric() {
  local component=$1
  local metric=$2
  local threshold=$3
  local operator=$4
  local current=$5

  echo -n "  ${metric}: ${current} "

  if (( $(echo "$current $operator $threshold" | bc -l) )); then
    echo -e "${GREEN}✓${NC}"
  else
    echo -e "${YELLOW}⚠${NC}"
    ((WARNINGS++))
  fi
}

echo "=== Full System Health Check ==="
echo "Timestamp: $(date -u +%Y-%m-%dT%H:%M:%SZ)"
echo ""

# Kubernetes Health
echo "[Kubernetes Cluster]"
check_component "API Server" "kubectl cluster-info"
check_component "Node Readiness" "kubectl get nodes | grep -v NotReady"

NODE_COUNT=$(kubectl get nodes --no-headers | wc -l)
READY_NODES=$(kubectl get nodes --no-headers | grep " Ready" | wc -l)
check_metric "Kubernetes" "Ready Nodes" "${NODE_COUNT}" "==" "${READY_NODES}"

# PostgreSQL Health
echo ""
echo "[PostgreSQL]"
check_component "Primary Connectivity" "pg_isready -h postgres-primary"
check_component "Replica Connectivity" "pg_isready -h postgres-replica"

REPLICATION_LAG=$(psql -h postgres-replica -U postgres -t -c "SELECT EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp()));" | tr -d ' ')
check_metric "PostgreSQL" "Replication Lag (sec)" "60" "<" "${REPLICATION_LAG:-0}"

CONNECTION_COUNT=$(psql -h postgres-primary -U postgres -t -c "SELECT count(*) FROM pg_stat_activity;" | tr -d ' ')
check_metric "PostgreSQL" "Active Connections" "100" "<" "${CONNECTION_COUNT}"

# ClickHouse Health
echo ""
echo "[ClickHouse]"
check_component "Connectivity" "clickhouse-client --query 'SELECT 1'"

MERGE_COUNT=$(clickhouse-client --query "SELECT count() FROM system.merges" 2>/dev/null || echo "0")
check_metric "ClickHouse" "Active Merges" "50" "<" "${MERGE_COUNT}"

# Kafka Health
echo ""
echo "[Kafka]"
check_component "Broker Connectivity" "kafka-broker-api-versions.sh --bootstrap-server kafka:9092"

URP_COUNT=$(kafka-topics.sh --bootstrap-server kafka:9092 --describe --under-replicated-partitions 2>/dev/null | wc -l)
check_metric "Kafka" "Under-Replicated Partitions" "0" "==" "${URP_COUNT}"

# Redis Health
echo ""
echo "[Redis]"
check_component "Master Connectivity" "redis-cli -h redis-master PING | grep PONG"
check_component "Replica Connectivity" "redis-cli -h redis-replica PING | grep PONG"

REDIS_MEMORY=$(redis-cli -h redis-master INFO memory | grep used_memory_human | cut -d: -f2 | tr -d '\r')
echo "  Memory Usage: ${REDIS_MEMORY}"

# Application Health
echo ""
echo "[Application Services]"
check_component "API Server" "curl -sf http://llm-research-lab-api:8000/health"
check_component "Experiment Runner" "curl -sf http://experiment-runner:8001/health"

# Backup Status
echo ""
echo "[Backup Status]"
LAST_PG_BACKUP=$(aws s3 ls s3://llm-research-lab-postgres-backups/wal-archive/ --recursive | sort | tail -1 | awk '{print $1" "$2}')
BACKUP_AGE=$(($(date +%s) - $(date -d "$LAST_PG_BACKUP" +%s)))
check_metric "Backups" "Last PostgreSQL Backup Age (hours)" "24" "<" "$((BACKUP_AGE / 3600))"

# Summary
echo ""
echo "=== Health Check Summary ==="
echo -e "Errors: ${RED}${ERRORS}${NC}"
echo -e "Warnings: ${YELLOW}${WARNINGS}${NC}"

if [ $ERRORS -eq 0 ] && [ $WARNINGS -eq 0 ]; then
  echo -e "${GREEN}All systems operational${NC}"
  exit 0
elif [ $ERRORS -eq 0 ]; then
  echo -e "${YELLOW}Systems operational with warnings${NC}"
  exit 0
else
  echo -e "${RED}System health check failed${NC}"
  exit 1
fi
```

---

## Appendix B: Runbook Templates

### B.1 Database Failover Runbook

```markdown
# PostgreSQL Failover Runbook

**Runbook ID**: RB-DB-001
**Service**: PostgreSQL
**RTO**: 15 minutes
**RPO**: 30 seconds
**Last Updated**: 2024-01-15
**Owner**: Database Team

---

## Prerequisites

- [ ] Access to Kubernetes cluster
- [ ] Database credentials
- [ ] PagerDuty incident created
- [ ] #incidents Slack channel open

---

## Detection

### Symptoms
- Primary database health check failing
- Connection timeout errors in application logs
- Prometheus alert: `PostgreSQLDown`
- Replication lag increasing rapidly

### Verification Commands

```bash
# Check primary status
kubectl get pod postgres-primary-0 -n llm-research-lab-data

# Check primary logs
kubectl logs postgres-primary-0 -n llm-research-lab-data --tail=50

# Test connectivity
pg_isready -h postgres-primary.llm-research-lab-data.svc.cluster.local
```

---

## Response Procedure

### Step 1: Confirm Primary Failure (2 minutes)

```bash
# Attempt connection
psql -h postgres-primary -U postgres -c "SELECT 1"

# Check pod status
kubectl describe pod postgres-primary-0 -n llm-research-lab-data

# Review recent events
kubectl get events -n llm-research-lab-data --sort-by='.lastTimestamp' | head -20
```

**Decision Point**: If primary is truly down, proceed. If recoverable, attempt restart first.

### Step 2: Enable Read-Only Mode (1 minute)

```bash
# Prevent new writes
kubectl patch configmap app-config -n llm-research-lab-prod \
  -p '{"data":{"READ_ONLY_MODE":"true"}}'

# Restart API to pick up config
kubectl rollout restart deployment llm-research-lab-api -n llm-research-lab-prod
```

### Step 3: Select Promotion Candidate (2 minutes)

```bash
# List replicas
kubectl get pods -l app=postgres,role=replica -n llm-research-lab-data

# Check LSN for each replica
for pod in $(kubectl get pods -l role=replica -n llm-research-lab-data -o name); do
  echo "=== $pod ==="
  kubectl exec -n llm-research-lab-data $pod -- \
    psql -U postgres -t -c "SELECT pg_last_wal_receive_lsn();"
done
```

Select replica with highest LSN.

### Step 4: Promote Replica (5 minutes)

```bash
REPLICA_POD="postgres-replica-0"  # Replace with selected pod

# Promote replica
kubectl exec -n llm-research-lab-data ${REPLICA_POD} -- \
  pg_ctl promote -D /var/lib/postgresql/data

# Wait for promotion
sleep 10

# Verify promotion
kubectl exec -n llm-research-lab-data ${REPLICA_POD} -- \
  psql -U postgres -c "SELECT pg_is_in_recovery();"  # Should return 'f'
```

### Step 5: Update Service (2 minutes)

```bash
# Get new primary IP
NEW_PRIMARY_IP=$(kubectl get pod ${REPLICA_POD} -n llm-research-lab-data -o jsonpath='{.status.podIP}')

# Update service to point to new primary
kubectl patch service postgres-primary -n llm-research-lab-data -p \
  "{\"spec\":{\"selector\":{\"statefulset.kubernetes.io/pod-name\":\"${REPLICA_POD}\"}}}"

# Verify service
kubectl get service postgres-primary -n llm-research-lab-data -o yaml
```

### Step 6: Verify Functionality (2 minutes)

```bash
# Test write operation
psql -h postgres-primary.llm-research-lab-data.svc.cluster.local \
  -U postgres -c "CREATE TABLE failover_test (id SERIAL PRIMARY KEY, ts TIMESTAMP DEFAULT NOW());"

# Verify
psql -h postgres-primary.llm-research-lab-data.svc.cluster.local \
  -U postgres -c "SELECT * FROM failover_test;"

# Cleanup
psql -h postgres-primary.llm-research-lab-data.svc.cluster.local \
  -U postgres -c "DROP TABLE failover_test;"
```

### Step 7: Disable Read-Only Mode (1 minute)

```bash
# Re-enable writes
kubectl patch configmap app-config -n llm-research-lab-prod \
  -p '{"data":{"READ_ONLY_MODE":"false"}}'

# Restart API
kubectl rollout restart deployment llm-research-lab-api -n llm-research-lab-prod
```

---

## Post-Failover

### Monitoring (30 minutes)

- [ ] Watch error rates
- [ ] Monitor connection pool
- [ ] Verify replication (if other replicas exist)
- [ ] Check application logs

### Cleanup

```bash
# Remove failed primary pod
kubectl delete pod postgres-primary-0 -n llm-research-lab-data

# Rebuild as new replica (if needed)
# Follow replica rebuild procedure
```

### Communication

```bash
# Send all-clear
curl -X POST https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK \
  -d '{"text":"✅ PostgreSQL failover completed. Service restored."}'
```

---

## Rollback

If failover fails:

```bash
# Revert service to point to original primary (if it recovers)
kubectl patch service postgres-primary -n llm-research-lab-data -p \
  "{\"spec\":{\"selector\":{\"statefulset.kubernetes.io/pod-name\":\"postgres-primary-0\"}}}"
```

---

## Contacts

| Role | Contact |
|------|---------|
| Database DRI | @db-oncall |
| Platform Lead | @platform-lead |
| Engineering Manager | @eng-manager |

---

## Related Resources

- [PostgreSQL Architecture](https://docs.company.com/architecture/postgres)
- [Grafana Dashboard](https://grafana.company.com/d/postgres)
- [Previous Incidents](https://docs.company.com/incidents/postgres)
```

---

## Appendix C: DR Testing Checklist

```markdown
# DR Testing Checklist

**Test Date**: __________
**Test Type**: __________
**Test Lead**: __________
**Participants**: __________

---

## Pre-Test Preparation

### Documentation
- [ ] Review DR plan and procedures
- [ ] Identify test objectives
- [ ] Define success criteria
- [ ] Prepare rollback procedures
- [ ] Schedule test window

### Communication
- [ ] Notify stakeholders
- [ ] Schedule team meeting
- [ ] Create test channel (#dr-test-YYYY-MM-DD)
- [ ] Prepare status updates

### Environment
- [ ] Verify backup status
- [ ] Check monitoring systems
- [ ] Prepare test environment (if applicable)
- [ ] Document current state

---

## During Test

### Execution
- [ ] Start recording (screen capture if needed)
- [ ] Document start time
- [ ] Follow runbook precisely
- [ ] Record all commands executed
- [ ] Note any deviations from plan
- [ ] Capture screenshots of key steps

### Monitoring
- [ ] Monitor system metrics
- [ ] Watch for unexpected issues
- [ ] Track recovery time
- [ ] Verify data integrity
- [ ] Test functionality

### Communication
- [ ] Provide regular updates
- [ ] Document issues encountered
- [ ] Note team collaboration effectiveness

---

## Post-Test

### Verification
- [ ] All services operational
- [ ] Data integrity confirmed
- [ ] Performance within SLA
- [ ] No data loss detected
- [ ] All functionality working

### Cleanup
- [ ] Restore original state (if needed)
- [ ] Clean up test artifacts
- [ ] Archive logs and recordings

### Documentation
- [ ] Calculate actual RTO achieved
- [ ] Calculate actual RPO achieved
- [ ] Document lessons learned
- [ ] Identify procedure improvements
- [ ] Update runbooks if needed

### Reporting
- [ ] Complete test report
- [ ] Share findings with team
- [ ] Update DR documentation
- [ ] Schedule follow-up actions

---

## Test Results

### Metrics

| Metric | Target | Actual | Pass/Fail |
|--------|--------|--------|-----------|
| RTO | ________ | ________ | ________ |
| RPO | ________ | ________ | ________ |
| Data Integrity | 100% | ________ | ________ |

### Success Criteria

- [ ] Met RTO target
- [ ] Met RPO target
- [ ] No data loss
- [ ] All services restored
- [ ] Runbook accuracy confirmed

### Issues Encountered

1. __________________________________________
2. __________________________________________
3. __________________________________________

### Action Items

| Action | Owner | Due Date | Status |
|--------|-------|----------|--------|
| ________ | ________ | ________ | ________ |
| ________ | ________ | ________ | ________ |

---

## Sign-Off

**Test Lead**: __________________ Date: __________
**Manager**: __________________ Date: __________

**Overall Result**: [ ] Pass [ ] Pass with Issues [ ] Fail

**Next Test Date**: __________
```

---

## Document Control

| Version | Date | Author | Changes |
|---------|------|--------|---------|
| 1.0 | 2024-01-15 | Platform Team | Initial release |

**Review Schedule**: Quarterly
**Next Review**: 2024-04-15
**Owner**: Platform Team / SRE
**Approved By**: Engineering Leadership

---

**End of Document**
---

## Handoff & Knowledge Transfer


## Table of Contents

1. [Overview](#overview)
2. [7.1 Operations Training Program](#71-operations-training-program)
3. [7.2 Documentation Package](#72-documentation-package)
4. [7.3 Knowledge Transfer Sessions](#73-knowledge-transfer-sessions)
5. [7.4 Handoff Checklist](#74-handoff-checklist)
6. [7.5 Support Transition](#75-support-transition)
7. [Appendix A: Training Templates](#appendix-a-training-templates)
8. [Appendix B: Certification Criteria](#appendix-b-certification-criteria)

---

## Overview

This section defines the comprehensive handoff and knowledge transfer process for transitioning LLM-Research-Lab from development to operations. The goal is to ensure operations teams have complete understanding, documentation, and capabilities to maintain 99.9% SLA.

### Transfer Objectives

1. **Complete Knowledge Transfer**: Operations team fully understands system architecture, dependencies, and operational procedures
2. **Documentation Completeness**: All runbooks, architecture docs, and troubleshooting guides are production-ready
3. **Operational Readiness**: Support team certified and equipped to handle incidents independently
4. **Continuous Support**: Seamless transition with shadow period and escalation paths established

### Success Criteria

| Metric | Target | Validation Method |
|--------|--------|------------------|
| **Training Completion Rate** | 100% | All operations staff complete certification |
| **Documentation Coverage** | 100% | All components have runbooks and architecture docs |
| **Incident Response Time** | <15 min | Shadow on-call successfully handles test incidents |
| **Handoff Approval** | Sign-off | Operations manager approves readiness |

---

## 7.1 Operations Training Program

### 7.1.1 Training Curriculum Outline

#### Module 1: System Architecture Overview (4 hours)

**Learning Objectives:**
- Understand high-level architecture of LLM-Research-Lab
- Identify core components and their interactions
- Map data flows between services
- Recognize integration points with LLM DevOps ecosystem

**Topics:**
1. **System Architecture** (60 min)
   - Component overview: API, Experiment Runner, Workers, Storage
   - Database architecture: PostgreSQL, ClickHouse, Redis
   - Message queue architecture: Kafka topics and consumer groups
   - Integration: LLM-Registry, LLM-Data-Vault, LLM-Analytics-Hub, LLM-Test-Bench

2. **Infrastructure Components** (60 min)
   - Kubernetes cluster architecture and namespaces
   - Load balancing and ingress configuration
   - Storage systems: S3-compatible object storage, persistent volumes
   - Network policies and security boundaries

3. **Data Flow Patterns** (60 min)
   - Experiment submission workflow
   - Dataset versioning and retrieval
   - Metrics collection and aggregation
   - Result storage and retrieval

4. **Hands-On Lab** (60 min)
   - Navigate Kubernetes cluster
   - Inspect running services
   - Query databases
   - Review Kafka topics

**Deliverables:**
- Architecture diagram walkthrough
- Component interaction mapping exercise
- Quiz: 20 questions (80% pass rate required)

---

#### Module 2: Operational Procedures (6 hours)

**Learning Objectives:**
- Execute standard operational tasks
- Follow runbook procedures accurately
- Use monitoring and observability tools
- Perform basic troubleshooting

**Topics:**
1. **Monitoring and Observability** (90 min)
   - Grafana dashboard navigation
   - Prometheus query basics (PromQL)
   - Log aggregation with Loki
   - Distributed tracing with Jaeger
   - Alert interpretation and triage

2. **Routine Operations** (90 min)
   - Service health checks
   - Database maintenance tasks
   - Cache management (Redis)
   - Kafka topic management
   - Backup verification

3. **Deployment Procedures** (90 min)
   - Rolling updates and canary deployments
   - Database migration execution
   - Configuration changes
   - Rollback procedures

4. **Hands-On Lab** (90 min)
   - Execute health check runbook
   - Perform simulated deployment
   - Investigate mock alert in Grafana
   - Execute rollback procedure

**Deliverables:**
- Runbook execution checklist
- Monitoring dashboard customization
- Practical exam: Complete 5 operational tasks

---

#### Module 3: Incident Response (4 hours)

**Learning Objectives:**
- Respond to production incidents effectively
- Use troubleshooting guides to diagnose issues
- Execute escalation procedures appropriately
- Document incidents for post-mortem analysis

**Topics:**
1. **Incident Response Framework** (60 min)
   - Incident severity classification (P0-P4)
   - Alert triage and prioritization
   - Communication protocols (Slack, PagerDuty, status page)
   - Incident command structure

2. **Common Failure Scenarios** (90 min)
   - Database connection pool exhaustion
   - Kafka consumer lag buildup
   - API latency spikes
   - Out of memory errors
   - Disk space exhaustion
   - Network connectivity issues

3. **Troubleshooting Methodology** (60 min)
   - Structured problem-solving approach
   - Log analysis techniques
   - Metrics correlation
   - Root cause analysis

4. **Tabletop Exercise** (30 min)
   - Simulated P1 incident: API latency spike
   - Team response and communication
   - Runbook execution
   - Post-incident review

**Deliverables:**
- Incident response checklist
- Troubleshooting decision tree
- Tabletop exercise report

---

#### Module 4: Advanced Topics (4 hours)

**Learning Objectives:**
- Perform capacity planning analysis
- Execute disaster recovery procedures
- Implement security best practices
- Optimize system performance

**Topics:**
1. **Capacity Management** (60 min)
   - Resource utilization monitoring
   - Forecasting and trending
   - Scaling strategies (horizontal vs vertical)
   - Cost optimization techniques

2. **Disaster Recovery** (90 min)
   - Backup and restore procedures
   - Failover mechanisms
   - RTO/RPO requirements
   - DR testing protocols

3. **Security Operations** (60 min)
   - Access control and RBAC
   - Secret management (Kubernetes secrets, external secret stores)
   - Security monitoring and audit logs
   - Vulnerability management

4. **Performance Tuning** (30 min)
   - Database query optimization
   - Cache tuning
   - Connection pool configuration
   - Resource allocation best practices

**Deliverables:**
- Capacity planning worksheet
- DR drill execution report
- Security audit checklist

---

### 7.1.2 Hands-On Exercises

#### Exercise 1: Service Health Check

**Objective:** Verify all LLM-Research-Lab services are healthy

**Steps:**
1. Access Kubernetes cluster: `kubectl config use-context llm-research-lab-prod`
2. Check pod status:
   ```bash
   kubectl get pods -n llm-research-lab-prod
   kubectl get pods -n llm-research-lab-data
   ```
3. Verify API health endpoint:
   ```bash
   curl https://api.llm-research-lab.company.com/health/ready
   ```
4. Check database connectivity:
   ```bash
   kubectl exec -n llm-research-lab-data -it postgres-0 -- psql -U postgres -c "SELECT 1"
   ```
5. Verify Kafka broker health:
   ```bash
   kubectl exec -n llm-research-lab-prod kafka-0 -- kafka-broker-api-versions.sh --bootstrap-server localhost:9092
   ```

**Expected Results:**
- All pods in `Running` state
- API returns `200 OK` with `{"status": "healthy"}`
- Database query returns `1`
- Kafka brokers respond with version information

**Validation:** Instructor reviews executed commands and verifies outputs

---

#### Exercise 2: Investigate API Latency Alert

**Objective:** Diagnose and resolve simulated API latency spike

**Scenario:** PagerDuty alert: `API p99 latency > 500ms for 5 minutes`

**Steps:**
1. Access Grafana dashboard: "LLM Research Lab - API Performance"
2. Identify latency spike timestamp
3. Check concurrent requests:
   ```promql
   api_request_in_flight{method="POST", path="/api/v1/experiments"}
   ```
4. Review error rates:
   ```promql
   rate(api_request_total{status=~"5.."}[5m])
   ```
5. Analyze database query performance:
   ```promql
   db:query_latency:p99:5m{query_type="INSERT"}
   ```
6. Check logs for errors:
   ```bash
   kubectl logs -n llm-research-lab-prod -l app=llm-research-api --tail=100 | grep ERROR
   ```
7. Correlate with distributed trace in Jaeger

**Expected Actions:**
- Identify root cause (e.g., database slow query)
- Execute remediation (e.g., kill long-running query, scale up pods)
- Document findings in incident report template

**Validation:** Trainee completes incident report with correct root cause analysis

---

#### Exercise 3: Execute Database Backup Restore

**Objective:** Restore PostgreSQL database from backup

**Steps:**
1. List available backups:
   ```bash
   kubectl exec -n llm-research-lab-data postgres-0 -- barman list-backup llm-research-postgres
   ```
2. Create test database for restore:
   ```bash
   kubectl exec -n llm-research-lab-data postgres-0 -- psql -U postgres -c "CREATE DATABASE restore_test"
   ```
3. Execute restore:
   ```bash
   kubectl exec -n llm-research-lab-data postgres-0 -- barman recover \
     --target-time "2025-11-28 10:00:00" \
     llm-research-postgres latest restore_test
   ```
4. Verify restore:
   ```bash
   kubectl exec -n llm-research-lab-data postgres-0 -- psql -U postgres -d restore_test -c "\dt"
   ```
5. Document restore completion time and verification steps

**Expected Results:**
- Restore completes successfully within 30 minutes
- All tables present in restored database
- Sample data queries return expected results

**Validation:** Instructor verifies restored database schema and data integrity

---

#### Exercise 4: Scale Experiment Workers

**Objective:** Horizontally scale experiment workers based on Kafka lag

**Steps:**
1. Check current worker count:
   ```bash
   kubectl get deployment experiment-worker -n llm-research-lab-prod
   ```
2. Monitor Kafka consumer lag:
   ```promql
   kafka_consumergroup_lag{topic="experiments",group="experiment-processor-v1"}
   ```
3. Calculate required worker count (target: 100 messages lag per worker)
4. Scale deployment:
   ```bash
   kubectl scale deployment experiment-worker -n llm-research-lab-prod --replicas=10
   ```
5. Verify scaling:
   ```bash
   kubectl rollout status deployment/experiment-worker -n llm-research-lab-prod
   ```
6. Monitor lag reduction in Grafana

**Expected Results:**
- Workers scale to target count within 2 minutes
- Kafka consumer lag decreases to target threshold
- No errors during scaling operation

**Validation:** Lag metric returns to healthy levels (< 1000 messages total)

---

### 7.1.3 Certification Requirements

#### Certification Tracks

**Track 1: L1 Support Operator**

**Prerequisites:**
- Basic Linux command-line proficiency
- Understanding of HTTP/REST APIs
- Familiarity with Kubernetes concepts

**Requirements:**
- Complete Modules 1-2 (10 hours)
- Pass written exam: 30 questions, 85% score required
- Complete 3 hands-on exercises successfully
- Shadow L2 engineer for 5 incident responses

**Certification Grants:**
- Read-only access to production cluster
- Execute pre-approved runbooks
- Escalate to L2 with context
- Update status page and incident tickets

---

**Track 2: L2 Support Engineer**

**Prerequisites:**
- L1 certification OR 2+ years operations experience
- Proficiency in Kubernetes, PostgreSQL, Kafka
- Programming knowledge (Rust/Python preferred)

**Requirements:**
- Complete all modules (18 hours)
- Pass written exam: 50 questions, 90% score required
- Complete all 4 hands-on exercises successfully
- Lead 3 incident responses during shadow period
- Execute 1 production deployment under supervision
- Complete DR drill successfully

**Certification Grants:**
- Full read access to production cluster
- Execute all runbooks
- Approve emergency changes
- Modify non-critical configurations
- Lead incident response

---

**Track 3: L3 SRE Specialist**

**Prerequisites:**
- L2 certification
- 3+ years SRE/DevOps experience
- Deep Rust and infrastructure-as-code knowledge

**Requirements:**
- Complete advanced training (additional 8 hours on architecture deep-dive)
- Review all source code in llm-research-lab repository
- Design and document 2 new runbooks
- Lead post-mortem for 3 incidents
- Mentor 2 L1/L2 trainees
- Pass architecture review interview with development team

**Certification Grants:**
- Full read-write access to production cluster
- Approve architectural changes
- Modify critical system configurations
- Create new deployment procedures
- Incident commander for P0/P1 incidents

---

### 7.1.4 Training Schedule

#### Week 1-2: Initial Training

| Day | Time | Module | Audience | Instructor |
|-----|------|--------|----------|-----------|
| Mon W1 | 9:00-13:00 | Module 1: Architecture | All | Lead Architect |
| Tue W1 | 9:00-12:00 | Module 2: Operations (Part 1) | All | Senior SRE |
| Wed W1 | 9:00-12:00 | Module 2: Operations (Part 2) | All | Senior SRE |
| Thu W1 | 9:00-13:00 | Module 3: Incident Response | All | Incident Commander |
| Fri W1 | 9:00-13:00 | Hands-On Lab Day | All | Ops Team Lead |
| Mon W2 | 9:00-13:00 | Module 4: Advanced Topics | L2/L3 | Principal Engineer |
| Tue W2 | 9:00-17:00 | Certification Exams | All | Training Coordinator |
| Wed W2 | 9:00-17:00 | Exam Review & Remediation | Failed Exams | Instructors |
| Thu W2 | 9:00-17:00 | Shadow Period Begins | All | Current On-Call |
| Fri W2 | 14:00-16:00 | Training Retrospective | All | Training Lead |

#### Week 3-4: Shadow On-Call Period

- **Weeks 3-4**: All certified operators shadow current on-call rotation
- **Daily**: 30-minute debrief with shadow mentor
- **Week 4 Friday**: Shadow period completion assessment

---

## 7.2 Documentation Package

### 7.2.1 System Architecture Documentation

#### High-Level Architecture Document

**File:** `/docs/architecture/overview.md`

**Contents:**
```markdown
# LLM-Research-Lab System Architecture

## Executive Summary
LLM-Research-Lab is a distributed system for LLM experimentation and evaluation,
comprising API services, asynchronous experiment workers, and data storage layers.

## Component Diagram
[Include C4 Level 1 Context Diagram]

## Core Components

### API Layer
- **Service:** llm-research-api
- **Technology:** Rust (Axum framework)
- **Responsibilities:**
  - Experiment submission and management
  - Dataset versioning API
  - Metrics query endpoints
  - Authentication and authorization
- **Scaling:** Horizontal (3-20 replicas, HPA-managed)
- **Dependencies:** PostgreSQL (metadata), Redis (caching), Kafka (async processing)

### Experiment Runner
- **Service:** experiment-worker
- **Technology:** Rust (Tokio async runtime)
- **Responsibilities:**
  - Consume experiment jobs from Kafka
  - Execute model inference against LLM providers
  - Collect evaluation metrics
  - Store results in ClickHouse and PostgreSQL
- **Scaling:** Horizontal (2-50 replicas based on Kafka lag)
- **Dependencies:** Kafka, ClickHouse, S3-compatible storage, LLM-Registry

### Data Layer
- **PostgreSQL:** Relational metadata (experiments, datasets, users)
- **ClickHouse:** Time-series metrics and analytics
- **Redis:** Session cache, rate limiting, distributed locks
- **Kafka:** Asynchronous job queue (experiments, results, metrics)
- **S3-Compatible Storage:** Model artifacts, datasets, experiment outputs

### Integration Points
- **LLM-Registry:** Fetch model metadata and versioned artifacts
- **LLM-Data-Vault:** Access controlled datasets with lineage tracking
- **LLM-Analytics-Hub:** Stream metrics for visualization
- **LLM-Test-Bench:** Submit models for standardized benchmarking

## Network Architecture
[Include network diagram showing ingress, service mesh, egress]

## Data Flow
[Include sequence diagrams for key workflows: experiment submission, result retrieval]

## Security Boundaries
[Include trust boundaries and authentication/authorization flows]
```

---

#### Component Deep-Dive Documents

**File:** `/docs/architecture/components/api-service.md`

**Template:**
```markdown
# API Service Architecture

## Overview
The LLM-Research-Lab API provides RESTful endpoints for experiment management.

## Technology Stack
- **Framework:** Axum (Rust async web framework)
- **Database ORM:** SQLx with compile-time query verification
- **Authentication:** JWT with RS256 signing
- **Serialization:** serde_json
- **Metrics:** Prometheus client library

## API Endpoints

### Experiments API
- `POST /api/v1/experiments` - Submit new experiment
- `GET /api/v1/experiments/{id}` - Retrieve experiment details
- `GET /api/v1/experiments` - List experiments (paginated)
- `DELETE /api/v1/experiments/{id}` - Cancel running experiment

### Datasets API
- `POST /api/v1/datasets` - Upload and version dataset
- `GET /api/v1/datasets/{name}/{version}` - Retrieve dataset
- `GET /api/v1/datasets` - List all dataset versions

### Metrics API
- `GET /api/v1/metrics/experiments/{id}` - Fetch experiment metrics
- `POST /api/v1/metrics/query` - Execute PromQL-style metric queries

## Configuration
Environment variables and config file locations documented in `/docs/configuration/api.md`

## Observability
- **Metrics:** Exposed on `:9090/metrics`
- **Logs:** Structured JSON to stdout (collected by Vector)
- **Traces:** OpenTelemetry exporter to Jaeger

## Scaling Characteristics
- Stateless design enables horizontal scaling
- Connection pooling: 25 connections per replica
- Target: 1000 req/s per replica at <50ms p99 latency

## Failure Modes
| Failure | Detection | Impact | Mitigation |
|---------|-----------|--------|------------|
| Database unavailable | Health check fails | API returns 503 | Circuit breaker, retry with backoff |
| Redis cache miss | Log warning | Slight latency increase | Database fallback |
| Kafka producer timeout | Metric alert | Experiment submission fails | Client retry, DLQ |
```

Repeat for:
- `/docs/architecture/components/experiment-worker.md`
- `/docs/architecture/components/database-layer.md`
- `/docs/architecture/components/messaging-layer.md`

---

### 7.2.2 API Documentation (Rustdoc + OpenAPI)

#### Rustdoc Generation

**Build Command:**
```bash
cargo doc --no-deps --document-private-items --open
```

**CI Pipeline Integration:**
```yaml
# .github/workflows/docs.yml
name: Generate Documentation
on:
  push:
    branches: [main]
jobs:
  docs:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions-rust-lang/setup-rust-toolchain@v1
      - name: Generate Rustdoc
        run: cargo doc --no-deps --workspace
      - name: Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./target/doc
```

**Access:** https://company.github.io/llm-research-lab/

---

#### OpenAPI Specification

**File:** `/docs/api/openapi.yaml`

**Generation:** Use `utoipa` crate for Rust code annotation

```rust
// Example from src/api/experiments.rs
use utoipa::{OpenApi, ToSchema};

#[derive(OpenApi)]
#[openapi(
    paths(
        submit_experiment,
        get_experiment,
        list_experiments
    ),
    components(
        schemas(ExperimentRequest, ExperimentResponse, ExperimentStatus)
    ),
    tags(
        (name = "experiments", description = "Experiment management endpoints")
    )
)]
pub struct ExperimentsApi;

#[utoipa::path(
    post,
    path = "/api/v1/experiments",
    request_body = ExperimentRequest,
    responses(
        (status = 201, description = "Experiment created", body = ExperimentResponse),
        (status = 400, description = "Invalid request"),
        (status = 401, description = "Unauthorized")
    ),
    tag = "experiments"
)]
async fn submit_experiment(
    Json(req): Json<ExperimentRequest>
) -> Result<Json<ExperimentResponse>, ApiError> {
    // Implementation
}
```

**Interactive Documentation:** Deploy Swagger UI at `https://api.llm-research-lab.company.com/docs`

---

### 7.2.3 Operational Runbooks Index

All runbooks follow standardized template in `/docs/runbooks/template.md`

#### Runbook: Service Health Check

**File:** `/docs/runbooks/health-check.md`

**Metadata:**
```yaml
title: LLM-Research-Lab Service Health Check
category: Routine Maintenance
frequency: Daily
duration: 15 minutes
required_role: L1 Operator
last_updated: 2025-11-28
```

**Purpose:** Verify all LLM-Research-Lab services are operational

**Prerequisites:**
- Access to Kubernetes cluster
- Read access to Grafana dashboards

**Procedure:**
1. **Check Pod Health**
   ```bash
   kubectl get pods -n llm-research-lab-prod -o wide
   kubectl get pods -n llm-research-lab-data -o wide
   ```
   **Expected:** All pods in `Running` state, no restarts in last 24h

2. **Verify API Endpoints**
   ```bash
   curl -f https://api.llm-research-lab.company.com/health/live
   curl -f https://api.llm-research-lab.company.com/health/ready
   ```
   **Expected:** Both return `200 OK`

3. **Check Database Connectivity**
   ```bash
   kubectl exec -n llm-research-lab-data postgres-0 -- \
     psql -U postgres -d llm_research -c "SELECT COUNT(*) FROM experiments;"
   ```
   **Expected:** Query returns count without errors

4. **Verify Kafka Topics**
   ```bash
   kubectl exec -n llm-research-lab-prod kafka-0 -- \
     kafka-topics.sh --bootstrap-server localhost:9092 --list
   ```
   **Expected:** Topics `experiments`, `experiment-results`, `metrics-events` present

5. **Review Grafana Dashboard**
   - Navigate to "LLM Research Lab - System Overview"
   - Confirm no critical alerts firing
   - Verify request rate > 0 (indicates active usage)

**Success Criteria:**
- All checks pass
- No critical alerts
- Response times within SLO

**Failure Actions:**
- If any check fails, escalate to L2 with failure details
- Create incident ticket with severity based on number of failures

**Automation Opportunity:** Convert to automated health check script

---

#### Runbook: Database Connection Pool Exhaustion

**File:** `/docs/runbooks/db-connection-pool-exhaustion.md`

**Metadata:**
```yaml
title: Resolve Database Connection Pool Exhaustion
category: Incident Response
severity: P2
mttr_target: 30 minutes
required_role: L2 Engineer
```

**Symptoms:**
- Alert: `db_connection_pool_utilization > 0.95`
- API errors: `"database connection timeout"`
- Logs: `ERROR: connection pool exhausted`

**Investigation:**
1. **Confirm Pool Exhaustion**
   ```promql
   db_connection_pool_size - db_connection_pool_idle
   ```
   **Expected:** Value near `db_connection_pool_size` (default 25)

2. **Identify Long-Running Queries**
   ```sql
   SELECT pid, now() - query_start AS duration, query
   FROM pg_stat_activity
   WHERE state = 'active' AND now() - query_start > interval '5 minutes'
   ORDER BY duration DESC;
   ```

3. **Check for Connection Leaks**
   ```sql
   SELECT application_name, COUNT(*)
   FROM pg_stat_activity
   GROUP BY application_name
   ORDER BY COUNT(*) DESC;
   ```
   **Look for:** Unexpected high connection counts from specific apps

**Resolution:**
1. **Immediate Mitigation (choose one):**
   - **Option A:** Kill long-running queries (if identified as root cause)
     ```sql
     SELECT pg_terminate_backend(pid) FROM pg_stat_activity
     WHERE pid = <problematic_pid>;
     ```
   - **Option B:** Restart API pods to reset connections
     ```bash
     kubectl rollout restart deployment/llm-research-api -n llm-research-lab-prod
     ```
   - **Option C:** Temporarily increase pool size (emergency only)
     ```bash
     kubectl set env deployment/llm-research-api -n llm-research-lab-prod \
       DATABASE_POOL_SIZE=50
     ```

2. **Verify Resolution**
   ```promql
   db_connection_pool:utilization < 0.80
   ```

3. **Root Cause Analysis**
   - Review slow query logs
   - Check for recent code deployments
   - Analyze API request patterns (unexpected load spike?)

**Prevention:**
- Implement connection timeout configurations
- Add alerts for slow queries (> 10s)
- Review and optimize identified slow queries
- Consider increasing default pool size if sustained high usage

**Escalation:**
- If resolution fails after 30 minutes, escalate to L3 SRE
- If database performance degraded, engage DBA team

---

#### Additional Runbooks (Index)

| Runbook | File | Category | Severity | Role |
|---------|------|----------|----------|------|
| API Latency Spike | `api-latency-spike.md` | Incident Response | P2 | L2 |
| Kafka Consumer Lag | `kafka-consumer-lag.md` | Incident Response | P3 | L2 |
| Disk Space Exhaustion | `disk-space-exhaustion.md` | Incident Response | P1 | L2 |
| Database Backup/Restore | `database-backup-restore.md` | Maintenance | N/A | L2 |
| Redis Failover | `redis-failover.md` | Incident Response | P2 | L2 |
| Certificate Renewal | `certificate-renewal.md` | Maintenance | N/A | L1 |
| Deployment Rollback | `deployment-rollback.md` | Change Management | P1 | L2 |
| Scale Experiment Workers | `scale-workers.md` | Capacity | N/A | L1 |
| ClickHouse Query Optimization | `clickhouse-optimization.md` | Performance | P3 | L3 |
| Disaster Recovery Drill | `disaster-recovery-drill.md` | DR Testing | N/A | L3 |

**Access:** All runbooks in `/docs/runbooks/` with index at `/docs/runbooks/README.md`

---

### 7.2.4 Troubleshooting Guides

#### Troubleshooting Decision Tree

**File:** `/docs/troubleshooting/decision-tree.md`

```
Problem: User reports "Experiment submission failed"
├─ Is API returning 5xx errors?
│  ├─ Yes → Check API pod health
│  │  ├─ Pods CrashLooping → Review logs for panic/OOM
│  │  ├─ Pods Running but slow → Check database connection pool
│  │  └─ No pods available → Check HPA configuration
│  └─ No → Check API returning 4xx errors?
│     ├─ 401 Unauthorized → Verify JWT token validity
│     ├─ 400 Bad Request → Review request payload schema
│     └─ 429 Too Many Requests → Rate limit exceeded
│
├─ Is Kafka accepting messages?
│  ├─ No → Check Kafka broker health
│  │  └─ Follow runbook: kafka-broker-recovery.md
│  └─ Yes → Check consumer lag
│     └─ High lag → Scale experiment workers
│
└─ Is experiment stuck in "pending" state?
   ├─ Check worker pod count
   ├─ Review worker logs for errors
   └─ Verify dataset availability in LLM-Data-Vault
```

---

#### Common Error Messages Reference

**File:** `/docs/troubleshooting/error-messages.md`

| Error Message | Cause | Resolution |
|--------------|-------|------------|
| `database connection timeout` | Connection pool exhausted | Runbook: `db-connection-pool-exhaustion.md` |
| `Kafka producer timeout` | Kafka broker unavailable or slow | Check Kafka broker health, verify network connectivity |
| `Dataset version not found: {name}:{version}` | Dataset not in LLM-Data-Vault | Verify dataset upload, check Data-Vault availability |
| `Model not found in registry: {model_id}` | Model not registered in LLM-Registry | Register model or use existing model ID |
| `OOM killed` | Container memory exceeded | Increase pod memory limits, investigate memory leak |
| `too many open files` | File descriptor limit reached | Increase ulimit or investigate file descriptor leak |
| `certificate has expired` | TLS certificate expired | Runbook: `certificate-renewal.md` |
| `RBAC: access denied` | Insufficient Kubernetes permissions | Review ServiceAccount permissions, update RBAC |

---

## 7.3 Knowledge Transfer Sessions

### 7.3.1 Session Schedule and Topics

#### Week 1: Architecture and Design

**Session 1.1: System Overview and Design Principles**
- **Date:** Monday, Week 1 | 10:00-12:00
- **Presenter:** Lead Architect
- **Audience:** All operations staff (L1/L2/L3)
- **Agenda:**
  1. LLM-Research-Lab purpose and scope (30 min)
  2. High-level architecture walkthrough (45 min)
  3. Design principles and trade-offs (30 min)
  4. Q&A (15 min)
- **Materials:** Architecture diagrams, design decision log
- **Recording:** Required, post to internal wiki

**Session 1.2: Database Architecture**
- **Date:** Tuesday, Week 1 | 14:00-16:00
- **Presenter:** Database Engineer
- **Audience:** L2/L3 engineers
- **Agenda:**
  1. PostgreSQL schema design and indexing strategy (30 min)
  2. ClickHouse sharding and replication (30 min)
  3. Redis caching patterns (20 min)
  4. Connection pooling and performance tuning (25 min)
  5. Q&A (15 min)
- **Materials:** ER diagrams, sample queries, performance benchmarks
- **Recording:** Required

**Session 1.3: Messaging Architecture**
- **Date:** Wednesday, Week 1 | 10:00-12:00
- **Presenter:** Senior Backend Engineer
- **Audience:** L2/L3 engineers
- **Agenda:**
  1. Kafka topic design and partitioning (30 min)
  2. Producer/consumer patterns (30 min)
  3. Message schemas and versioning (20 min)
  4. Error handling and dead-letter queues (25 min)
  5. Q&A (15 min)
- **Materials:** Topic configurations, message schemas, consumer group configs
- **Recording:** Required

---

#### Week 2: Operations and Monitoring

**Session 2.1: Observability Stack Deep-Dive**
- **Date:** Monday, Week 2 | 10:00-12:00
- **Presenter:** SRE Lead
- **Audience:** All operations staff
- **Agenda:**
  1. Prometheus metrics and recording rules (30 min)
  2. Grafana dashboards and alerting (30 min)
  3. Loki log aggregation and querying (20 min)
  4. Jaeger distributed tracing (25 min)
  5. Hands-on: Create custom dashboard (15 min)
- **Materials:** Prometheus queries, dashboard JSON templates, log query examples
- **Recording:** Required

**Session 2.2: Incident Response Procedures**
- **Date:** Tuesday, Week 2 | 14:00-16:00
- **Presenter:** Incident Commander
- **Audience:** All operations staff
- **Agenda:**
  1. Incident severity classification (20 min)
  2. Communication protocols and escalation (25 min)
  3. Runbook walkthrough: Top 5 incidents (45 min)
  4. Post-mortem process (15 min)
  5. Q&A (15 min)
- **Materials:** Incident response playbook, sample post-mortems, escalation matrix
- **Recording:** Required

**Session 2.3: Deployment and Rollback Procedures**
- **Date:** Wednesday, Week 2 | 10:00-12:00
- **Presenter:** DevOps Engineer
- **Audience:** L2/L3 engineers
- **Agenda:**
  1. CI/CD pipeline overview (20 min)
  2. Deployment strategies (rolling, canary, blue-green) (30 min)
  3. Database migration workflow (25 min)
  4. Rollback procedures and automation (30 min)
  5. Live demo: Execute deployment (15 min)
- **Materials:** CI/CD configs, deployment checklists, rollback scripts
- **Recording:** Required

---

#### Week 3: Advanced Topics

**Session 3.1: Capacity Planning and Scaling**
- **Date:** Monday, Week 3 | 10:00-12:00
- **Presenter:** Principal SRE
- **Audience:** L2/L3 engineers
- **Agenda:**
  1. Resource utilization analysis (30 min)
  2. Forecasting and trend analysis (30 min)
  3. Autoscaling configurations (HPA, VPA, Cluster Autoscaler) (30 min)
  4. Cost optimization techniques (15 min)
  5. Q&A (15 min)
- **Materials:** Capacity planning spreadsheets, historical usage data, cost reports
- **Recording:** Required

**Session 3.2: Security and Compliance**
- **Date:** Tuesday, Week 3 | 14:00-16:00
- **Presenter:** Security Engineer
- **Audience:** L2/L3 engineers
- **Agenda:**
  1. RBAC and access control (25 min)
  2. Secret management best practices (25 min)
  3. Security monitoring and audit logs (30 min)
  4. Vulnerability management workflow (25 min)
  5. Q&A (15 min)
- **Materials:** RBAC policies, secret rotation procedures, security audit checklist
- **Recording:** Required

**Session 3.3: Disaster Recovery and Business Continuity**
- **Date:** Wednesday, Week 3 | 10:00-12:00
- **Presenter:** DR Lead
- **Audience:** L2/L3 engineers
- **Agenda:**
  1. RTO/RPO requirements and SLAs (20 min)
  2. Backup strategies and automation (30 min)
  3. Failover procedures (30 min)
  4. DR testing and validation (25 min)
  5. Q&A (15 min)
- **Materials:** DR plan, backup schedules, failover runbooks, test results
- **Recording:** Required

---

### 7.3.2 Recorded Session Requirements

#### Recording Standards

**Technical Requirements:**
- **Platform:** Zoom with cloud recording
- **Video Quality:** 1080p minimum
- **Audio Quality:** 48kHz, noise cancellation enabled
- **Screen Sharing:** Record presenter screen + webcam
- **Duration:** Full session including Q&A
- **Closed Captions:** Auto-generated, manually reviewed for accuracy

**Content Requirements:**
- **Intro Slide:** Session title, presenter, date, objectives
- **Agenda Slide:** Topics and time allocation
- **Demo/Hands-On:** Screen recording of all live demonstrations
- **Q&A:** All questions and answers captured
- **Resources Slide:** Links to documentation, runbooks, source code

**Post-Processing:**
- **Editing:** Remove long pauses, technical difficulties
- **Chapters:** Add video chapters for each agenda item
- **Transcript:** Generate full text transcript
- **Slides:** Export slides as PDF and attach to recording

**Publishing:**
- **Platform:** Internal video library (e.g., company LMS or wiki)
- **Access Control:** Restrict to operations and engineering teams
- **Metadata:** Tag with keywords, session number, presenter, date
- **Searchability:** Ensure transcript is indexed for full-text search

---

### 7.3.3 Q&A Documentation

#### Q&A Session Format

Each knowledge transfer session includes dedicated Q&A time. All questions and answers are documented in shared tracker.

**Q&A Tracker:** Google Sheet or Confluence page

**Template:**

| ID | Session | Question | Asked By | Answer | Answered By | Follow-Up Required | Status |
|----|---------|----------|----------|--------|-------------|-------------------|--------|
| 001 | 1.1 | What is the expected query load on PostgreSQL? | Jane (L2) | ~500 queries/sec during peak hours. See capacity plan doc. | Lead Architect | No | Closed |
| 002 | 2.1 | How do we customize Grafana dashboards? | Mike (L1) | Follow runbook: grafana-dashboard-customization.md | SRE Lead | Yes - Create runbook | Open |
| 003 | 2.2 | What is escalation path for P0 incidents? | Sarah (L2) | L1 → L2 → L3 → Dev Team Oncall. See escalation matrix. | Incident Commander | No | Closed |

**Process:**
1. **During Session:** Moderator captures questions in real-time
2. **After Session:** Presenter provides detailed written answers within 48 hours
3. **Follow-Up:** If question requires runbook creation or documentation update, create Jira ticket
4. **Review:** All Q&A reviewed in weekly handoff sync meeting

---

### 7.3.4 Shadow On-Call Period

#### Shadow On-Call Program Structure

**Duration:** 2 weeks (minimum) per operator

**Objectives:**
1. Observe real-world incident response
2. Practice runbook execution in live environment
3. Build confidence in escalation and communication
4. Validate training effectiveness

**Roles:**
- **Primary On-Call:** Experienced engineer (current rotation)
- **Shadow:** Operations trainee (L1 or L2 candidate)
- **Mentor:** Designated coach (usually Shadow's manager)

---

#### Shadow On-Call Schedule

**Week 1: Observation**

| Day | Activity | Primary Responsibility | Shadow Responsibility |
|-----|----------|----------------------|----------------------|
| Mon | Daily Health Check | Execute | Observe and document |
| Mon | Alert Triage | Lead response | Observe, take notes |
| Tue | Incident Response (if any) | Lead | Observe, assist with documentation |
| Wed | Deployment Review | Approve | Observe approval process |
| Thu | Capacity Review | Analyze metrics | Observe and ask questions |
| Fri | Weekly Debrief | Present findings | Share observations |

**Week 2: Active Participation**

| Day | Activity | Primary Responsibility | Shadow Responsibility |
|-----|----------|----------------------|----------------------|
| Mon | Daily Health Check | Supervise | Execute with guidance |
| Mon | Alert Triage | Supervise | Lead response with oversight |
| Tue | Incident Response (if any) | Backup | Lead with primary as backup |
| Wed | Deployment Review | Supervise | Execute approval checklist |
| Thu | Capacity Review | Supervise | Analyze and present findings |
| Fri | Certification Assessment | Evaluate | Demonstrate competency |

---

#### Shadow On-Call Guidelines

**For Shadows:**
1. **Be Present:** Available during primary's on-call hours
2. **Ask Questions:** No question is too basic during learning period
3. **Document Everything:** Take detailed notes on all activities
4. **Propose Solutions:** Suggest approaches before primary acts (learning exercise)
5. **Hands-On (Week 2):** Execute runbooks with primary supervision

**For Primary On-Call:**
1. **Narrate Actions:** Explain reasoning behind each decision
2. **Encourage Participation:** Let shadow attempt tasks (with supervision)
3. **Provide Context:** Share historical context and patterns
4. **Give Feedback:** Daily 15-min debrief on shadow's performance
5. **Safety First:** Override shadow if incorrect action could cause impact

**For Mentors:**
1. **Daily Check-In:** 15-min 1:1 with shadow to discuss learnings
2. **Review Notes:** Read shadow's daily documentation
3. **Answer Questions:** Provide clarification on complex topics
4. **Assess Progress:** Determine if additional shadow time needed
5. **Certification Decision:** Approve or defer certification based on performance

---

#### Shadow On-Call Evaluation Criteria

**Competency Checklist:**

| Skill | Week 1 | Week 2 | Required for Certification |
|-------|--------|--------|---------------------------|
| Execute health check runbook | Observed | Performed | Yes (L1) |
| Triage and classify alerts | Observed | Performed | Yes (L1) |
| Navigate Grafana dashboards | Observed | Performed | Yes (L1) |
| Query logs in Loki | Observed | Performed | Yes (L2) |
| Execute deployment runbook | Observed | Performed | Yes (L2) |
| Lead incident response | Observed | Performed | Yes (L2) |
| Perform root cause analysis | Observed | Performed | Yes (L2) |
| Execute rollback procedure | Observed | Performed | Yes (L2) |
| Make architectural decisions | Observed | Observed | Yes (L3) |

**Assessment Rubric:**

**Level 1 (Novice):** Can execute runbook with significant guidance
**Level 2 (Competent):** Can execute runbook independently with occasional questions
**Level 3 (Proficient):** Can execute runbook independently and troubleshoot deviations
**Level 4 (Expert):** Can execute runbook, handle edge cases, and mentor others

**Certification Threshold:**
- **L1:** Achieve Level 2 on all L1-required skills
- **L2:** Achieve Level 3 on all L2-required skills
- **L3:** Achieve Level 4 on all L3-required skills

---

## 7.4 Handoff Checklist

### 7.4.1 Access Provisioning

#### Kubernetes Access

**Production Cluster Access:**

| Role | Namespace Access | Permissions | Approval Required |
|------|-----------------|-------------|------------------|
| L1 Operator | llm-research-lab-prod (read-only) | get, list, describe pods/services/deployments | Manager |
| L2 Engineer | llm-research-lab-prod (read-write) | get, list, describe, logs, exec (non-destructive) | Manager + Lead SRE |
| L3 SRE | llm-research-lab-prod (admin) | Full access including delete, edit | Manager + Engineering Director |

**Provisioning Steps:**
1. Create ServiceAccount:
   ```bash
   kubectl create serviceaccount <username>-sa -n llm-research-lab-prod
   ```
2. Apply RoleBinding:
   ```bash
   kubectl apply -f rbac/<role>-rolebinding.yaml
   ```
3. Generate kubeconfig:
   ```bash
   ./scripts/generate-kubeconfig.sh <username> <role>
   ```
4. Verify access:
   ```bash
   kubectl auth can-i --as=system:serviceaccount:llm-research-lab-prod:<username>-sa list pods
   ```

**Checklist:**
- [ ] ServiceAccount created
- [ ] RoleBinding applied
- [ ] Kubeconfig delivered securely (1Password vault)
- [ ] Access verified by user
- [ ] Access documented in access log

---

#### Database Access

**PostgreSQL Access:**

| Role | Database | Access Level | Approval Required |
|------|----------|--------------|------------------|
| L1 Operator | llm_research | Read-only (SELECT on specific tables) | Manager |
| L2 Engineer | llm_research | Read-only (SELECT all tables) | Manager + DBA |
| L3 SRE | llm_research | Read-write (SELECT, INSERT, UPDATE on specific tables) | Manager + DBA + Security |

**Provisioning Steps:**
1. Create database user:
   ```sql
   CREATE USER <username> WITH PASSWORD '<secure-password>';
   ```
2. Grant permissions:
   ```sql
   -- L1: Read-only on experiments and datasets tables
   GRANT SELECT ON experiments, datasets TO <username>;

   -- L2: Read-only on all tables
   GRANT SELECT ON ALL TABLES IN SCHEMA public TO <username>;

   -- L3: Read-write on specific tables
   GRANT SELECT, INSERT, UPDATE ON experiments, datasets TO <username>;
   ```
3. Store credentials in secret manager:
   ```bash
   kubectl create secret generic <username>-db-creds \
     --from-literal=username=<username> \
     --from-literal=password=<password> \
     -n llm-research-lab-prod
   ```

**Checklist:**
- [ ] Database user created
- [ ] Permissions granted per role
- [ ] Credentials stored in 1Password/Vault
- [ ] Connection string provided
- [ ] Access verified by user

---

#### Monitoring and Observability Access

**Grafana Access:**

| Role | Access Level | Dashboards | Edit Permissions |
|------|--------------|-----------|-----------------|
| L1 Operator | Viewer | All LLM-Research-Lab dashboards | No |
| L2 Engineer | Editor | All dashboards | Create/Edit personal dashboards |
| L3 SRE | Admin | All dashboards | Create/Edit/Delete all dashboards |

**Provisioning Steps:**
1. Create Grafana user via UI or API
2. Assign to appropriate organization and team
3. Set dashboard permissions
4. Configure data source access

**Checklist:**
- [ ] Grafana user created
- [ ] Added to "LLM-Research-Lab Operations" team
- [ ] Dashboard permissions verified
- [ ] Login credentials delivered

**PagerDuty Access:**

- **L1 Operator:** Add to escalation policy as responder
- **L2 Engineer:** Add as primary on-call rotation
- **L3 SRE:** Add as escalation point

**Checklist:**
- [ ] PagerDuty user created/updated
- [ ] Added to escalation policy
- [ ] Notification preferences configured
- [ ] Test page sent and acknowledged

---

### 7.4.2 Tool Access Verification

#### Access Verification Matrix

| Tool | L1 Operator | L2 Engineer | L3 SRE | Verification Method |
|------|-------------|-------------|---------|-------------------|
| Kubernetes Cluster | Read pods, logs | Read + exec | Full admin | `kubectl auth can-i` |
| PostgreSQL | SELECT on key tables | SELECT all tables | SELECT/INSERT/UPDATE | Test query |
| ClickHouse | Read-only | Read-only | Read-write | Test query |
| Redis | No direct access | redis-cli read | redis-cli read-write | Test command |
| Kafka | No direct access | kafka-console-consumer | kafka-topics admin | Test command |
| Grafana | Viewer | Editor | Admin | Login and verify |
| Prometheus | Query access | Query access | Admin API | Test query |
| Jaeger | Read traces | Read traces | Read traces | Search trace |
| PagerDuty | Incident responder | Primary on-call | Escalation point | Test page |
| GitHub Repo | Read | Read + Comment | Read + Write | Clone repo |
| 1Password Vault | Read shared secrets | Read shared secrets | Manage secrets | Retrieve test secret |

**Verification Procedure:**

1. **Schedule Verification Session:** 1 hour per operator
2. **Execute Verification:**
   - Operator attempts access to each tool
   - Operator performs test action (read/write based on role)
   - Verifier observes and confirms
3. **Document Results:** Record pass/fail for each tool in checklist
4. **Remediate Failures:** If access fails, re-provision and retest
5. **Sign-Off:** Both operator and verifier sign access verification form

---

### 7.4.3 Emergency Contact List

#### Primary Contacts

| Role | Name | Phone | Email | Slack Handle | Availability |
|------|------|-------|-------|--------------|-------------|
| **L3 SRE Lead** | [Name] | +1-XXX-XXX-XXXX | sre-lead@company.com | @sre-lead | 24/7 |
| **Engineering Manager** | [Name] | +1-XXX-XXX-XXXX | eng-manager@company.com | @eng-manager | 24/7 |
| **Lead Architect** | [Name] | +1-XXX-XXX-XXXX | architect@company.com | @architect | Business hours + on-call |
| **DBA On-Call** | Rotation | +1-XXX-XXX-XXXX | dba-oncall@company.com | @dba-oncall | 24/7 |
| **Security On-Call** | Rotation | +1-XXX-XXX-XXXX | security-oncall@company.com | @security-oncall | 24/7 |
| **Infrastructure Lead** | [Name] | +1-XXX-XXX-XXXX | infra-lead@company.com | @infra-lead | 24/7 |

#### Vendor Support Contacts

| Vendor | Service | Contact Method | SLA | Support Level |
|--------|---------|---------------|-----|--------------|
| **Cloud Provider** | Kubernetes, VMs | Support portal + phone | 1 hour response (P1) | Premium |
| **Database Vendor** | PostgreSQL support | Email + Slack | 4 hour response | Standard |
| **Kafka Support** | Confluent Cloud | Support portal | 2 hour response (P1) | Enterprise |
| **Monitoring Vendor** | Grafana Cloud | Support portal + email | 8 hour response | Pro |

#### Escalation Matrix

**Incident Escalation Path:**

```
P0 (Critical Outage):
L1 Operator → L2 Engineer (immediate) → L3 SRE (if unresolved in 15 min) → Engineering Manager (if unresolved in 30 min)

P1 (Severe Degradation):
L1 Operator → L2 Engineer (immediate) → L3 SRE (if unresolved in 30 min) → Engineering Manager (if unresolved in 1 hour)

P2 (Moderate Impact):
L1 Operator → L2 Engineer (within 30 min) → L3 SRE (if unresolved in 2 hours)

P3 (Low Impact):
L1 Operator → L2 Engineer (within 2 hours)

P4 (No Impact):
L1 Operator creates ticket, no immediate escalation
```

**Emergency Contact Procedures:**
1. **Primary Contact:** Always attempt primary contact first (Slack, then phone)
2. **Escalation Delay:** Follow time-based escalation per severity
3. **After-Hours:** Use PagerDuty to auto-escalate if primary does not respond in 10 minutes
4. **All-Hands:** For P0 incidents, Engineering Manager may invoke "all-hands" to pull in additional engineers

---

### 7.4.4 Escalation Procedures Confirmed

#### Escalation Procedure Validation

**Pre-Handoff Test:**
1. **Simulate P1 Incident:** Create realistic test incident (e.g., API latency spike in staging)
2. **L1 Operator Response:** L1 triages and follows escalation procedure
3. **L2 Engineer Response:** L2 responds and attempts resolution
4. **L3 SRE Escalation (if needed):** L3 is notified and provides guidance
5. **Debrief:** Team reviews escalation flow, communication, and resolution

**Validation Checklist:**
- [ ] L1 correctly identified incident severity
- [ ] L1 escalated to L2 within target time
- [ ] L2 acknowledged page within 5 minutes
- [ ] Communication in Slack incident channel was clear and timely
- [ ] Status page updated appropriately
- [ ] Incident documented in ticketing system
- [ ] Post-incident review scheduled

**Sign-Off:**
- [ ] L1 Operator confirms understanding of escalation procedures
- [ ] L2 Engineer confirms receipt of escalations
- [ ] L3 SRE confirms backup support availability
- [ ] Engineering Manager signs off on escalation readiness

---

## 7.5 Support Transition

### 7.5.1 Support Tier Definitions

#### L1 Support Operator

**Responsibilities:**
- Monitor dashboards and alert queues
- Execute standard health check runbooks
- Triage incoming alerts and incidents
- Escalate to L2 with detailed context
- Update status page and incident tickets
- Perform routine maintenance tasks (certificate checks, log rotation verification)

**Skills Required:**
- Basic Kubernetes knowledge (kubectl get, describe, logs)
- Ability to read and interpret Grafana dashboards
- Understanding of HTTP status codes and API concepts
- Familiarity with ticketing systems (Jira, ServiceNow)
- Clear written and verbal communication

**Scope of Changes:**
- **Allowed:** No configuration changes, read-only access
- **Escalation Required:** Any changes beyond health checks

**Typical Incidents:**
- P3/P4 alerts (low priority)
- Routine monitoring and status checks
- Customer support ticket triage

---

#### L2 Support Engineer

**Responsibilities:**
- Respond to P1-P3 incidents
- Execute diagnostic and remediation runbooks
- Perform deployments and rollbacks
- Conduct root cause analysis
- Mentor L1 operators
- Create and update runbooks
- Execute database and infrastructure maintenance

**Skills Required:**
- Advanced Kubernetes operations (exec, port-forward, edit resources)
- PostgreSQL query analysis and optimization
- Kafka consumer group management
- Prometheus/PromQL proficiency
- Distributed tracing analysis
- Scripting (bash, Python)

**Scope of Changes:**
- **Allowed:** Restart pods, scale deployments, execute approved runbooks, modify non-critical configs
- **Escalation Required:** Schema changes, architectural modifications, critical config changes

**Typical Incidents:**
- P1-P3 incidents
- Performance degradation
- Deployment failures
- Database optimization
- Capacity adjustments

---

#### L3 SRE Specialist

**Responsibilities:**
- Respond to P0/P1 critical incidents as escalation point
- Architect-level troubleshooting and decision-making
- Emergency architectural changes
- Capacity planning and long-term optimization
- Disaster recovery execution
- Review and approve runbooks created by L2
- Mentor L2 engineers
- Participate in design reviews for new features

**Skills Required:**
- Expert Rust and systems programming knowledge
- Deep understanding of LLM-Research-Lab architecture
- Database administration (PostgreSQL, ClickHouse)
- Infrastructure-as-code (Terraform, Helm)
- Performance profiling and optimization
- Security hardening and compliance

**Scope of Changes:**
- **Allowed:** All changes including architectural modifications, critical config changes, emergency schema migrations
- **Escalation Required:** Business-impacting decisions (e.g., data deletion) require Engineering Manager approval

**Typical Incidents:**
- P0/P1 critical outages
- Complex multi-component failures
- Disaster recovery scenarios
- Major performance optimization
- Security incidents

---

### 7.5.2 SLA Requirements

#### Service Level Objectives (SLOs)

| Metric | Target | Measurement Window | Consequences of Miss |
|--------|--------|--------------------|---------------------|
| **API Availability** | 99.9% | 30 days | Executive review, RCA required |
| **API Latency (p99)** | < 100ms | 7 days | Performance optimization sprint |
| **Experiment Success Rate** | > 99% | 30 days | Investigate error patterns |
| **Data Durability** | 99.999% | Annual | Immediate DR review |

#### Support Response SLAs

| Severity | Response Time | Resolution Time | Support Tier | Business Hours | After-Hours |
|----------|--------------|-----------------|--------------|---------------|-------------|
| **P0** (Critical Outage) | 15 minutes | 4 hours | L3 SRE | Yes | Yes |
| **P1** (Severe Degradation) | 30 minutes | 8 hours | L2 → L3 | Yes | Yes |
| **P2** (Moderate Impact) | 2 hours | 24 hours | L2 | Yes | Best effort |
| **P3** (Low Impact) | 8 hours | 3 business days | L1 → L2 | Yes | No |
| **P4** (No Impact) | 24 hours | 5 business days | L1 | Yes | No |

**SLA Tracking:**
- Incidents tracked in Jira with timestamps for detection, response, and resolution
- Weekly SLA compliance reports generated automatically
- Monthly SLA review meeting with stakeholders

---

### 7.5.3 Ticket Routing Rules

#### Automated Ticket Routing

**Integration:** PagerDuty → Jira Service Desk

**Routing Logic:**

```yaml
# ticket-routing-rules.yaml
rules:
  - name: "Critical Alerts"
    condition: "severity == 'P0' OR severity == 'P1'"
    action:
      - create_incident: true
      - assign_to: "L2-oncall-rotation"
      - notify: ["slack://incidents", "pagerduty://sre-team"]
      - priority: "Critical"

  - name: "Performance Degradation"
    condition: "alert_name contains 'latency' OR alert_name contains 'throughput'"
    action:
      - create_ticket: true
      - assign_to: "L2-performance-team"
      - labels: ["performance", "auto-routed"]
      - priority: "High"

  - name: "Database Issues"
    condition: "component == 'postgresql' OR component == 'clickhouse'"
    action:
      - create_ticket: true
      - assign_to: "L2-database-team"
      - cc: ["dba-team"]
      - labels: ["database", "auto-routed"]

  - name: "Kafka/Messaging"
    condition: "component == 'kafka' OR alert_name contains 'consumer_lag'"
    action:
      - create_ticket: true
      - assign_to: "L2-platform-team"
      - labels: ["messaging", "auto-routed"]

  - name: "Security Alerts"
    condition: "alert_name contains 'security' OR alert_name contains 'unauthorized'"
    action:
      - create_incident: true
      - assign_to: "security-team"
      - notify: ["pagerduty://security-oncall"]
      - priority: "Critical"

  - name: "Default Routing"
    condition: "default"
    action:
      - create_ticket: true
      - assign_to: "L1-triage-queue"
      - priority: "Medium"
```

**Manual Routing (via Jira):**

- **Component Field:** Set to "llm-research-lab"
- **Labels:** Performance, Database, Kafka, Security, Deployment, etc.
- **Assign Based on Expertise:** L1 triages and manually assigns to appropriate L2 team if auto-routing fails

---

### 7.5.4 Knowledge Base Setup

#### Knowledge Base Structure

**Platform:** Confluence or Internal Wiki

**Hierarchy:**
```
LLM-Research-Lab Operations Knowledge Base
├── Architecture
│   ├── System Overview
│   ├── Component Deep-Dives
│   ├── Data Flow Diagrams
│   └── Integration Points
├── Runbooks
│   ├── Health Checks
│   ├── Incident Response
│   ├── Maintenance Procedures
│   └── Deployment Guides
├── Troubleshooting
│   ├── Common Issues
│   ├── Error Messages Reference
│   ├── Decision Trees
│   └── Root Cause Analysis Examples
├── How-To Guides
│   ├── Access Provisioning
│   ├── Monitoring Setup
│   ├── Custom Dashboard Creation
│   └── Deployment Procedures
├── Reference
│   ├── API Documentation (link to Rustdoc)
│   ├── Database Schema
│   ├── Kafka Topics Reference
│   ├── Metrics Glossary
│   └── Configuration Reference
└── Training Materials
    ├── Video Recordings
    ├── Training Slides
    ├── Certification Exams
    └── Q&A Archive
```

#### Knowledge Base Content Requirements

**Each Article Must Include:**
1. **Title:** Clear, descriptive, follows naming convention
2. **Metadata:** Last updated date, owner, review cycle
3. **Summary:** 2-3 sentence overview
4. **Prerequisites:** Required knowledge, access, tools
5. **Content:** Step-by-step instructions with screenshots/code samples
6. **Validation:** How to verify successful completion
7. **Troubleshooting:** Common issues and resolutions
8. **Related Articles:** Links to related documentation
9. **Feedback:** Comment section for questions and improvements

**Quality Standards:**
- **Accuracy:** Technically accurate and tested
- **Clarity:** Written for target audience (L1/L2/L3)
- **Completeness:** All steps included, no assumed knowledge
- **Currency:** Reviewed and updated quarterly
- **Searchability:** Proper tagging and keywords

#### Knowledge Base Maintenance

**Ownership:**
- Each article assigned to a primary owner (engineer responsible for updates)
- Quarterly review cycle to ensure accuracy
- Automated reminders for articles >6 months old

**Update Triggers:**
- New feature deployment
- Architecture change
- Incident reveals gap in documentation
- Feedback from operations team

**Approval Workflow:**
1. Author creates/updates article
2. Peer review by L2/L3 engineer
3. Technical review by subject matter expert
4. Final approval by SRE Lead
5. Published to knowledge base
6. Announced in team Slack channel

---

## Appendix A: Training Templates

### Template: Training Module

```markdown
# Training Module: [Module Name]

## Module Information
- **Module ID:** [Unique identifier]
- **Duration:** [Hours]
- **Target Audience:** [L1/L2/L3]
- **Prerequisites:** [Required knowledge]
- **Instructor:** [Name]

## Learning Objectives
By the end of this module, participants will be able to:
1. [Objective 1]
2. [Objective 2]
3. [Objective 3]

## Agenda
| Time | Topic | Format |
|------|-------|--------|
| 0:00-0:30 | [Topic 1] | Lecture |
| 0:30-1:00 | [Topic 2] | Demo |
| 1:00-1:30 | [Hands-On Lab] | Practical |
| 1:30-1:45 | Q&A | Discussion |

## Materials
- Slides: [Link]
- Lab Instructions: [Link]
- Reference Documentation: [Link]

## Assessment
- Quiz: [Number of questions], [Pass percentage]%
- Practical Exam: [Description]

## Resources
- [Resource 1]
- [Resource 2]
```

---

### Template: Hands-On Exercise

```markdown
# Exercise: [Exercise Name]

## Objective
[Clear statement of what participant will accomplish]

## Prerequisites
- Access to: [List required access]
- Tools installed: [List required tools]
- Knowledge of: [Required concepts]

## Scenario
[Realistic scenario description]

## Steps
### Step 1: [Action]
```bash
[Command or action]
```
**Expected Result:** [What should happen]

### Step 2: [Action]
[Instructions]

**Expected Result:** [What should happen]

## Validation
[How to confirm successful completion]

## Troubleshooting
| Issue | Cause | Resolution |
|-------|-------|------------|
| [Common issue 1] | [Root cause] | [Fix] |

## Estimated Time
[Minutes to complete]

## Instructor Notes
[Tips for instructors]
```

---

## Appendix B: Certification Criteria

### L1 Support Operator Certification

**Written Exam (30 questions, 85% pass required):**

Sample Questions:
1. What is the first step when responding to an alert? (Multiple choice)
2. Which Grafana dashboard shows API latency metrics? (Multiple choice)
3. Describe the escalation path for a P1 incident. (Short answer)
4. What command would you use to check pod status in Kubernetes? (Fill in blank)
5. Identify the error from this log message: `ERROR: connection pool exhausted` (Multiple choice)

**Practical Exam (3 exercises, all must pass):**

1. **Exercise 1:** Execute service health check runbook
   - Time limit: 20 minutes
   - Pass criteria: All health checks completed, results documented

2. **Exercise 2:** Triage mock alert and escalate appropriately
   - Time limit: 15 minutes
   - Pass criteria: Correct severity classification, proper escalation, incident ticket created

3. **Exercise 3:** Navigate Grafana to identify latency spike
   - Time limit: 15 minutes
   - Pass criteria: Identify timestamp of spike, correlate with logs, document findings

**Shadow Period (5 incidents):**
- Observe and assist with 5 real or simulated incidents
- Mentor signs off on each incident response
- Demonstrate communication and documentation skills

---

### L2 Support Engineer Certification

**Written Exam (50 questions, 90% pass required):**

Includes L1 content plus:
- Advanced Kubernetes operations
- Database query optimization
- Kafka troubleshooting
- Deployment procedures
- Root cause analysis methodology

**Practical Exam (4 exercises, all must pass):**

1. **Exercise 1:** Diagnose and resolve API latency spike
2. **Exercise 2:** Execute database backup and restore
3. **Exercise 3:** Scale experiment workers based on Kafka lag
4. **Exercise 4:** Execute deployment rollback

**Shadow Period (3 incidents + 1 deployment):**
- Lead 3 incident responses under L3 supervision
- Execute 1 production deployment under supervision

**Deliverables:**
- 2 incident post-mortems written by candidate
- 1 new runbook or documentation contribution

---

### L3 SRE Specialist Certification

**Advanced Training (8 additional hours):**
- Architecture deep-dive with source code review
- Performance optimization techniques
- Disaster recovery planning and execution

**Practical Assessment:**
- Design 2 new runbooks for complex scenarios
- Lead post-mortem for 3 incidents (demonstrating root cause analysis)
- Mentor 2 L1/L2 engineers through certification

**Architecture Review Interview:**
- 90-minute technical interview with development team
- Questions on system design, trade-offs, and optimization
- Pass/fail decision by panel

**Sign-Off:**
- Engineering Manager approval
- Lead Architect approval
- Principal SRE approval

---

## Document Metadata

| Field | Value |
|-------|-------|
| **Version** | 1.0.0 |
| **Status** | Complete |
| **SPARC Phase** | Phase 5 - Completion |
| **Section** | 7 - Handoff & Knowledge Transfer |
| **Created** | 2025-11-28 |
| **Target Audience** | Operations Teams, Engineering Managers, SREs |
| **Handoff Timeline** | 4 weeks (2 weeks training + 2 weeks shadow) |

---

## Next Steps

1. **Week -2:** Finalize training materials and schedule sessions
2. **Week 1-2:** Execute training program with all operations staff
3. **Week 2:** Administer certification exams and practical assessments
4. **Week 3-4:** Shadow on-call period with experienced engineers
5. **Week 4 End:** Handoff approval and sign-off
6. **Week 5+:** Operations team assumes full responsibility with dev team backup

---

*This document is part of the SPARC Phase 5 (Completion) specification for LLM-Research-Lab, ensuring a comprehensive and successful transition from development to operations.*
---

## Final Validation & Sign-off


## Table of Contents

1. [Overview](#overview)
2. [8.1 Production Certification Checklist](#81-production-certification-checklist)
3. [8.2 Stakeholder Sign-off Matrix](#82-stakeholder-sign-off-matrix)
4. [8.3 Go-Live Criteria](#83-go-live-criteria)
5. [8.4 Launch Sequence](#84-launch-sequence)
6. [8.5 Post-Launch Validation](#85-post-launch-validation)
7. [8.6 Project Closure](#86-project-closure)
8. [Appendix A: Certification Templates](#appendix-a-certification-templates)
9. [Appendix B: Sign-off Forms](#appendix-b-sign-off-forms)
10. [Appendix C: Launch Runbook](#appendix-c-launch-runbook)
11. [Appendix D: Validation Scripts](#appendix-d-validation-scripts)
12. [Appendix E: Closure Templates](#appendix-e-closure-templates)

---

## Overview

This section establishes the final validation and sign-off requirements for LLM-Research-Lab production deployment. All criteria must be met and signed off before go-live authorization.

### Validation Objectives

1. **Performance Validation**: Verify all SLOs are met under production-like load
2. **Security Certification**: Confirm zero critical vulnerabilities and compliance adherence
3. **Operational Readiness**: Validate operations team training and runbook completeness
4. **Stakeholder Alignment**: Obtain formal sign-off from all required parties

### Sign-off Gates

| Gate | Requirement | Owner | Blocker Level |
|------|-------------|-------|---------------|
| **Technical Certification** | All technical criteria met | VP Engineering | Critical |
| **Security Certification** | Pen test passed, zero critical vulns | Security Lead | Critical |
| **Compliance Certification** | SOC2, GDPR, audit complete | Compliance | Critical |
| **Business Approval** | Product, Legal, Compliance sign-off | Product VP | Critical |
| **Executive Approval** | CTO/CEO final authorization | CTO/CEO | Critical |

---

## 8.1 Production Certification Checklist

### 8.1.1 Performance Validation

#### SLO Compliance Verification

```yaml
# performance-slo-validation.yaml
slo_requirements:
  api_availability:
    metric: api_availability
    target: 99.9%
    measurement_window: 30 days
    validation_method: Prometheus query
    query: |
      (
        sum(rate(api_request_total{status=~"2..|3.."}[30d])) /
        sum(rate(api_request_total[30d]))
      ) * 100
    acceptance_criteria: ">= 99.9"
    current_value: __TO_BE_MEASURED__
    status: [ ] Pass [ ] Fail
    verified_by: __________
    verified_date: __________

  api_latency_p99:
    metric: api_latency_p99
    target: "< 100ms"
    measurement_window: 7 days
    validation_method: Prometheus query
    query: |
      histogram_quantile(0.99,
        sum(rate(api_request_duration_seconds_bucket[7d])) by (le)
      ) * 1000
    acceptance_criteria: "< 100"
    current_value: __TO_BE_MEASURED__
    status: [ ] Pass [ ] Fail
    verified_by: __________
    verified_date: __________

  api_latency_p95:
    metric: api_latency_p95
    target: "< 50ms"
    measurement_window: 7 days
    validation_method: Prometheus query
    query: |
      histogram_quantile(0.95,
        sum(rate(api_request_duration_seconds_bucket[7d])) by (le)
      ) * 1000
    acceptance_criteria: "< 50"
    current_value: __TO_BE_MEASURED__
    status: [ ] Pass [ ] Fail
    verified_by: __________
    verified_date: __________

  experiment_success_rate:
    metric: experiment_success_rate
    target: "> 99%"
    measurement_window: 30 days
    validation_method: Prometheus query
    query: |
      (
        sum(rate(experiment_completed_total{status="success"}[30d])) /
        sum(rate(experiment_completed_total[30d]))
      ) * 100
    acceptance_criteria: ">= 99"
    current_value: __TO_BE_MEASURED__
    status: [ ] Pass [ ] Fail
    verified_by: __________
    verified_date: __________

  database_query_p99:
    metric: database_query_latency_p99
    target: "< 50ms"
    measurement_window: 7 days
    validation_method: Prometheus query
    query: |
      histogram_quantile(0.99,
        sum(rate(db_query_duration_seconds_bucket[7d])) by (le)
      ) * 1000
    acceptance_criteria: "< 50"
    current_value: __TO_BE_MEASURED__
    status: [ ] Pass [ ] Fail
    verified_by: __________
    verified_date: __________

  kafka_consumer_lag:
    metric: kafka_consumer_lag
    target: "< 1000 messages"
    measurement_window: 7 days
    validation_method: Prometheus query
    query: |
      max_over_time(
        kafka_consumergroup_lag{topic="experiments"}[7d]
      )
    acceptance_criteria: "< 1000"
    current_value: __TO_BE_MEASURED__
    status: [ ] Pass [ ] Fail
    verified_by: __________
    verified_date: __________

  error_rate:
    metric: api_error_rate
    target: "< 0.1%"
    measurement_window: 30 days
    validation_method: Prometheus query
    query: |
      (
        sum(rate(api_request_total{status=~"5.."}[30d])) /
        sum(rate(api_request_total[30d]))
      ) * 100
    acceptance_criteria: "< 0.1"
    current_value: __TO_BE_MEASURED__
    status: [ ] Pass [ ] Fail
    verified_by: __________
    verified_date: __________
```

#### Load Testing Certification

```yaml
# load-testing-certification.yaml
load_tests:
  baseline_load_test:
    description: Sustained baseline load (1000 req/s for 1 hour)
    target_rps: 1000
    duration: 1 hour
    concurrent_users: 500
    test_date: __________
    results:
      avg_latency: __________ ms
      p95_latency: __________ ms
      p99_latency: __________ ms
      error_rate: __________ %
      cpu_utilization: __________ %
      memory_utilization: __________ %
    acceptance_criteria:
      - p99_latency < 100ms
      - error_rate < 0.1%
      - cpu_utilization < 70%
      - memory_utilization < 80%
    status: [ ] Pass [ ] Fail
    report_link: __________
    verified_by: __________

  peak_load_test:
    description: Peak load (5000 req/s for 15 minutes)
    target_rps: 5000
    duration: 15 minutes
    concurrent_users: 2500
    test_date: __________
    results:
      avg_latency: __________ ms
      p95_latency: __________ ms
      p99_latency: __________ ms
      error_rate: __________ %
      cpu_utilization: __________ %
      memory_utilization: __________ %
    acceptance_criteria:
      - p99_latency < 200ms
      - error_rate < 1%
      - cpu_utilization < 85%
      - no pod restarts
    status: [ ] Pass [ ] Fail
    report_link: __________
    verified_by: __________

  stress_test:
    description: Stress test to failure (incremental load until degradation)
    starting_rps: 1000
    increment: 1000 req/s every 5 minutes
    max_rps_achieved: __________ req/s
    degradation_point: __________ req/s
    test_date: __________
    results:
      breaking_point: __________ req/s
      failure_mode: __________
      recovery_time: __________ seconds
    acceptance_criteria:
      - breaking_point > 10000 req/s
      - graceful degradation (no cascading failures)
      - automatic recovery within 5 minutes
    status: [ ] Pass [ ] Fail
    report_link: __________
    verified_by: __________

  endurance_test:
    description: Sustained load over 24 hours
    target_rps: 2000
    duration: 24 hours
    test_date: __________
    results:
      memory_leak_detected: [ ] Yes [ ] No
      avg_latency_drift: __________ %
      error_rate: __________ %
      pod_restarts: __________
    acceptance_criteria:
      - no memory leaks
      - latency drift < 10%
      - error_rate < 0.1%
      - zero unplanned restarts
    status: [ ] Pass [ ] Fail
    report_link: __________
    verified_by: __________

  chaos_test:
    description: Chaos engineering - random pod kills, network delays
    duration: 2 hours
    scenarios:
      - Random pod termination (10% every 10 minutes)
      - Network latency injection (100ms, 10% of traffic)
      - Kafka broker restart
      - Database connection drops
    test_date: __________
    results:
      service_downtime: __________ seconds
      data_loss: [ ] Yes [ ] No
      automatic_recovery: [ ] Yes [ ] No
      recovery_time: __________ seconds
    acceptance_criteria:
      - no user-facing downtime
      - zero data loss
      - automatic recovery < 60 seconds
    status: [ ] Pass [ ] Fail
    report_link: __________
    verified_by: __________
```

#### Performance Validation Script

```bash
#!/bin/bash
# scripts/validate-performance.sh
# Production Performance Validation Script

set -e

RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m'

PROMETHEUS_URL="http://prometheus.llm-research-lab-prod.svc.cluster.local:9090"
VALIDATION_REPORT="/tmp/performance-validation-$(date +%Y%m%d_%H%M%S).txt"

echo "=== LLM-Research-Lab Performance Validation ===" | tee $VALIDATION_REPORT
echo "Timestamp: $(date -u +%Y-%m-%dT%H:%M:%SZ)" | tee -a $VALIDATION_REPORT
echo "" | tee -a $VALIDATION_REPORT

PASS_COUNT=0
FAIL_COUNT=0

validate_metric() {
  local metric_name=$1
  local query=$2
  local threshold=$3
  local operator=$4

  echo -n "Validating ${metric_name}... " | tee -a $VALIDATION_REPORT

  # Query Prometheus
  RESULT=$(curl -s "${PROMETHEUS_URL}/api/v1/query" --data-urlencode "query=${query}" | \
    jq -r '.data.result[0].value[1]')

  if [ -z "$RESULT" ] || [ "$RESULT" == "null" ]; then
    echo -e "${RED}FAIL${NC} (No data)" | tee -a $VALIDATION_REPORT
    ((FAIL_COUNT++))
    return 1
  fi

  # Compare against threshold
  if (( $(echo "$RESULT $operator $threshold" | bc -l) )); then
    echo -e "${GREEN}PASS${NC} (${RESULT})" | tee -a $VALIDATION_REPORT
    ((PASS_COUNT++))
  else
    echo -e "${RED}FAIL${NC} (${RESULT}, expected ${operator} ${threshold})" | tee -a $VALIDATION_REPORT
    ((FAIL_COUNT++))
  fi
}

echo "[SLO Validation]" | tee -a $VALIDATION_REPORT

# API Availability (99.9%)
validate_metric "API Availability" \
  "(sum(rate(api_request_total{status=~\"2..|3..\"}[30d])) / sum(rate(api_request_total[30d]))) * 100" \
  "99.9" \
  ">="

# API Latency P99 (< 100ms)
validate_metric "API Latency P99" \
  "histogram_quantile(0.99, sum(rate(api_request_duration_seconds_bucket[7d])) by (le)) * 1000" \
  "100" \
  "<"

# API Latency P95 (< 50ms)
validate_metric "API Latency P95" \
  "histogram_quantile(0.95, sum(rate(api_request_duration_seconds_bucket[7d])) by (le)) * 1000" \
  "50" \
  "<"

# Experiment Success Rate (> 99%)
validate_metric "Experiment Success Rate" \
  "(sum(rate(experiment_completed_total{status=\"success\"}[30d])) / sum(rate(experiment_completed_total[30d]))) * 100" \
  "99" \
  ">="

# Database Query P99 (< 50ms)
validate_metric "Database Query P99" \
  "histogram_quantile(0.99, sum(rate(db_query_duration_seconds_bucket[7d])) by (le)) * 1000" \
  "50" \
  "<"

# Kafka Consumer Lag (< 1000 messages)
validate_metric "Kafka Consumer Lag" \
  "max_over_time(kafka_consumergroup_lag{topic=\"experiments\"}[7d])" \
  "1000" \
  "<"

# Error Rate (< 0.1%)
validate_metric "Error Rate" \
  "(sum(rate(api_request_total{status=~\"5..\"}[30d])) / sum(rate(api_request_total[30d]))) * 100" \
  "0.1" \
  "<"

echo "" | tee -a $VALIDATION_REPORT
echo "=== Validation Summary ===" | tee -a $VALIDATION_REPORT
echo -e "Passed: ${GREEN}${PASS_COUNT}${NC}" | tee -a $VALIDATION_REPORT
echo -e "Failed: ${RED}${FAIL_COUNT}${NC}" | tee -a $VALIDATION_REPORT
echo "" | tee -a $VALIDATION_REPORT
echo "Full report: ${VALIDATION_REPORT}" | tee -a $VALIDATION_REPORT

if [ $FAIL_COUNT -eq 0 ]; then
  echo -e "${GREEN}All performance validations PASSED${NC}" | tee -a $VALIDATION_REPORT
  exit 0
else
  echo -e "${RED}Performance validation FAILED${NC}" | tee -a $VALIDATION_REPORT
  exit 1
fi
```

---

### 8.1.2 Security Certification

#### Penetration Testing Requirements

```yaml
# penetration-testing-requirements.yaml
penetration_test:
  provider: __________
  test_type: Full-scope web application penetration test
  methodology: OWASP Top 10, SANS Top 25
  scope:
    - API endpoints (/api/v1/*)
    - Authentication/Authorization
    - Database access patterns
    - Infrastructure (Kubernetes, network)
    - Third-party integrations

  test_schedule:
    start_date: __________
    end_date: __________
    duration: 2 weeks

  deliverables:
    - Executive summary
    - Technical report with findings
    - Remediation recommendations
    - Retest verification

  acceptance_criteria:
    critical_vulnerabilities: 0
    high_vulnerabilities: 0
    medium_vulnerabilities: "< 5 (with remediation plan)"
    low_vulnerabilities: Documented

  findings:
    critical: __________
    high: __________
    medium: __________
    low: __________
    informational: __________

  remediation:
    critical_fixed: [ ] Yes [ ] No [ ] N/A
    high_fixed: [ ] Yes [ ] No [ ] N/A
    medium_plan: __________

  retest_date: __________
  retest_status: [ ] Pass [ ] Fail

  sign_off:
    security_lead: __________
    date: __________
    report_link: __________
```

#### Vulnerability Scan Results

```yaml
# vulnerability-scan-certification.yaml
vulnerability_scans:
  container_image_scan:
    tool: Trivy / Clair / Anchore
    scan_date: __________
    images_scanned:
      - llm-research-lab-api:latest
      - experiment-worker:latest
      - background-worker:latest
    results:
      critical: __________
      high: __________
      medium: __________
      low: __________
    acceptance_criteria:
      critical: 0
      high: 0
    status: [ ] Pass [ ] Fail
    report_link: __________
    verified_by: __________

  dependency_scan:
    tool: Cargo audit / OWASP Dependency-Check
    scan_date: __________
    languages:
      - Rust (Cargo)
      - Python (pip)
      - JavaScript (npm)
    results:
      critical: __________
      high: __________
      medium: __________
      low: __________
    acceptance_criteria:
      critical: 0
      high: 0
    status: [ ] Pass [ ] Fail
    report_link: __________
    verified_by: __________

  infrastructure_scan:
    tool: kube-bench / kube-hunter
    scan_date: __________
    scope:
      - Kubernetes cluster configuration
      - RBAC policies
      - Network policies
      - Pod security policies
    results:
      critical: __________
      high: __________
      medium: __________
      low: __________
    acceptance_criteria:
      critical: 0
      high: "< 3 (with mitigation)"
    status: [ ] Pass [ ] Fail
    report_link: __________
    verified_by: __________

  secrets_scan:
    tool: TruffleHog / GitLeaks
    scan_date: __________
    scope:
      - Git repository history
      - Configuration files
      - Environment variables
    results:
      secrets_found: __________
      false_positives: __________
      true_positives: __________
    acceptance_criteria:
      true_positives: 0
    status: [ ] Pass [ ] Fail
    report_link: __________
    verified_by: __________
```

#### Security Controls Checklist

```yaml
# security-controls-checklist.yaml
security_controls:
  authentication:
    - control: JWT-based authentication implemented
      status: [ ] Complete [ ] Incomplete
      verified_by: __________
    - control: Token expiration enforced (15 min access, 7 day refresh)
      status: [ ] Complete [ ] Incomplete
      verified_by: __________
    - control: OAuth2/OIDC integration tested
      status: [ ] Complete [ ] Incomplete
      verified_by: __________
    - control: MFA available for admin accounts
      status: [ ] Complete [ ] Incomplete
      verified_by: __________

  authorization:
    - control: RBAC policies defined and enforced
      status: [ ] Complete [ ] Incomplete
      verified_by: __________
    - control: Principle of least privilege applied
      status: [ ] Complete [ ] Incomplete
      verified_by: __________
    - control: API endpoint authorization tested
      status: [ ] Complete [ ] Incomplete
      verified_by: __________
    - control: Resource-level permissions validated
      status: [ ] Complete [ ] Incomplete
      verified_by: __________

  data_protection:
    - control: Data at rest encrypted (AES-256)
      status: [ ] Complete [ ] Incomplete
      verified_by: __________
    - control: Data in transit encrypted (TLS 1.3)
      status: [ ] Complete [ ] Incomplete
      verified_by: __________
    - control: Database credentials rotated and secured
      status: [ ] Complete [ ] Incomplete
      verified_by: __________
    - control: PII data identified and protected
      status: [ ] Complete [ ] Incomplete
      verified_by: __________

  network_security:
    - control: Network policies restrict pod-to-pod communication
      status: [ ] Complete [ ] Incomplete
      verified_by: __________
    - control: Ingress/egress rules configured
      status: [ ] Complete [ ] Incomplete
      verified_by: __________
    - control: TLS certificates valid and auto-renewed
      status: [ ] Complete [ ] Incomplete
      verified_by: __________
    - control: DDoS protection enabled
      status: [ ] Complete [ ] Incomplete
      verified_by: __________

  secrets_management:
    - control: Kubernetes secrets encrypted at rest
      status: [ ] Complete [ ] Incomplete
      verified_by: __________
    - control: External secret store integrated (Vault/AWS Secrets Manager)
      status: [ ] Complete [ ] Incomplete
      verified_by: __________
    - control: No hardcoded secrets in code/configs
      status: [ ] Complete [ ] Incomplete
      verified_by: __________
    - control: Secret rotation policy defined (90 days)
      status: [ ] Complete [ ] Incomplete
      verified_by: __________

  logging_monitoring:
    - control: Security events logged (auth failures, access violations)
      status: [ ] Complete [ ] Incomplete
      verified_by: __________
    - control: Audit logs immutable and retained (1 year)
      status: [ ] Complete [ ] Incomplete
      verified_by: __________
    - control: Anomaly detection alerts configured
      status: [ ] Complete [ ] Incomplete
      verified_by: __________
    - control: SIEM integration tested
      status: [ ] Complete [ ] Incomplete
      verified_by: __________

  incident_response:
    - control: Security incident response plan documented
      status: [ ] Complete [ ] Incomplete
      verified_by: __________
    - control: Incident response team identified
      status: [ ] Complete [ ] Incomplete
      verified_by: __________
    - control: Incident response runbooks created
      status: [ ] Complete [ ] Incomplete
      verified_by: __________
    - control: Security contact info published
      status: [ ] Complete [ ] Incomplete
      verified_by: __________
```

---

### 8.1.3 Compliance Verification

#### SOC 2 Type II Readiness

```yaml
# soc2-readiness-checklist.yaml
soc2_type2_requirements:
  security:
    - control: CC6.1 - Logical and physical access controls
      evidence:
        - RBAC policies documented
        - Access reviews conducted quarterly
        - MFA enabled for privileged access
      status: [ ] Complete [ ] Incomplete
      auditor_notes: __________

    - control: CC6.6 - Logical access restricted to authorized users
      evidence:
        - User access provisioning process
        - Deprovisioning automation
        - Access recertification logs
      status: [ ] Complete [ ] Incomplete
      auditor_notes: __________

    - control: CC6.7 - Timely restriction of access
      evidence:
        - Automated deprovisioning on termination
        - 24-hour access removal SLA
      status: [ ] Complete [ ] Incomplete
      auditor_notes: __________

    - control: CC7.2 - Detection of security incidents
      evidence:
        - IDS/IPS configured
        - Security monitoring dashboards
        - Alert escalation procedures
      status: [ ] Complete [ ] Incomplete
      auditor_notes: __________

  availability:
    - control: A1.2 - System availability monitoring
      evidence:
        - Uptime SLO defined (99.9%)
        - Monitoring dashboards
        - Historical availability reports
      status: [ ] Complete [ ] Incomplete
      auditor_notes: __________

    - control: A1.3 - Incident response and recovery
      evidence:
        - Incident response plan
        - DR runbooks
        - DR test results (quarterly)
      status: [ ] Complete [ ] Incomplete
      auditor_notes: __________

  confidentiality:
    - control: C1.1 - Confidential information protection
      evidence:
        - Data classification policy
        - Encryption at rest and in transit
        - Access controls on confidential data
      status: [ ] Complete [ ] Incomplete
      auditor_notes: __________

  audit_preparation:
    - item: Policy documentation complete
      status: [ ] Complete [ ] Incomplete
    - item: Evidence collection automated
      status: [ ] Complete [ ] Incomplete
    - item: Control testing completed
      status: [ ] Complete [ ] Incomplete
    - item: Audit artifacts organized
      status: [ ] Complete [ ] Incomplete
    - item: Pre-audit review with auditor
      date: __________
      status: [ ] Complete [ ] Incomplete

  audit_schedule:
    kickoff_date: __________
    fieldwork_dates: __________
    report_delivery: __________
    certification_date: __________

  sign_off:
    compliance_lead: __________
    date: __________
    auditor: __________
    audit_firm: __________
```

#### GDPR Compliance Checklist

```yaml
# gdpr-compliance-checklist.yaml
gdpr_requirements:
  lawful_basis:
    - requirement: Lawful basis for processing identified
      basis: [ ] Consent [ ] Contract [ ] Legal Obligation [ ] Legitimate Interest
      documented: [ ] Yes [ ] No
      verified_by: __________

  data_subject_rights:
    - right: Right to access (Article 15)
      implementation: API endpoint /api/v1/users/{id}/data-export
      tested: [ ] Yes [ ] No
      response_time_sla: 30 days
      verified_by: __________

    - right: Right to erasure (Article 17)
      implementation: Data deletion workflow with retention policy
      tested: [ ] Yes [ ] No
      response_time_sla: 30 days
      verified_by: __________

    - right: Right to rectification (Article 16)
      implementation: User profile update API
      tested: [ ] Yes [ ] No
      response_time_sla: 30 days
      verified_by: __________

    - right: Right to data portability (Article 20)
      implementation: JSON export format
      tested: [ ] Yes [ ] No
      response_time_sla: 30 days
      verified_by: __________

  data_protection_measures:
    - measure: Encryption at rest (AES-256)
      implemented: [ ] Yes [ ] No
      verified_by: __________

    - measure: Encryption in transit (TLS 1.3)
      implemented: [ ] Yes [ ] No
      verified_by: __________

    - measure: Pseudonymization of PII
      implemented: [ ] Yes [ ] No
      verified_by: __________

    - measure: Data minimization policy
      implemented: [ ] Yes [ ] No
      verified_by: __________

  documentation:
    - document: Privacy policy published
      location: https://company.com/privacy
      last_updated: __________
      status: [ ] Complete [ ] Incomplete

    - document: Data processing agreements with vendors
      count: __________
      status: [ ] Complete [ ] Incomplete

    - document: Data breach notification procedure
      location: __________
      status: [ ] Complete [ ] Incomplete

    - document: Records of processing activities (ROPA)
      location: __________
      status: [ ] Complete [ ] Incomplete

  accountability:
    - item: DPO (Data Protection Officer) appointed
      name: __________
      contact: __________
      status: [ ] Complete [ ] Incomplete

    - item: Privacy impact assessment (PIA) completed
      date: __________
      status: [ ] Complete [ ] Incomplete

    - item: Data breach response plan tested
      last_test_date: __________
      status: [ ] Complete [ ] Incomplete

  sign_off:
    legal_counsel: __________
    dpo: __________
    compliance_officer: __________
    date: __________
```

---

### 8.1.4 Documentation Completeness

```yaml
# documentation-completeness-checklist.yaml
documentation_requirements:
  architecture:
    - document: System architecture overview
      location: /docs/architecture/overview.md
      status: [ ] Complete [ ] Incomplete
      last_reviewed: __________
      reviewer: __________

    - document: Component deep-dive docs (API, Workers, Database)
      location: /docs/architecture/components/
      count: __________
      status: [ ] Complete [ ] Incomplete
      last_reviewed: __________

    - document: Network architecture diagrams
      location: /docs/architecture/network-diagram.png
      status: [ ] Complete [ ] Incomplete
      last_reviewed: __________

    - document: Data flow diagrams
      location: /docs/architecture/data-flows/
      status: [ ] Complete [ ] Incomplete
      last_reviewed: __________

  api_documentation:
    - document: OpenAPI specification
      location: /docs/api/openapi.yaml
      version: __________
      status: [ ] Complete [ ] Incomplete
      deployed_to: https://api.company.com/docs

    - document: Rustdoc (auto-generated)
      location: https://company.github.io/llm-research-lab/
      build_status: [ ] Passing [ ] Failing
      last_updated: __________

  operational:
    - document: Runbook index
      location: /docs/runbooks/README.md
      runbook_count: __________
      status: [ ] Complete [ ] Incomplete
      last_reviewed: __________

    - document: On-call playbook
      location: /docs/operations/on-call-playbook.md
      status: [ ] Complete [ ] Incomplete
      last_reviewed: __________

    - document: Deployment procedures
      location: /docs/operations/deployment.md
      status: [ ] Complete [ ] Incomplete
      last_reviewed: __________

    - document: Rollback procedures
      location: /docs/operations/rollback.md
      status: [ ] Complete [ ] Incomplete
      last_reviewed: __________

    - document: Monitoring and alerting guide
      location: /docs/operations/monitoring.md
      status: [ ] Complete [ ] Incomplete
      last_reviewed: __________

  disaster_recovery:
    - document: Disaster recovery plan
      location: /docs/dr/dr-plan.md
      status: [ ] Complete [ ] Incomplete
      last_reviewed: __________

    - document: Backup and restore procedures
      location: /docs/dr/backup-restore.md
      status: [ ] Complete [ ] Incomplete
      last_tested: __________

    - document: Failover procedures
      location: /docs/dr/failover.md
      status: [ ] Complete [ ] Incomplete
      last_tested: __________

  training:
    - document: Operations training curriculum
      location: /docs/training/curriculum.md
      status: [ ] Complete [ ] Incomplete
      last_updated: __________

    - document: Training videos recorded
      location: https://company.lms.com/llm-research-lab
      count: __________
      status: [ ] Complete [ ] Incomplete

    - document: Certification exam questions
      count: __________
      status: [ ] Complete [ ] Incomplete

  compliance:
    - document: Security policies
      location: /docs/compliance/security-policies.md
      status: [ ] Complete [ ] Incomplete
      approved_by: __________

    - document: Data retention policy
      location: /docs/compliance/data-retention.md
      status: [ ] Complete [ ] Incomplete
      approved_by: __________

    - document: Incident response plan
      location: /docs/compliance/incident-response.md
      status: [ ] Complete [ ] Incomplete
      approved_by: __________

  sign_off:
    documentation_lead: __________
    technical_writer: __________
    date: __________
```

---

### 8.1.5 Training Completion

```yaml
# training-completion-certification.yaml
training_certification:
  l1_operators:
    required_count: __________
    certified_count: __________
    certification_rate: __________ %
    acceptance_criteria: 100%
    status: [ ] Complete [ ] Incomplete

    certified_individuals:
      - name: __________
        cert_date: __________
        cert_level: L1
      - name: __________
        cert_date: __________
        cert_level: L1

  l2_engineers:
    required_count: __________
    certified_count: __________
    certification_rate: __________ %
    acceptance_criteria: 100%
    status: [ ] Complete [ ] Incomplete

    certified_individuals:
      - name: __________
        cert_date: __________
        cert_level: L2
      - name: __________
        cert_date: __________
        cert_level: L2

  l3_sres:
    required_count: __________
    certified_count: __________
    certification_rate: __________ %
    acceptance_criteria: 100%
    status: [ ] Complete [ ] Incomplete

    certified_individuals:
      - name: __________
        cert_date: __________
        cert_level: L3

  shadow_period:
    completion_rate: __________ %
    acceptance_criteria: 100%
    average_incidents_shadowed: __________
    status: [ ] Complete [ ] Incomplete

  knowledge_transfer_sessions:
    total_sessions: __________
    completed_sessions: __________
    completion_rate: __________ %
    recordings_available: [ ] Yes [ ] No
    status: [ ] Complete [ ] Incomplete

  sign_off:
    training_lead: __________
    operations_manager: __________
    date: __________
```

---

## 8.2 Stakeholder Sign-off Matrix

### 8.2.1 Technical Sign-off

```yaml
# technical-sign-off-matrix.yaml
technical_sign_off:
  vp_engineering:
    name: __________
    email: __________
    sign_off_criteria:
      - All performance SLOs met
      - Load testing passed
      - Zero critical bugs in production
      - CI/CD pipeline operational
      - Monitoring and alerting configured

    checklist:
      - item: Performance validation complete
        status: [ ] Approved [ ] Rejected [ ] Pending
        comments: __________

      - item: Architecture review complete
        status: [ ] Approved [ ] Rejected [ ] Pending
        comments: __________

      - item: Code quality standards met
        status: [ ] Approved [ ] Rejected [ ] Pending
        comments: __________

      - item: Technical debt documented
        status: [ ] Approved [ ] Rejected [ ] Pending
        comments: __________

    final_approval:
      decision: [ ] Approved [ ] Rejected [ ] Conditional
      conditions: __________
      signature: __________
      date: __________

  security_lead:
    name: __________
    email: __________
    sign_off_criteria:
      - Penetration test passed (0 critical, 0 high)
      - Vulnerability scans clean
      - Security controls implemented
      - Secrets management validated
      - Incident response plan ready

    checklist:
      - item: Penetration testing complete
        status: [ ] Approved [ ] Rejected [ ] Pending
        comments: __________

      - item: Vulnerability remediation complete
        status: [ ] Approved [ ] Rejected [ ] Pending
        comments: __________

      - item: Security controls verified
        status: [ ] Approved [ ] Rejected [ ] Pending
        comments: __________

      - item: Compliance requirements met
        status: [ ] Approved [ ] Rejected [ ] Pending
        comments: __________

    final_approval:
      decision: [ ] Approved [ ] Rejected [ ] Conditional
      conditions: __________
      signature: __________
      date: __________

  sre_lead:
    name: __________
    email: __________
    sign_off_criteria:
      - Operations team trained and certified
      - Runbooks complete and tested
      - Monitoring dashboards operational
      - On-call rotation staffed
      - DR procedures tested

    checklist:
      - item: Operations readiness validated
        status: [ ] Approved [ ] Rejected [ ] Pending
        comments: __________

      - item: Runbooks tested
        status: [ ] Approved [ ] Rejected [ ] Pending
        comments: __________

      - item: DR drills passed
        status: [ ] Approved [ ] Rejected [ ] Pending
        comments: __________

      - item: On-call prepared
        status: [ ] Approved [ ] Rejected [ ] Pending
        comments: __________

    final_approval:
      decision: [ ] Approved [ ] Rejected [ ] Conditional
      conditions: __________
      signature: __________
      date: __________

  infrastructure_lead:
    name: __________
    email: __________
    sign_off_criteria:
      - Production infrastructure provisioned
      - Kubernetes cluster validated
      - Network policies configured
      - Backup systems operational
      - Capacity planning complete

    checklist:
      - item: Infrastructure provisioned
        status: [ ] Approved [ ] Rejected [ ] Pending
        comments: __________

      - item: HA configuration validated
        status: [ ] Approved [ ] Rejected [ ] Pending
        comments: __________

      - item: Backup systems tested
        status: [ ] Approved [ ] Rejected [ ] Pending
        comments: __________

      - item: Scaling policies configured
        status: [ ] Approved [ ] Rejected [ ] Pending
        comments: __________

    final_approval:
      decision: [ ] Approved [ ] Rejected [ ] Conditional
      conditions: __________
      signature: __________
      date: __________
```

---

### 8.2.2 Business Sign-off

```yaml
# business-sign-off-matrix.yaml
business_sign_off:
  product_vp:
    name: __________
    email: __________
    sign_off_criteria:
      - All required features complete
      - User acceptance testing passed
      - Product documentation ready
      - Customer communication plan ready
      - Launch timeline approved

    checklist:
      - item: Feature completeness verified
        status: [ ] Approved [ ] Rejected [ ] Pending
        comments: __________

      - item: UAT results acceptable
        status: [ ] Approved [ ] Rejected [ ] Pending
        comments: __________

      - item: Go-to-market ready
        status: [ ] Approved [ ] Rejected [ ] Pending
        comments: __________

      - item: Customer support prepared
        status: [ ] Approved [ ] Rejected [ ] Pending
        comments: __________

    final_approval:
      decision: [ ] Approved [ ] Rejected [ ] Conditional
      conditions: __________
      signature: __________
      date: __________

  legal_counsel:
    name: __________
    email: __________
    sign_off_criteria:
      - Terms of service reviewed
      - Privacy policy compliant
      - Data processing agreements in place
      - Regulatory requirements met
      - IP protection verified

    checklist:
      - item: Legal documentation complete
        status: [ ] Approved [ ] Rejected [ ] Pending
        comments: __________

      - item: Compliance verified
        status: [ ] Approved [ ] Rejected [ ] Pending
        comments: __________

      - item: Vendor agreements signed
        status: [ ] Approved [ ] Rejected [ ] Pending
        comments: __________

      - item: Data protection validated
        status: [ ] Approved [ ] Rejected [ ] Pending
        comments: __________

    final_approval:
      decision: [ ] Approved [ ] Rejected [ ] Conditional
      conditions: __________
      signature: __________
      date: __________

  compliance_officer:
    name: __________
    email: __________
    sign_off_criteria:
      - SOC 2 audit ready
      - GDPR compliance verified
      - Audit trails configured
      - Policy documentation complete
      - Training records maintained

    checklist:
      - item: SOC 2 readiness validated
        status: [ ] Approved [ ] Rejected [ ] Pending
        comments: __________

      - item: GDPR compliance confirmed
        status: [ ] Approved [ ] Rejected [ ] Pending
        comments: __________

      - item: Audit requirements met
        status: [ ] Approved [ ] Rejected [ ] Pending
        comments: __________

      - item: Policy compliance verified
        status: [ ] Approved [ ] Rejected [ ] Pending
        comments: __________

    final_approval:
      decision: [ ] Approved [ ] Rejected [ ] Conditional
      conditions: __________
      signature: __________
      date: __________

  finance_controller:
    name: __________
    email: __________
    sign_off_criteria:
      - Budget approved
      - Cost projections reviewed
      - Billing system integrated
      - Revenue tracking ready
      - Financial controls in place

    checklist:
      - item: Infrastructure costs approved
        status: [ ] Approved [ ] Rejected [ ] Pending
        comments: __________

      - item: Operating budget confirmed
        status: [ ] Approved [ ] Rejected [ ] Pending
        comments: __________

      - item: Cost monitoring configured
        status: [ ] Approved [ ] Rejected [ ] Pending
        comments: __________

      - item: Financial reporting ready
        status: [ ] Approved [ ] Rejected [ ] Pending
        comments: __________

    final_approval:
      decision: [ ] Approved [ ] Rejected [ ] Conditional
      conditions: __________
      signature: __________
      date: __________
```

---

### 8.2.3 Executive Approval

```yaml
# executive-approval-matrix.yaml
executive_approval:
  cto:
    name: __________
    email: __________
    sign_off_criteria:
      - All technical sign-offs obtained
      - Risk assessment reviewed
      - Technical strategy aligned
      - Innovation goals met
      - Engineering team confident

    review_items:
      - item: Technical risk assessment
        risk_level: [ ] Low [ ] Medium [ ] High
        mitigation: __________
        acceptable: [ ] Yes [ ] No

      - item: Architectural alignment
        strategic_fit: [ ] Excellent [ ] Good [ ] Acceptable [ ] Poor
        comments: __________

      - item: Team readiness
        confidence_level: [ ] High [ ] Medium [ ] Low
        concerns: __________

      - item: Long-term maintainability
        rating: [ ] Excellent [ ] Good [ ] Acceptable [ ] Poor
        comments: __________

    final_approval:
      decision: [ ] Approved [ ] Rejected [ ] Conditional
      conditions: __________
      signature: __________
      date: __________

  ceo:
    name: __________
    email: __________
    sign_off_criteria:
      - All stakeholder sign-offs obtained
      - Business value validated
      - Risk acceptable
      - Go-to-market ready
      - Strategic alignment confirmed

    review_items:
      - item: Business impact assessment
        impact: [ ] High [ ] Medium [ ] Low
        value_proposition: __________

      - item: Risk vs. reward analysis
        risk_level: [ ] Low [ ] Medium [ ] High
        reward: __________
        acceptable: [ ] Yes [ ] No

      - item: Market readiness
        readiness: [ ] Ready [ ] Not Ready
        timeline: __________

      - item: Stakeholder alignment
        alignment: [ ] Full [ ] Partial [ ] Low
        concerns: __________

    final_approval:
      decision: [ ] Approved [ ] Rejected [ ] Conditional
      conditions: __________
      signature: __________
      date: __________
      authorization_code: __________
```

---

### 8.2.4 Sign-off Form Template

```yaml
# sign-off-form-template.yaml
sign_off_form:
  metadata:
    project_name: LLM-Research-Lab
    version: 1.0.0
    environment: Production
    go_live_date: __________
    form_id: SIGNOFF-__________
    created_date: __________

  stakeholder:
    name: __________
    title: __________
    department: __________
    email: __________
    phone: __________

  certification_statement: |
    I, [NAME], in my capacity as [TITLE], hereby certify that:

    1. I have reviewed all materials and evidence relevant to my area of responsibility
    2. All criteria within my scope of authority have been met
    3. All identified risks have been reviewed and are acceptable or mitigated
    4. My team is prepared to support this production deployment
    5. I authorize the deployment of LLM-Research-Lab to production

  review_checklist:
    - item: Reviewed all documentation relevant to my role
      checked: [ ] Yes [ ] No

    - item: Verified all acceptance criteria met
      checked: [ ] Yes [ ] No

    - item: Reviewed and accepted identified risks
      checked: [ ] Yes [ ] No

    - item: Confirmed team readiness
      checked: [ ] Yes [ ] No

    - item: No outstanding blockers in my area
      checked: [ ] Yes [ ] No

  risk_acknowledgment:
    risks_identified: __________
    mitigation_acceptable: [ ] Yes [ ] No
    residual_risk: [ ] Low [ ] Medium [ ] High
    comments: __________

  conditions:
    conditional_approval: [ ] Yes [ ] No
    conditions_list: __________
    conditions_must_be_met_by: __________

  final_decision:
    decision: [ ] Approve [ ] Reject [ ] Approve with Conditions
    comments: __________

    signature: __________
    printed_name: __________
    date: __________
    timestamp: __________

  attachments:
    - name: __________
      type: __________
      location: __________
```

---

## 8.3 Go-Live Criteria

### 8.3.1 Blocker Resolution

```yaml
# blocker-resolution-tracking.yaml
blocker_tracking:
  critical_blockers:
    - id: BLOCK-001
      title: __________
      severity: Critical
      description: __________
      impact: __________
      assigned_to: __________
      created_date: __________
      due_date: __________
      status: [ ] Open [ ] In Progress [ ] Resolved [ ] Verified
      resolution: __________
      verified_by: __________
      verified_date: __________

    - id: BLOCK-002
      title: __________
      severity: Critical
      description: __________
      impact: __________
      assigned_to: __________
      created_date: __________
      due_date: __________
      status: [ ] Open [ ] In Progress [ ] Resolved [ ] Verified
      resolution: __________
      verified_by: __________
      verified_date: __________

  high_priority_issues:
    - id: ISSUE-001
      title: __________
      severity: High
      description: __________
      impact: __________
      assigned_to: __________
      created_date: __________
      due_date: __________
      status: [ ] Open [ ] In Progress [ ] Resolved [ ] Deferred
      resolution_or_deferral_reason: __________
      approved_by: __________

  acceptance_criteria:
    critical_blockers_resolved: 0 open
    high_priority_resolved_or_deferred: All items addressed
    medium_low_acceptable: Documented and triaged

  go_live_gate:
    all_critical_blockers_resolved: [ ] Yes [ ] No
    all_high_priority_addressed: [ ] Yes [ ] No
    blocker_list_reviewed_by: __________
    approval_date: __________
    decision: [ ] Proceed [ ] Delay
```

---

### 8.3.2 Rollback Testing

```yaml
# rollback-testing-certification.yaml
rollback_testing:
  rollback_scenarios:
    deployment_rollback:
      description: Rollback Kubernetes deployment to previous version
      test_date: __________
      test_environment: Staging
      steps_executed:
        - Deploy version 1.0.0 to staging
        - Verify functionality
        - Deploy version 1.1.0 with intentional breaking change
        - Detect failure via health checks
        - Execute rollback to 1.0.0
        - Verify service restoration

      results:
        rollback_time: __________ seconds
        data_loss: [ ] Yes [ ] No
        service_restored: [ ] Yes [ ] No
        user_impact: [ ] None [ ] Minimal [ ] Moderate [ ] Severe

      acceptance_criteria:
        - Rollback completes within 5 minutes
        - Zero data loss
        - Service fully restored
        - No manual intervention required

      status: [ ] Pass [ ] Fail
      issues_encountered: __________
      verified_by: __________

    database_migration_rollback:
      description: Rollback database schema migration
      test_date: __________
      test_environment: Staging
      steps_executed:
        - Backup current database state
        - Apply migration v2
        - Verify migration success
        - Execute rollback migration
        - Verify database state matches backup
        - Validate application functionality

      results:
        rollback_time: __________ seconds
        data_loss: [ ] Yes [ ] No
        schema_restored: [ ] Yes [ ] No
        data_integrity: [ ] Verified [ ] Issues Found

      acceptance_criteria:
        - Rollback completes within 10 minutes
        - Zero data loss
        - Schema fully restored
        - All foreign keys intact

      status: [ ] Pass [ ] Fail
      issues_encountered: __________
      verified_by: __________

    configuration_rollback:
      description: Rollback configuration changes
      test_date: __________
      test_environment: Staging
      steps_executed:
        - Apply new ConfigMap/Secret
        - Restart affected pods
        - Detect configuration issue
        - Revert to previous ConfigMap
        - Restart affected pods
        - Verify service functionality

      results:
        rollback_time: __________ seconds
        service_downtime: __________ seconds
        data_loss: [ ] Yes [ ] No

      acceptance_criteria:
        - Rollback completes within 3 minutes
        - Service downtime < 60 seconds
        - Zero data loss

      status: [ ] Pass [ ] Fail
      issues_encountered: __________
      verified_by: __________

  rollback_automation:
    automated_rollback_enabled: [ ] Yes [ ] No
    rollback_triggers:
      - Health check failures (3 consecutive)
      - Error rate > 5% for 2 minutes
      - Manual trigger via CI/CD
    automated_rollback_tested: [ ] Yes [ ] No
    test_date: __________
    test_results: __________

  rollback_documentation:
    rollback_runbook: /docs/operations/rollback.md
    rollback_decision_tree: /docs/operations/rollback-decision-tree.md
    rollback_commands_documented: [ ] Yes [ ] No
    rollback_tested_in_prod_like_env: [ ] Yes [ ] No

  sign_off:
    deployment_lead: __________
    sre_lead: __________
    date: __________
    rollback_ready: [ ] Yes [ ] No
```

---

### 8.3.3 On-Call Staffing

```yaml
# on-call-staffing-readiness.yaml
on_call_readiness:
  coverage_model:
    model: 24/7 follow-the-sun
    timezones:
      - Americas (UTC-8 to UTC-5)
      - EMEA (UTC+0 to UTC+3)
      - APAC (UTC+8 to UTC+10)

    tiers:
      - tier: L1 (First Response)
        required_coverage: 24/7
        current_coverage: __________ %
        staff_count: __________

      - tier: L2 (Engineering)
        required_coverage: 24/7
        current_coverage: __________ %
        staff_count: __________

      - tier: L3 (SRE/Escalation)
        required_coverage: 24/7
        current_coverage: __________ %
        staff_count: __________

  rotation_schedule:
    rotation_type: Weekly
    handoff_day: Monday
    handoff_time: 09:00 local time

    primary_rotation:
      - week: Week of __________
        timezone: Americas
        primary: __________
        backup: __________
        status: [ ] Confirmed [ ] Pending

      - week: Week of __________
        timezone: EMEA
        primary: __________
        backup: __________
        status: [ ] Confirmed [ ] Pending

      - week: Week of __________
        timezone: APAC
        primary: __________
        backup: __________
        status: [ ] Confirmed [ ] Pending

  escalation_chain:
    - level: L1 Response
      response_time: 15 minutes
      personnel:
        - name: __________
          contact: __________
        - name: __________
          contact: __________

    - level: L2 Engineering
      escalation_time: 30 minutes (if unresolved)
      personnel:
        - name: __________
          contact: __________
        - name: __________
          contact: __________

    - level: L3 SRE
      escalation_time: 1 hour (if unresolved)
      personnel:
        - name: __________
          contact: __________

    - level: Engineering Manager
      escalation_time: 2 hours (P0/P1 only)
      personnel:
        - name: __________
          contact: __________

  pagerduty_configuration:
    service_configured: [ ] Yes [ ] No
    escalation_policies: [ ] Configured [ ] Not Configured
    schedules_created: [ ] Yes [ ] No
    notification_rules: [ ] Configured [ ] Not Configured
    integration_tested: [ ] Yes [ ] No
    test_date: __________

  on_call_tools:
    - tool: PagerDuty
      access_verified: [ ] Yes [ ] No
      training_complete: [ ] Yes [ ] No

    - tool: Kubernetes (kubectl)
      access_verified: [ ] Yes [ ] No
      training_complete: [ ] Yes [ ] No

    - tool: Grafana
      access_verified: [ ] Yes [ ] No
      training_complete: [ ] Yes [ ] No

    - tool: Incident management (Jira)
      access_verified: [ ] Yes [ ] No
      training_complete: [ ] Yes [ ] No

  readiness_verification:
    - verification: All shifts staffed for next 4 weeks
      status: [ ] Complete [ ] Incomplete

    - verification: Backup on-call identified for each shift
      status: [ ] Complete [ ] Incomplete

    - verification: All on-call personnel certified
      status: [ ] Complete [ ] Incomplete

    - verification: Escalation chain tested
      status: [ ] Complete [ ] Incomplete
      test_date: __________

    - verification: PagerDuty test pages sent and acknowledged
      status: [ ] Complete [ ] Incomplete
      test_date: __________

  sign_off:
    operations_manager: __________
    sre_lead: __________
    date: __________
    on_call_ready: [ ] Yes [ ] No
```

---

### 8.3.4 Communication Plan

```yaml
# communication-plan-readiness.yaml
communication_plan:
  internal_communications:
    pre_launch:
      - audience: Engineering Team
        message: Go-live timeline, roles, war room details
        channel: Email + Slack #engineering
        timing: 1 week before
        owner: Engineering Manager
        status: [ ] Sent [ ] Draft [ ] Not Started

      - audience: Product Team
        message: Launch status, feature availability, known limitations
        channel: Email + Slack #product
        timing: 3 days before
        owner: Product Manager
        status: [ ] Sent [ ] Draft [ ] Not Started

      - audience: Customer Support
        message: New features, FAQs, support procedures
        channel: Email + Support Portal
        timing: 1 week before
        owner: Support Manager
        status: [ ] Sent [ ] Draft [ ] Not Started

      - audience: Sales Team
        message: Product updates, customer messaging, demo availability
        channel: Email + Slack #sales
        timing: 1 week before
        owner: Sales Enablement
        status: [ ] Sent [ ] Draft [ ] Not Started

    during_launch:
      - audience: All Hands (Company-wide)
        message: Go-live announcement, war room status updates
        channel: Slack #general
        frequency: Hourly during deployment
        owner: CTO
        status: [ ] Ready [ ] Not Ready

      - audience: Incident Response Team
        message: Real-time deployment status, issues, resolutions
        channel: Slack #incidents
        frequency: Continuous
        owner: Deployment Lead
        status: [ ] Ready [ ] Not Ready

    post_launch:
      - audience: All Hands
        message: Successful launch announcement, metrics, next steps
        channel: Email + Slack #general
        timing: Within 24 hours
        owner: CEO
        status: [ ] Drafted [ ] Not Started

      - audience: Engineering Team
        message: Post-mortem meeting invite, lessons learned
        channel: Calendar invite
        timing: Within 1 week
        owner: Engineering Manager
        status: [ ] Scheduled [ ] Not Scheduled

  external_communications:
    customer_notifications:
      - audience: All Customers
        message: New features announcement, migration guide (if applicable)
        channel: Email + In-app notification
        timing: Go-live day
        owner: Product Marketing
        status: [ ] Approved [ ] Draft [ ] Not Started
        legal_review: [ ] Approved [ ] Pending

      - audience: Beta Customers
        message: Thank you, graduation to GA, new features
        channel: Email
        timing: Go-live day
        owner: Product Manager
        status: [ ] Approved [ ] Draft [ ] Not Started

    public_announcements:
      - announcement: Blog post
        title: __________
        content_status: [ ] Published [ ] Draft [ ] Not Started
        publish_date: __________
        owner: Marketing

      - announcement: Social media
        platforms: Twitter, LinkedIn
        content_status: [ ] Scheduled [ ] Draft [ ] Not Started
        publish_date: __________
        owner: Social Media Manager

      - announcement: Press release (if applicable)
        content_status: [ ] Approved [ ] Draft [ ] Not Started [ ] N/A
        distribution_date: __________
        owner: PR Team

  status_page:
    status_page_url: https://status.company.com
    scheduled_maintenance:
      scheduled: [ ] Yes [ ] No
      maintenance_window: __________
      notification_sent: [ ] Yes [ ] No
      notification_date: __________

    incident_templates:
      - template: Deployment in progress
        status: [ ] Ready [ ] Not Ready
      - template: Performance degradation
        status: [ ] Ready [ ] Not Ready
      - template: Service restored
        status: [ ] Ready [ ] Not Ready

  communication_channels:
    - channel: "#war-room-llm-research-lab"
      purpose: Real-time deployment coordination
      created: [ ] Yes [ ] No
      participants: Engineering, Product, SRE, Management

    - channel: "#incidents"
      purpose: Incident tracking and resolution
      created: [ ] Yes [ ] No
      participants: On-call, SRE, Engineering

    - channel: Zoom war room
      purpose: Voice/video coordination
      meeting_link: __________
      created: [ ] Yes [ ] No

  sign_off:
    communications_lead: __________
    product_marketing: __________
    date: __________
    communications_ready: [ ] Yes [ ] No
```

---

### 8.3.5 War Room Setup

```yaml
# war-room-setup.yaml
war_room_configuration:
  physical_setup:
    location: __________
    room_reserved: [ ] Yes [ ] No
    reservation_dates: __________ to __________
    capacity: __________ people
    equipment:
      - Large monitors for dashboards: [ ] Ready [ ] Not Ready
      - Conference call capability: [ ] Ready [ ] Not Ready
      - Whiteboards: [ ] Ready [ ] Not Ready
      - Power/network for laptops: [ ] Ready [ ] Not Ready

  virtual_setup:
    zoom_meeting:
      meeting_link: __________
      meeting_id: __________
      passcode: __________
      host: __________
      co_hosts: __________
      scheduled: [ ] Yes [ ] No
      recurring: [ ] Yes [ ] No
      duration: 24 hours

    slack_channel:
      channel_name: "#war-room-llm-research-lab"
      created: [ ] Yes [ ] No
      members_added: __________ members
      pinned_messages:
        - Runbook links
        - Dashboard links
        - Escalation contacts
        - Rollback commands
      status: [ ] Ready [ ] Not Ready

  dashboards:
    - dashboard: Deployment Status Dashboard
      url: https://grafana.company.com/d/deployment
      access_verified: [ ] Yes [ ] No
      displayed_in_war_room: [ ] Yes [ ] No

    - dashboard: System Health Dashboard
      url: https://grafana.company.com/d/health
      access_verified: [ ] Yes [ ] No
      displayed_in_war_room: [ ] Yes [ ] No

    - dashboard: Error Tracking Dashboard
      url: https://grafana.company.com/d/errors
      access_verified: [ ] Yes [ ] No
      displayed_in_war_room: [ ] Yes [ ] No

    - dashboard: Performance Metrics Dashboard
      url: https://grafana.company.com/d/performance
      access_verified: [ ] Yes [ ] No
      displayed_in_war_room: [ ] Yes [ ] No

  roles_and_responsibilities:
    - role: Incident Commander
      name: __________
      responsibilities:
        - Overall coordination
        - Decision authority
        - Stakeholder communication
      contact: __________
      backup: __________

    - role: Deployment Lead
      name: __________
      responsibilities:
        - Execute deployment steps
        - Monitor deployment progress
        - Trigger rollback if needed
      contact: __________
      backup: __________

    - role: SRE Lead
      name: __________
      responsibilities:
        - Monitor system health
        - Investigate anomalies
        - Coordinate with on-call
      contact: __________
      backup: __________

    - role: Communications Lead
      name: __________
      responsibilities:
        - Status updates (internal/external)
        - Stakeholder notifications
        - Status page updates
      contact: __________
      backup: __________

    - role: Scribe
      name: __________
      responsibilities:
        - Document timeline
        - Record decisions
        - Track action items
      contact: __________
      backup: __________

  war_room_schedule:
    go_live_date: __________
    war_room_open: __________ (2 hours before deployment)
    war_room_close: __________ (4 hours after verification)
    extended_monitoring: 24 hours post-launch

    shifts:
      - shift: Initial Deployment
        time: __________ to __________
        staff: Full team

      - shift: Post-Deployment Monitoring
        time: __________ to __________
        staff: Reduced team (IC, SRE, Deployment Lead)

      - shift: Extended Monitoring
        time: __________ to __________
        staff: On-call only

  runbook_access:
    - runbook: Deployment Procedure
      location: /docs/operations/deployment.md
      accessible: [ ] Yes [ ] No
      printed_copy: [ ] Yes [ ] No

    - runbook: Rollback Procedure
      location: /docs/operations/rollback.md
      accessible: [ ] Yes [ ] No
      printed_copy: [ ] Yes [ ] No

    - runbook: Emergency Contacts
      location: /docs/operations/emergency-contacts.md
      accessible: [ ] Yes [ ] No
      printed_copy: [ ] Yes [ ] No

  decision_log:
    template_ready: [ ] Yes [ ] No
    scribe_assigned: [ ] Yes [ ] No
    format: |
      Timestamp | Decision | Made By | Rationale | Outcome

  go_no_go_checklist:
    - item: All stakeholder sign-offs obtained
      status: [ ] Go [ ] No-Go

    - item: All blockers resolved
      status: [ ] Go [ ] No-Go

    - item: Rollback tested and verified
      status: [ ] Go [ ] No-Go

    - item: On-call staffed for 48 hours
      status: [ ] Go [ ] No-Go

    - item: War room ready
      status: [ ] Go [ ] No-Go

    - item: Communication plan ready
      status: [ ] Go [ ] No-Go

    - item: Monitoring dashboards operational
      status: [ ] Go [ ] No-Go

    - item: No other major deployments in flight
      status: [ ] Go [ ] No-Go

  final_go_no_go_decision:
    decision: [ ] GO [ ] NO-GO
    decided_by: __________
    date: __________
    time: __________
    notes: __________

  sign_off:
    incident_commander: __________
    deployment_lead: __________
    sre_lead: __________
    date: __________
    war_room_ready: [ ] Yes [ ] No
```

---

## Appendix A: Certification Templates

### A.1 Performance Certification Template

```markdown
# Performance Certification Report

**Project**: LLM-Research-Lab
**Version**: 1.0.0
**Environment**: Production
**Test Date**: __________
**Certified By**: __________

---

## Executive Summary

This document certifies that LLM-Research-Lab has successfully met all performance SLOs and is ready for production deployment.

**Certification Status**: [ ] PASS [ ] FAIL

---

## SLO Validation Results

| SLO | Target | Actual | Status |
|-----|--------|--------|--------|
| API Availability | 99.9% | ____% | [ ] Pass [ ] Fail |
| API Latency P99 | < 100ms | ____ms | [ ] Pass [ ] Fail |
| API Latency P95 | < 50ms | ____ms | [ ] Pass [ ] Fail |
| Experiment Success Rate | > 99% | ____% | [ ] Pass [ ] Fail |
| Database Query P99 | < 50ms | ____ms | [ ] Pass [ ] Fail |
| Kafka Consumer Lag | < 1000 msg | ____ msg | [ ] Pass [ ] Fail |
| Error Rate | < 0.1% | ____% | [ ] Pass [ ] Fail |

---

## Load Testing Results

### Baseline Load Test
- **Target**: 1000 req/s for 1 hour
- **Result**: [ ] Pass [ ] Fail
- **P99 Latency**: ____ms
- **Error Rate**: ____%

### Peak Load Test
- **Target**: 5000 req/s for 15 minutes
- **Result**: [ ] Pass [ ] Fail
- **P99 Latency**: ____ms
- **Error Rate**: ____%

### Stress Test
- **Breaking Point**: ____ req/s
- **Result**: [ ] Pass [ ] Fail
- **Recovery**: [ ] Automatic [ ] Manual

### Endurance Test
- **Duration**: 24 hours
- **Result**: [ ] Pass [ ] Fail
- **Memory Leak**: [ ] None [ ] Detected

---

## Performance Bottlenecks Identified

1. __________
2. __________
3. __________

**Mitigation**: __________

---

## Certification

I hereby certify that LLM-Research-Lab meets all performance requirements for production deployment.

**Signature**: __________
**Name**: __________
**Title**: Performance Engineer / SRE Lead
**Date**: __________
```

---

### A.2 Security Certification Template

```markdown
# Security Certification Report

**Project**: LLM-Research-Lab
**Version**: 1.0.0
**Environment**: Production
**Assessment Date**: __________
**Certified By**: __________

---

## Executive Summary

This document certifies that LLM-Research-Lab has undergone comprehensive security testing and meets all security requirements for production deployment.

**Certification Status**: [ ] PASS [ ] FAIL

---

## Penetration Testing Results

**Test Date**: __________
**Vendor**: __________
**Methodology**: OWASP Top 10, SANS Top 25

| Severity | Count | Remediated | Outstanding |
|----------|-------|------------|-------------|
| Critical | ____ | ____ | ____ |
| High | ____ | ____ | ____ |
| Medium | ____ | ____ | ____ |
| Low | ____ | ____ | ____ |

**Acceptance Criteria Met**: [ ] Yes [ ] No

---

## Vulnerability Scan Results

### Container Images
- **Critical**: ____
- **High**: ____
- **Status**: [ ] Pass [ ] Fail

### Dependencies
- **Critical**: ____
- **High**: ____
- **Status**: [ ] Pass [ ] Fail

### Infrastructure
- **Critical**: ____
- **High**: ____
- **Status**: [ ] Pass [ ] Fail

---

## Security Controls Validation

| Control | Status | Verified By |
|---------|--------|-------------|
| Authentication | [ ] Pass [ ] Fail | __________ |
| Authorization (RBAC) | [ ] Pass [ ] Fail | __________ |
| Encryption at Rest | [ ] Pass [ ] Fail | __________ |
| Encryption in Transit | [ ] Pass [ ] Fail | __________ |
| Secrets Management | [ ] Pass [ ] Fail | __________ |
| Network Policies | [ ] Pass [ ] Fail | __________ |
| Logging & Monitoring | [ ] Pass [ ] Fail | __________ |
| Incident Response | [ ] Pass [ ] Fail | __________ |

---

## Outstanding Security Items

1. __________
2. __________
3. __________

**Risk Acceptance**: [ ] Accepted [ ] Not Accepted
**Accepted By**: __________

---

## Certification

I hereby certify that LLM-Research-Lab meets all security requirements for production deployment.

**Signature**: __________
**Name**: __________
**Title**: Security Lead / CISO
**Date**: __________
```

---

## Appendix B: Sign-off Forms

### B.1 Master Sign-off Tracking Sheet

```yaml
# master-sign-off-tracking.yaml
master_sign_off_tracking:
  project: LLM-Research-Lab
  version: 1.0.0
  go_live_date: __________

  technical_sign_offs:
    - stakeholder: VP Engineering
      required: true
      status: [ ] Pending [ ] Approved [ ] Rejected [ ] Conditional
      date_signed: __________
      conditions: __________

    - stakeholder: Security Lead
      required: true
      status: [ ] Pending [ ] Approved [ ] Rejected [ ] Conditional
      date_signed: __________
      conditions: __________

    - stakeholder: SRE Lead
      required: true
      status: [ ] Pending [ ] Approved [ ] Rejected [ ] Conditional
      date_signed: __________
      conditions: __________

    - stakeholder: Infrastructure Lead
      required: true
      status: [ ] Pending [ ] Approved [ ] Rejected [ ] Conditional
      date_signed: __________
      conditions: __________

  business_sign_offs:
    - stakeholder: Product VP
      required: true
      status: [ ] Pending [ ] Approved [ ] Rejected [ ] Conditional
      date_signed: __________
      conditions: __________

    - stakeholder: Legal Counsel
      required: true
      status: [ ] Pending [ ] Approved [ ] Rejected [ ] Conditional
      date_signed: __________
      conditions: __________

    - stakeholder: Compliance Officer
      required: true
      status: [ ] Pending [ ] Approved [ ] Rejected [ ] Conditional
      date_signed: __________
      conditions: __________

    - stakeholder: Finance Controller
      required: false
      status: [ ] Pending [ ] Approved [ ] Rejected [ ] Conditional
      date_signed: __________
      conditions: __________

  executive_sign_offs:
    - stakeholder: CTO
      required: true
      status: [ ] Pending [ ] Approved [ ] Rejected [ ] Conditional
      date_signed: __________
      conditions: __________

    - stakeholder: CEO
      required: true
      status: [ ] Pending [ ] Approved [ ] Rejected [ ] Conditional
      date_signed: __________
      conditions: __________

  summary:
    total_required: __________
    total_approved: __________
    total_conditional: __________
    total_rejected: __________
    total_pending: __________

    ready_for_go_live: [ ] Yes [ ] No

    final_authorization:
      authorized_by: __________
      date: __________
      authorization_code: __________
```

---

## Document Control

| Version | Date | Author | Changes |
|---------|------|--------|---------|
| 1.0 | 2025-11-28 | Platform Team | Initial release |

**Review Schedule**: Before each major release
**Next Review**: Prior to v2.0 deployment
**Owner**: Engineering Leadership
**Approved By**: CTO, CEO

---

## 8.4 Launch Sequence

### 8.4.1 Launch Timeline Overview

The launch sequence follows a carefully orchestrated timeline with specific checkpoints and validation gates.

**T-24h: Final Staging Validation & Team Briefing**
- Execute comprehensive staging validation
- Review all pre-launch criteria
- Conduct team briefing on roles and responsibilities
- Validate rollback procedures
- Confirm communication templates ready

**T-12h: Production Environment Check & Backup Verification**
- Complete production infrastructure health check
- Verify all backups within 24h window
- Validate disaster recovery readiness
- Confirm monitoring and alerting operational
- Execute security scan validation

**T-4h: Go/No-Go Meeting & Final Approvals**
- Review all validation results
- Assess risks and confirm mitigations
- Obtain final stakeholder approvals
- Make GO/NO-GO decision
- Issue launch authorization or postponement

**T-1h: War Room Setup & Monitoring Dashboards**
- Activate war room (Slack + Zoom)
- Verify all monitoring dashboards operational
- Test communication channels
- Conduct final runbook review
- Confirm team readiness

**T-0: Deployment Execution & Traffic Ramp**
- Execute deployment automation
- Perform smoke tests on new version
- Progressive traffic ramp (5% → 25% → 50% → 100%)
- Real-time validation at each checkpoint
- Monitor metrics continuously

**T+0 to T+24h: Post-Launch Monitoring (Critical Period)**
- Hours 0-4: Monitor every 15 minutes
- Hours 4-12: Monitor every hour
- Hours 12-24: Monitor every 2 hours
- Track error rates, latency, and system health
- Document any incidents or anomalies

---

### 8.4.2 T-24h Final Staging Validation

#### Validation Checklist

```bash
#!/bin/bash
# T-24h: Final staging validation script

echo "=== Final Staging Validation ==="
date

# Infrastructure
kubectl get nodes --context=staging | grep Ready || exit 1
kubectl get pods -n llm-research-lab-staging --context=staging || exit 1

# Database
psql -h staging-db -U postgres -d llm_research_lab -c "SELECT 1" || exit 1

# API Smoke Tests
curl -sf https://staging-api.llm-research-lab.company.com/health || exit 1

# Performance Baseline
./scripts/performance-baseline-check.sh staging || exit 1

# Security Scan
./scripts/security-scan.sh staging || exit 1

# Integration Tests
pytest tests/integration/ --env=staging || exit 1

# Load Test (Light)
./scripts/load-test-light.sh staging || exit 1

# Rollback Procedure
./scripts/validate-rollback-procedure.sh staging || exit 1

echo "✓ All validation checks passed"
```

#### Team Briefing Agenda

**Duration**: 1 hour
**Attendees**: All launch team members

1. **Launch Overview** (10 min)
   - Timeline review
   - Success criteria
   - Deployment strategy

2. **Role Assignments** (15 min)
   - Deployment Lead
   - SRE on-call
   - War room coordinator
   - Communications lead
   - Incident response roles

3. **Technical Review** (20 min)
   - Staging validation results
   - Production readiness
   - Traffic ramp strategy
   - Monitoring dashboards

4. **Risk Assessment** (10 min)
   - Known risks and mitigations
   - Rollback criteria
   - Escalation paths

5. **Q&A** (5 min)

---

### 8.4.3 T-12h Production Environment Check

```bash
#!/bin/bash
# T-12h: Production environment readiness check

echo "=== Production Readiness Check ==="

# Infrastructure
kubectl cluster-info --context=production || exit 1
kubectl get nodes --context=production | grep Ready || exit 1

# Database
pg_isready -h prod-postgres-primary || exit 1
pg_isready -h prod-postgres-replica || exit 1

# Backup Verification
aws s3 ls s3://llm-research-lab-postgres-backups/ | grep $(date +%Y%m%d) || exit 1

# Monitoring
curl -sf http://prometheus/api/v1/query?query=up || exit 1
curl -sf https://grafana/api/health || exit 1

# Security
./scripts/check-ssl-expiry.sh || exit 1
./scripts/security-scan-prod.sh || exit 1

# Capacity
./scripts/check-cpu-capacity.sh || exit 1
./scripts/check-memory-capacity.sh || exit 1
./scripts/check-disk-capacity.sh || exit 1

echo "✓ Production environment ready"
```

---

### 8.4.4 T-4h Go/No-Go Meeting

#### Decision Framework

**Critical Criteria (Must Pass)**:
- [ ] All staging tests passed
- [ ] Production environment healthy
- [ ] Backups verified < 24h
- [ ] Zero P0/P1 incidents (last 48h)
- [ ] Security clearance obtained
- [ ] Team available
- [ ] Rollback procedures tested

**Non-Critical Criteria (Risk Acceptance Possible)**:
- [ ] Load testing 20% headroom
- [ ] Documentation complete
- [ ] Customer communication prepared

**External Factors**:
- [ ] No major holidays
- [ ] No other deployments
- [ ] Stakeholder availability

**Decision Matrix**:
- **GO**: All critical PASS, max 1 non-critical FAIL
- **NO-GO**: Any critical FAIL, >1 non-critical FAIL
- **POSTPONE**: Borderline results, need more validation

---

### 8.4.5 T-1h War Room Setup

```bash
#!/bin/bash
# T-1h: War room activation

LAUNCH_DATE=$(date +%Y-%m-%d)
WAR_ROOM="#launch-${LAUNCH_DATE}"

# Create Slack channel
slack-cli create-channel "$WAR_ROOM"

# Invite team
slack-cli invite-users "$WAR_ROOM" "@deployment-lead @sre-oncall @eng-manager"

# Pin information
slack-cli post-message "$WAR_ROOM" "
📋 Launch Information
- Runbook: https://docs.company.com/launch-runbook
- Grafana: https://grafana.company.com/d/launch
- Rollback: https://docs.company.com/rollback
"

# Enable enhanced monitoring
kubectl patch configmap monitoring-config \
  -p '{"data":{"ENHANCED_MONITORING":"true"}}'

echo "✓ War room operational"
```

---

### 8.4.6 T-0 Deployment Execution

```bash
#!/bin/bash
# T-0: Production deployment with traffic ramp

set -e

echo "=== Production Deployment ==="
DEPLOYMENT_ID="deploy-$(date +%Y%m%d-%H%M%S)"

# Phase 1: Pre-deployment check
./scripts/pre-deployment-check.sh || exit 1

# Phase 2: Database migrations
./scripts/run-migrations.sh production || exit 1

# Phase 3: Deploy new version (0% traffic)
kubectl apply -f k8s/production/api-v2.yaml
kubectl rollout status deployment/api-v2 -n prod

# Phase 4: Smoke tests
./scripts/smoke-tests.sh production api-v2 || exit 1

# Phase 5: Traffic ramp 5%
kubectl patch virtualservice api -p '{"spec":{"http":[{"route":[
  {"destination":{"host":"api-v2"},"weight":5},
  {"destination":{"host":"api-v1"},"weight":95}
]}]}}'
sleep 300
./scripts/validate-metrics.sh 5 || exit 1

# Phase 6: Traffic ramp 25%
kubectl patch virtualservice api -p '{"spec":{"http":[{"route":[
  {"destination":{"host":"api-v2"},"weight":25},
  {"destination":{"host":"api-v1"},"weight":75}
]}]}}'
sleep 300
./scripts/validate-metrics.sh 25 || exit 1

# Phase 7: Traffic ramp 50%
kubectl patch virtualservice api -p '{"spec":{"http":[{"route":[
  {"destination":{"host":"api-v2"},"weight":50},
  {"destination":{"host":"api-v1"},"weight":50}
]}]}}'
sleep 600
./scripts/validate-metrics.sh 50 || exit 1

# Phase 8: Traffic ramp 100%
kubectl patch virtualservice api -p '{"spec":{"http":[{"route":[
  {"destination":{"host":"api-v2"},"weight":100}
]}]}}'
sleep 600
./scripts/validate-metrics.sh 100 || exit 1

# Phase 9: Cleanup
kubectl delete deployment api-v1 -n prod

echo "✓ Deployment successful: $DEPLOYMENT_ID"
```

---

### 8.4.7 Post-Launch Monitoring

**Hours 0-4 (Every 15 minutes)**:
- [ ] Error rate < 0.1%
- [ ] API latency p99 < 100ms
- [ ] Database connections stable
- [ ] Kafka consumer lag < 1000
- [ ] No critical alerts
- [ ] Active users trending normally
- [ ] Resources within limits

**Hours 4-12 (Every hour)**:
- [ ] Cumulative error rate < 0.05%
- [ ] Performance metrics stable
- [ ] No incident tickets
- [ ] User feedback neutral/positive
- [ ] System health green

**Hours 12-24 (Every 2 hours)**:
- [ ] All metrics within SLA
- [ ] No degradation trends
- [ ] Resource utilization steady
- [ ] Backups running successfully

---

## 8.5 Post-Launch Validation

### 8.5.1 24-Hour Stability Check

```bash
#!/bin/bash
# T+24h: Stability validation

echo "=== 24-Hour Stability Validation ==="

# Performance Validation
ERROR_RATE=$(prometheus-query 'rate(api_request_total{status=~"5.."}[24h])')
if (( $(echo "$ERROR_RATE < 0.001" | bc -l) )); then
  echo "✓ Error rate: $ERROR_RATE"
else
  echo "✗ Error rate: $ERROR_RATE (threshold: < 0.1%)"
  FAILURES=$((FAILURES+1))
fi

LATENCY_P99=$(prometheus-query 'histogram_quantile(0.99, rate(api_request_duration_seconds_bucket[24h]))')
if (( $(echo "$LATENCY_P99 < 0.1" | bc -l) )); then
  echo "✓ Latency p99: ${LATENCY_P99}s"
else
  echo "✗ Latency p99: ${LATENCY_P99}s (threshold: < 100ms)"
  FAILURES=$((FAILURES+1))
fi

# Reliability Validation
AVAILABILITY=$(prometheus-query 'rate(api_request_total{status!~"5.."}[24h]) / rate(api_request_total[24h])')
if (( $(echo "$AVAILABILITY > 0.999" | bc -l) )); then
  echo "✓ Availability: $AVAILABILITY"
else
  echo "✗ Availability: $AVAILABILITY (threshold: > 99.9%)"
  FAILURES=$((FAILURES+1))
fi

# Incident Validation
P0_COUNT=$(incident-api count --severity=P0 --since=24h)
P1_COUNT=$(incident-api count --severity=P1 --since=24h)

if [ "$P0_COUNT" -eq 0 ]; then
  echo "✓ P0 incidents: 0"
else
  echo "✗ P0 incidents: $P0_COUNT (threshold: 0)"
  FAILURES=$((FAILURES+1))
fi

if [ "$P1_COUNT" -le 1 ]; then
  echo "✓ P1 incidents: $P1_COUNT"
else
  echo "✗ P1 incidents: $P1_COUNT (threshold: <= 1)"
  FAILURES=$((FAILURES+1))
fi

# Summary
if [ $FAILURES -eq 0 ]; then
  echo "✅ All stability criteria PASSED"
  exit 0
else
  echo "❌ $FAILURES criteria FAILED"
  exit 1
fi
```

---

### 8.5.2 Performance Baseline Comparison

| Metric | Baseline | Current | Delta | Status |
|--------|----------|---------|-------|--------|
| API Latency p50 | 40ms | __ms | ___% | [ ] Pass |
| API Latency p95 | 80ms | __ms | ___% | [ ] Pass |
| API Latency p99 | 100ms | __ms | ___% | [ ] Pass |
| Throughput | 1000 req/s | __ req/s | ___% | [ ] Pass |
| Error Rate | 0.1% | __% | ___% | [ ] Pass |

**Acceptance**: Delta < 10% for all metrics

---

### 8.5.3 User Acceptance Verification

**Day 1-2: Internal Validation**
- [ ] Product team feature walkthrough
- [ ] Engineering team dogfooding
- [ ] Documentation validation
- [ ] Support team review

**Day 3-5: Beta User Feedback**
- [ ] Survey to 100 beta users
- [ ] 10 user interviews
- [ ] Feature usage analytics
- [ ] Support ticket analysis

**Day 6-7: General Feedback**
- [ ] In-app feedback enabled
- [ ] NPS survey deployed
- [ ] Usage patterns analyzed
- [ ] Performance feedback collected

**Success Criteria**:
- Beta satisfaction > 80%
- Feature adoption > 50%
- Support tickets < 20% increase
- NPS score > 40
- Zero critical bugs

---

### 8.5.4 Error Rate Monitoring

**Continuous Monitoring Alerts**:

```yaml
# High 5xx error rate (critical)
- alert: HighServerErrorRate
  expr: rate(api_request_total{status=~"5.."}[5m]) / rate(api_request_total[5m]) > 0.01
  for: 2m
  severity: critical

# Elevated 4xx rate (warning)
- alert: ElevatedClientErrorRate
  expr: rate(api_request_total{status=~"4.."}[5m]) / rate(api_request_total[5m]) > 0.05
  for: 5m
  severity: warning

# Database errors
- alert: DatabaseErrorSpike
  expr: rate(db_errors_total[5m]) > 10
  for: 2m
  severity: critical

# Experiment failures
- alert: HighExperimentFailureRate
  expr: rate(experiments_total{status="failed"}[10m]) / rate(experiments_total[10m]) > 0.05
  for: 5m
  severity: critical
```

---

### 8.5.5 Lessons Learned Capture

**Retrospective Template**:

```markdown
# Launch Lessons Learned

**Launch Date**: __________
**Retrospective Date**: __________
**Facilitator**: __________

## What Went Well

### Technical
-

### Process
-

### Communication
-

## What Could Be Improved

| Category | Issue | Impact | Root Cause | Improvement | Owner |
|----------|-------|--------|------------|-------------|-------|
| Technical | | | | | |
| Process | | | | | |
| Communication | | | | | |

## Action Items

| Action | Owner | Due Date | Status |
|--------|-------|----------|--------|
| | | | |

## Metrics

| Metric | Target | Actual | Variance |
|--------|--------|--------|----------|
| Deployment Duration | 2h | __h | __% |
| Incidents (24h) | 0 | __ | __ |
| Error Rate (24h avg) | < 0.1% | __% | __% |
| User Satisfaction | > 80% | __% | __% |

## Key Takeaways

1.
2.
3.

## Next Launch Improvements

1.
2.
3.
```

---

## 8.6 Project Closure

### 8.6.1 Final Documentation Archive

**Archive Structure**:

```
/docs/archive/llm-research-lab-v1.0/
├── 01-requirements/
│   ├── product-requirements.md
│   ├── technical-requirements.md
│   └── architecture-decisions/
├── 02-design/
│   ├── system-architecture.md
│   ├── database-schema.sql
│   ├── api-specifications/
│   └── diagrams/
├── 03-development/
│   ├── source-code-archive.zip
│   ├── dependencies-manifest.json
│   └── build-configurations/
├── 04-testing/
│   ├── test-plans/
│   ├── test-results/
│   └── performance-benchmarks/
├── 05-deployment/
│   ├── infrastructure-configs/
│   ├── kubernetes-manifests/
│   └── ci-cd-pipelines/
├── 06-operations/
│   ├── runbooks/
│   ├── monitoring-dashboards/
│   └── incident-reports/
├── 07-launch/
│   ├── launch-plan.md
│   ├── launch-logs/
│   ├── launch-metrics.json
│   └── lessons-learned.md
└── 08-handoff/
    ├── knowledge-transfer-materials/
    ├── training-records/
    └── certification-records/
```

**Archive Creation Script**:

```bash
#!/bin/bash
# Create documentation archive

PROJECT_VERSION="v1.0"
ARCHIVE_DATE=$(date +%Y-%m-%d)
ARCHIVE_NAME="llm-research-lab-${PROJECT_VERSION}-archive-${ARCHIVE_DATE}"

# Create structure
mkdir -p "${ARCHIVE_NAME}"/{01-requirements,02-design,03-development,04-testing,05-deployment,06-operations,07-launch,08-handoff}

# Archive requirements
cp -r /docs/requirements/* "${ARCHIVE_NAME}/01-requirements/"

# Archive design
cp /docs/architecture/* "${ARCHIVE_NAME}/02-design/"
pg_dump -s llm_research_lab > "${ARCHIVE_NAME}/02-design/database-schema.sql"

# Archive source code
git archive --format=zip --output="${ARCHIVE_NAME}/03-development/source-code.zip" HEAD

# Archive test artifacts
cp -r /tests/* "${ARCHIVE_NAME}/04-testing/"

# Archive deployment configs
cp -r /k8s/* "${ARCHIVE_NAME}/05-deployment/"

# Archive operations docs
cp -r /docs/runbooks/* "${ARCHIVE_NAME}/06-operations/"

# Archive launch materials
cp /docs/launch/* "${ARCHIVE_NAME}/07-launch/"

# Archive handoff materials
cp -r /docs/training/* "${ARCHIVE_NAME}/08-handoff/"

# Create README
cat > "${ARCHIVE_NAME}/README.md" <<EOF
# LLM-Research-Lab Archive

**Version**: ${PROJECT_VERSION}
**Archive Date**: ${ARCHIVE_DATE}
**Files**: $(find "${ARCHIVE_NAME}" -type f | wc -l)
**Size**: $(du -sh "${ARCHIVE_NAME}" | cut -f1)

## Retention
- Period: 7 years
- Location: S3://company-archives/llm-research-lab/
- Backup: S3://company-archives-dr/llm-research-lab/
EOF

# Compress
tar -czf "${ARCHIVE_NAME}.tar.gz" "${ARCHIVE_NAME}"

# Upload to S3
aws s3 cp "${ARCHIVE_NAME}.tar.gz" \
  s3://company-archives/llm-research-lab/ \
  --storage-class GLACIER

echo "✓ Archive complete: ${ARCHIVE_NAME}.tar.gz"
```

---

### 8.6.2 Metrics Summary Report

**Template**:

```markdown
# LLM-Research-Lab Final Metrics Summary

**Project**: LLM-Research-Lab
**Version**: v1.0
**Report Date**: __________
**Period**: [Start] to [Launch]

## Executive Summary

[2-3 paragraph summary]

## Timeline

| Milestone | Planned | Actual | Variance |
|-----------|---------|--------|----------|
| Kickoff | __ | __ | __ |
| Design Complete | __ | __ | __ |
| Development Complete | __ | __ | __ |
| Testing Complete | __ | __ | __ |
| Production Launch | __ | __ | __ |
| Handoff Complete | __ | __ | __ |

## Delivery Metrics

| Metric | Target | Actual | Status |
|--------|--------|--------|--------|
| Features Delivered | __ | __ | ✓/✗ |
| Test Coverage | > 80% | __% | ✓/✗ |
| Critical Bugs | 0 | __ | ✓/✗ |
| API Latency p99 | < 100ms | __ms | ✓/✗ |
| Availability | 99.9% | __% | ✓/✗ |

## Launch Performance

- **Launch Date**: __________
- **Deployment Duration**: __ hours
- **Traffic Ramp**: Smooth (YES/NO)
- **Incidents (30d)**: P0:__ P1:__ P2:__ P3:__
- **User Satisfaction**: __%

## Financial Summary

| Category | Budget | Actual | Variance |
|----------|--------|--------|----------|
| Development | $__ | $__ | __% |
| Infrastructure | $__ | $__ | __% |
| Total | $__ | $__ | __% |

## Lessons Learned

**What Went Well**:
1.
2.
3.

**Challenges**:
1.
2.
3.

**Recommendations**:
1.
2.
3.

## Handoff

- [X] Team trained and certified
- [X] Documentation complete
- [X] Operations ready
- [X] Stakeholders signed off

**Approved By**:
- Engineering Manager: __________ Date: __________
- Product Manager: __________ Date: __________
- Operations Manager: __________ Date: __________
- Executive Sponsor: __________ Date: __________
```

---

### 8.6.3 Team Recognition

**Recognition Program**:

```markdown
# Team Recognition - LLM-Research-Lab

## Core Team

**Engineering Lead**: [Name]
- Led technical architecture
- Mentored team members
- Delivered high-quality solution

**Senior Engineers**: [Names]
- Implemented core services
- Established quality standards
- Drove performance optimizations

**QA Lead**: [Name]
- Comprehensive test strategy
- Achieved >80% coverage
- Validated production readiness

**SRE Lead**: [Name]
- Built CI/CD pipeline
- Designed infrastructure
- Ensured operational excellence

**Product Manager**: [Name]
- Defined vision
- Managed stakeholders
- Delivered business objectives

## Special Recognition

**Innovation Award**: [Name] - [Achievement]
**Excellence in Quality**: [Name] - [Achievement]
**Outstanding Collaboration**: [Name] - [Achievement]
**Technical Leadership**: [Name] - [Achievement]

## Celebration

**Date**: __________
**Activity**: Team dinner/outing
**Budget**: $______

## Thank You

Thank you to everyone who contributed to the success of LLM-Research-Lab!
```

---

### 8.6.4 Retrospective Template

```markdown
# Project Retrospective - LLM-Research-Lab

**Date**: __________
**Duration**: 2 hours
**Facilitator**: __________
**Attendees**: All team members

## Format: Start-Stop-Continue

### Start (What we should start doing)

| Item | Benefit | Owner | Priority |
|------|---------|-------|----------|
| | | | H/M/L |

### Stop (What we should stop doing)

| Item | Why Stop? | Impact | Owner |
|------|-----------|--------|-------|
| | | | |

### Continue (What we should keep doing)

| Item | Why Continue? | Owner |
|------|---------------|-------|
| | | |

## Action Items

| Action | Owner | Due Date | Status |
|--------|-------|----------|--------|
| | | | |

## Metrics Review

- **Velocity**: __ story points/sprint
- **Code Quality**: __% coverage
- **Deployment Frequency**: __ deploys/week
- **MTTR**: __ hours

## Team Health

- **Satisfaction**: __/10
- **Burnout Risk**: Low/Medium/High
- **Work-Life Balance**: __/10

## Closing

[Summary of key takeaways]
```

---

### 8.6.5 Archive Location & Retention

**Retention Policy**:

```yaml
# archive-policy.yaml
archive_policy:
  project: LLM-Research-Lab
  version: v1.0

  primary_archive:
    location: s3://company-archives/llm-research-lab/
    region: us-east-1
    storage_class: GLACIER
    encryption: AES-256

  dr_archive:
    location: s3://company-archives-dr/llm-research-lab/
    region: us-west-2
    replication: enabled

  retention:
    source_code: 7 years
    design_documents: 7 years
    test_artifacts: 3 years
    deployment_logs: 3 years
    operational_metrics: 5 years
    incident_reports: 5 years
    training_materials: Indefinite

  access:
    read: [Engineering, Product, Compliance, Executive]
    write: [Archive Administrators]
    audit_logging: enabled

  review:
    frequency: Annual
    next_review: [Date + 1 year]
    owner: Engineering Manager
```

---

## Appendix C: Launch Runbook

```markdown
# Production Launch Runbook

**Version**: 1.0
**Owner**: SRE Team

## Pre-Launch (T-24h)

- [ ] Staging validation 100% pass
- [ ] Production health checks pass
- [ ] Backups verified
- [ ] Team briefing complete
- [ ] Rollback validated

### Commands
\`\`\`bash
./scripts/final-staging-validation.sh
./scripts/production-environment-check.sh
\`\`\`

## Go/No-Go (T-4h)

**Criteria**:
- Staging tests: PASS/FAIL
- Production ready: PASS/FAIL
- Team available: YES/NO
- No blockers: YES/NO

**Decision**: GO / NO-GO

## War Room (T-1h)

\`\`\`bash
./scripts/war-room-setup.sh
kubectl patch configmap monitoring-config \
  -p '{"data":{"ENHANCED_MONITORING":"true"}}'
\`\`\`

## Deployment (T-0)

\`\`\`bash
./scripts/production-deployment.sh
kubectl rollout status deployment/api -n prod
\`\`\`

### Traffic Ramp
- T+15min: 5%
- T+30min: 25%
- T+45min: 50%
- T+60min: 100%

## Post-Launch (T+0 to T+24h)

**Monitoring**:
- Hours 0-4: Every 15 min
- Hours 4-12: Every hour
- Hours 12-24: Every 2 hours

**Metrics**:
- Error rate < 0.1%
- Latency p99 < 100ms
- No P0/P1 incidents

## Rollback

**Triggers**:
- Error rate > 1%
- P0 incident
- Performance degradation > 50%

\`\`\`bash
./scripts/rollback-deployment.sh
\`\`\`

## Contacts

| Role | Contact |
|------|---------|
| Deployment Lead | @deployment-lead |
| SRE On-Call | @sre-oncall |
| Engineering Manager | @eng-manager |
```

---

## Appendix D: Validation Scripts

See referenced scripts:
- `/workspaces/llm-research-lab/scripts/final-staging-validation.sh`
- `/workspaces/llm-research-lab/scripts/production-environment-check.sh`
- `/workspaces/llm-research-lab/scripts/24h-stability-validation.sh`
- `/workspaces/llm-research-lab/scripts/production-deployment.sh`
- `/workspaces/llm-research-lab/scripts/war-room-setup.sh`

---

## Appendix E: Closure Templates

### E.1 Metrics Summary Report

See Section 8.6.2 template

### E.2 Team Recognition

See Section 8.6.3 template

### E.3 Retrospective

See Section 8.6.4 template

### E.4 Lessons Learned

See Section 8.5.5 template

---

## Document Updates

| Version | Date | Author | Changes |
|---------|------|--------|---------|
| 1.0 | 2025-11-28 | Platform Team | Initial release (Sections 8.1-8.3) |
| 1.1 | 2025-11-28 | Platform Team | Added Sections 8.4-8.6 (Launch, Validation, Closure) |

**Review Schedule**: Before each major release
**Next Review**: Prior to v2.0 deployment
**Owner**: Engineering Leadership
**Approved By**: CTO, CEO

---

**End of Document**

---

# APPENDICES

## Appendix A: Document Index

This master specification was consolidated from the following source documents:

### Phase 1: Specification
| Document | Lines | Description |
|----------|-------|-------------|
| LLM-Research-Lab-Specification.md | 339 | Core specification |

### Phase 2: Pseudocode
| Document | Lines | Description |
|----------|-------|-------------|
| LLM-Research-Lab-Pseudocode-Part1.md | 2,553 | Core data models, experiment tracking |
| LLM-Research-Lab-Pseudocode-Part2.md | 3,517 | Metric system, dataset management |
| LLM-Research-Lab-Pseudocode-Part3.md | 3,121 | Workflow engine, API contracts |

### Phase 3: Architecture
| Document | Lines | Description |
|----------|-------|-------------|
| LLM-Research-Lab-Architecture-Part1.md | 1,713 | System design, service architecture |
| LLM-Research-Lab-Architecture-Part2.md | 3,217 | Database design, infrastructure |

### Phase 4: Refinement
| Document | Lines | Description |
|----------|-------|-------------|
| LLM-Research-Lab-Refinement.md | 4,020 | Quality standards, testing, security |
| SPARC-Refinement-Sections-10-11.md | 1,271 | Compliance, release criteria |

### Phase 5: Completion
| Document | Lines | Description |
|----------|-------|-------------|
| SPARC-Phase5-Completion-Sections-1-2.md | 1,128 | Executive summary, objectives |
| SPARC-Phase5-Section3-Deployment-Readiness.md | 1,604 | Pre-deployment, artifacts |
| SPARC-Phase5-Section3-Deployment-Strategies-Smoke-Tests.md | 1,341 | Deployment strategies |
| SPARC-Phase5-Section4-Production-Infrastructure.md | 2,420 | Kubernetes, databases |
| SPARC-Phase5-Section5-Operations-Monitoring.md | 3,206 | Observability, alerting |
| SPARC-Phase5-Section6-Disaster-Recovery.md | 2,938 | DR, business continuity |
| SPARC-Phase5-Section7-Handoff-Knowledge-Transfer.md | 1,878 | Training, documentation |
| SPARC-Phase5-Section8-Final-Validation-Sign-off.md | 3,352 | Certification, launch |

---

## Appendix B: Technology Stack Summary

```yaml
# Complete Technology Stack for LLM-Research-Lab

runtime:
  language: Rust 1.75+
  async_runtime: Tokio
  web_framework: Axum
  grpc: Tonic
  serialization: Serde

databases:
  primary: PostgreSQL 16 (CloudNativePG)
  time_series: ClickHouse
  cache: Redis 7 (Cluster)
  search: Meilisearch
  object_storage: S3-compatible

messaging:
  event_streaming: Apache Kafka 3.6 (Strimzi)
  task_queue: Redis Streams

orchestration:
  container: Kubernetes 1.28+
  ingress: NGINX Ingress Controller
  service_mesh: Istio (optional)
  secrets: External Secrets Operator
  gitops: ArgoCD

observability:
  metrics: Prometheus + Grafana
  logging: Vector + Loki
  tracing: OpenTelemetry + Jaeger
  profiling: Pyroscope

ci_cd:
  pipelines: GitHub Actions
  registry: Harbor
  iac: Terraform
  security: cargo-audit, Trivy, Semgrep
```

---

## Appendix C: Quality Gates Summary

| Gate | Threshold | Enforcement |
|------|-----------|-------------|
| Code Coverage | ≥ 85% | CI blocker |
| Branch Coverage | ≥ 80% | CI blocker |
| Mutation Score | ≥ 70% | CI warning |
| Clippy Warnings | Zero | CI blocker |
| Security Vulnerabilities (Critical/High) | Zero | CI blocker |
| API Latency (p99) | < 100ms | Performance test |
| Error Rate | < 0.1% | SLO monitor |
| Availability | 99.9% | SLO monitor |

---

## Appendix D: Timeline Summary

| Phase | Duration | Key Deliverables |
|-------|----------|------------------|
| Phase 1: Specification | 1 week | Requirements, scope, success criteria |
| Phase 2: Pseudocode | 2 weeks | Data models, algorithms, API contracts |
| Phase 3: Architecture | 2 weeks | System design, infrastructure, security |
| Phase 4: Refinement | 8 weeks | Quality standards, testing, compliance |
| Phase 5: Completion | 8-10 weeks | Deployment, operations, launch |

**Total**: ~21-23 weeks from specification to production launch

---

## Document Control

| Field | Value |
|-------|-------|
| **Document** | SPARC Master Specification |
| **Version** | 1.0.0 |
| **Project** | LLM-Research-Lab |
| **Ecosystem** | LLM DevOps |
| **Status** | Complete |
| **Total Lines** | ~38,000+ |

---

*This master specification document consolidates all five SPARC phases for the LLM-Research-Lab project.*
*Generated by Claude Flow Swarm - SPARC Specification Consolidation*

